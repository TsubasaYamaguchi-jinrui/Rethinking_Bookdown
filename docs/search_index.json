[["index.html", "Statistical Rethinking Second Edition 本稿の目的", " Statistical Rethinking Second Edition Tsubasa Yamaguchi 2023-02-01 本稿の目的 本稿は、Statistical Rethinking: A Bayesian Course with Examples in R and Stan Second Edition (McElreath 2020)をRで勉強した際のまとめ資料です。この本は、一般化線形モデル(GLM)および一般化線形混合モデル(GLMM)について解説と、それをベイズモデリングで実装する方法をまとめた本です。欧米でベイズモデリングを学ぶ際によく用いられているテキストで、人類学者でもあるRichard McElreathが書いているということもあり、霊長類学や人類学の例を用いた開設が行われることが多く、私たちが読んでいて興味深い点が多いことも特徴である。 本稿では、McElreathが本で使用しているrethinkingパッケージではなく、より一般に広く用いられているbrmsパッケージを使用した。参考にしたのはこののサイトである。 練習問題の解答は、こちらとこちらを参考にした。 References "],["パッケージの読み込み.html", "パッケージの読み込み", " パッケージの読み込み 使用したパッケージは以下の通り。ベイズモデリングには、基本的にはbrmsパッケージを用いている。 library(DT) library(GGally) library(rstan) library(bookdown) options(kableExtra.latex.load_packages = FALSE) library(kableExtra) library(knitr) library(tidyverse) library(rethinking) library(brms) library(ggmcmc) library(tidybayes) library(ggrepel) library(patchwork) library(ggdag) library(dagitty) library(bayesplot) library(ggrepel) library(ggthemes) library(loo) library(gt) library(ggrepel) library(extrafont) require(systemfonts) require(fontregisterer) extrafont::loadfonts(device = &quot;win&quot;) theme_set(theme_classic(base_size=10.5)) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) "],["the-golem-of-prague.html", "1 The Golem of Prague ", " 1 The Golem of Prague 本全体の導入のため割愛。 "],["small-worlds-and-large-worlds.html", "2 Small Worlds and Large Worlds 2.1 Byesian Updating 2.2 Making th model 2.3 Practice", " 2 Small Worlds and Large Worlds 2.1 Byesian Updating 地球儀の例について、逐次更新をシミュレート (d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;))) ## # A tibble: 9 × 1 ## toss ## &lt;chr&gt; ## 1 w ## 2 l ## 3 w ## 4 w ## 5 w ## 6 l ## 7 w ## 8 l ## 9 w (d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss==&quot;w&quot;))) ## # A tibble: 9 × 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 sequence_length &lt;- 50 # 各トライアルごとに横軸を追加 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) -&gt;d2 # 1つ前のtrialの番号と成功数を追加 d2 %&gt;% group_by(p_water) %&gt;% mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1))-&gt; d3 # 尤度を計算 d3 %&gt;% ungroup() %&gt;% mutate(prior = ifelse(n_trials == 1, .5, dbinom(x=lagged_n_success, size =lagged_n_trials, prob = p_water)), likelihood = dbinom(x = n_success, size = n_trials, prob = p_water), strip = str_c(&quot;n = &quot;, n_trials)) -&gt; d4 # 尤度のスケールを変更 d4 %&gt;% group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) -&gt;d5 # 描画 d5 %&gt;% ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood))+ scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ strip, scale=&quot;free_y&quot;) 2.2 Making th model 2.2.1 Grid approximation tibble(n_points = c(5,20)) %&gt;% mutate(p_grid = purrr::map(n_points, ~seq(0,1,length.out=.))) %&gt;% unnest(p_grid) %&gt;% expand(nesting(n_points,p_grid), priors = c(&quot;ifelse(p_grid &lt; 0.5, 0, 1)&quot;, &quot;exp(-5*abs(p_grid - 0.5))&quot;)) %&gt;% mutate(prior = ifelse(priors == &quot;ifelse(p_grid &lt; 0.5, 0, 1)&quot;, ifelse(p_grid &lt; 0.5, 0, 1), exp(-5 * abs(p_grid - 0.5)))) %&gt;% mutate(likelihood = dbinom(6,9,prob = p_grid)) %&gt;% mutate(posterior = likelihood*prior / sum(likelihood*prior)) %&gt;% ggplot(aes(x=p_grid,y=posterior))+ geom_line()+ geom_point()+ theme(panel.grid = element_blank())+ facet_grid(n_points ~ priors, scale = &quot;free&quot;) 2.2.2 Quadratic approximation library(rethinking) #globe.qa &lt;- quap( #alist( #W ~ dbinom(W+L,p), #p ~ dunif(0,1) #), #data = list(W=6, L =3)) #precis(globe.qa) 2.2.3 MCMC MCMCをもちいて推定してみる。 library(brms) b2.1 &lt;- brm(data = list(W = 24), family = binomial(link=&quot;identity&quot;), formula = W|trials(36) ~ 0+Intercept, prior = prior(beta(1,1),class = &quot;b&quot;, lb = 0, ub =1), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter2/b2.1&quot;, seed =123) 推定結果は以下の通り。 print(b2.1) ## Family: binomial ## Links: mu = identity ## Formula: W | trials(36) ~ 0 + Intercept ## Data: list(W = 24) (Number of observations: 1) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.66 0.08 0.50 0.80 1.00 1658 1849 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 事後分布は以下の通り。 posterior_samples(b2.1) %&gt;% ggplot(aes(x=b_Intercept))+ geom_density(fill=&quot;black&quot;)+ theme(panel.grid=element_blank()) 2.3 Practice 2.3.1 2M1 Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p. (1) W, W, W (2) W, W, W, L (3) L, W, W, L, W, W, W dist &lt;- tibble(p_grid = seq(0,1,length.out = 30)) %&gt;% mutate(prior = rep(1,30), likelihood_1 = dbinom(3,3,p_grid), likelihood_2 = dbinom(3,4,p_grid), likelihood_3 = dbinom(5,7,p_grid), across(starts_with(&quot;likelihood&quot;), ~ .x * prior), across(starts_with(&quot;likelihood&quot;), ~ .x / sum(.x))) %&gt;% pivot_longer(cols = starts_with(&quot;likelihood&quot;),names_to = &quot;pattern&quot;, values_to = &quot;posterior&quot;) %&gt;% arrange(pattern) %&gt;% separate(pattern, c(NA, &quot;pattern&quot;), sep = &quot;_&quot;, convert = TRUE) %&gt;% mutate(obs = case_when(pattern == 1L ~ &quot;W,W,L&quot;, pattern == 2L ~ &quot;W,W,W,L&quot;, pattern == 3L ~ &quot; L,W,W,L,W,W,W&quot;)) dist %&gt;% ggplot(aes(x=p_grid,y=posterior))+ geom_line()+ geom_point()+ theme(panel.grid = element_blank())+ scale_x_continuous(&quot;probability&quot;)+ facet_wrap(vars(fct_inorder(obs)),nrow=1) 2.3.2 2M2 Now assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above. dist &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20)) %&gt;% mutate(prior = case_when(p_grid &lt; 0.5 ~ 0L, TRUE ~ 1L), likelihood_1 = dbinom(3, size = 3, prob = p_grid), likelihood_2 = dbinom(3, size = 4, prob = p_grid), likelihood_3 = dbinom(5, size = 7, prob = p_grid), across(starts_with(&quot;likelihood&quot;), ~ .x * prior), across(starts_with(&quot;likelihood&quot;), ~ .x / sum(.x))) %&gt;% pivot_longer(cols = starts_with(&quot;likelihood&quot;), names_to = &quot;pattern&quot;, values_to = &quot;posterior&quot;) %&gt;% separate(pattern, c(NA, &quot;pattern&quot;), sep = &quot;_&quot;, convert = TRUE) %&gt;% mutate(obs = case_when(pattern == 1L ~ &quot;W, W, W&quot;, pattern == 2L ~ &quot;W, W, W, L&quot;, pattern == 3L ~ &quot;L, W, W, L, W, W, W&quot;)) ggplot(dist, aes(x = p_grid, y = posterior)) + facet_wrap(vars(fct_inorder(obs)), nrow = 1) + geom_line() + geom_point() + labs(x = &quot;Proportion Water (p)&quot;, y = &quot;Posterior Density&quot;) 2.3.3 2M3 Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observatiion. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” (Pr(Earth|land)), is 0.23. # probability of land, given Earth p_le &lt;- 0.3 # probability of land, given Mars p_lm &lt;- 1.0 # probability of Earth p_e &lt;- 0.5 # probability of land p_l &lt;- (p_e * p_le) + ((1 - p_e) * p_lm) # probability of Earth, given land (using Bayes&#39; Theorem) p_el &lt;- (p_le * p_e) / p_l p_el ## [1] 0.2307692 2.3.4 2M4 Suppose you have a deck with only three cards. Each card has only two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side faceing up on the table). card_bb_likelihood &lt;- 2 card_bw_likelihood &lt;- 1 card_ww_likelihood &lt;- 0 likelihood &lt;- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood) prior &lt;- c(1, 1, 1) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) posterior[1] ## [1] 0.6666667 2.3.5 2M5 Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before. card_bb_likelihood &lt;- 2 card_bw_likelihood &lt;- 1 card_ww_likelihood &lt;- 0 likelihood &lt;- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood) prior &lt;- c(1, 2, 3) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) posterior[1] ## [1] 0.5 2.3.6 2M7 Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, countng all the ways to see each observation, for each possiible first card. # 2 choices for first card, with 3 options for second card: 2 W/W + 1 W/B card_bb_likelihood &lt;- 2 * 3 card_wb_likelihood &lt;- 1 * 2 card_ww_likelihood &lt;- 0 likelihood &lt;- c(card_bb_likelihood, card_wb_likelihood, card_ww_likelihood) prior &lt;- c(1,1,1) posterior &lt;- prior * likelihood posterior &lt;- posterior / sum(posterior) posterior[1] ## [1] 0.75 2.3.7 2H1 Suppose there are two species of panda bear. Both are equally common in the wild and live in the sample places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species births twins 20% of the time, ottherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field research. Now suppose you are managing a captive panda breeding program. You have a newe female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins? # After first birth, likelihood of species A and B is equal to the rate the # species give birth to twins a_likelihood &lt;- 0.1 b_likelihood &lt;- 0.2 # Next calculate the posterior probability that the panda belongs to each # species, assume species are equally likely likelihood &lt;- c(a_likelihood, b_likelihood) prior &lt;- c(1, 1) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) posterior ## [1] 0.3333333 0.6666667 # The probability the next birth is twins is the probability the panda belongs # to each species times the likelihood each species gives birth to twins (posterior[1] * a_likelihood) + (posterior[2] * b_likelihood) ## [1] 0.1666667 2.3.8 2H2 Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, asssuming we have observed only the first birth at that it was twins. # probability of species A p_a &lt;- 0.5 # probability of twins, given species A p_ta &lt;- 0.1 # probability of twins, given species B p_tb &lt;- 0.2 # probability of twins p_t &lt;- (p_a * p_ta) + ((1 - p_a) * p_tb) # probability of species A, given twins (using Bayes&#39; Theorem) # (note this is equivalent to `posterior[1]` above) p_at &lt;- (p_ta * p_a) / p_t p_at ## [1] 0.3333333 2.3.9 2H3 Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A. # likelihood for each species is Pr(twins) * Pr(singleton) a_likelihood &lt;- 0.1 * (1 - 0.1) b_likelihood &lt;- 0.2 * (1 - 0.2) # compute posterior probabilities likelihood &lt;- c(a_likelihood, b_likelihood) prior &lt;- c(1, 1) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) posterior[1] ## [1] 0.36 2.3.10 2H4 A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of different types. So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test: The probability it correctly identifies a species A panda is 0.8. The probability it correctly identifies a species B panda is 0.65. The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well. # use Bayes&#39; Theorem to determine the probability of species A, given a positive # test p_ap &lt;- (0.8 * 0.5) / ((0.5 * 0.8) + (0.5 * 0.35)) p_ap ## [1] 0.6956522 # Now include test data with observed births # likelihood for each species is Pr(twins) * Pr(singleton) a_likelihood &lt;- 0.1 * (1 - 0.1) b_likelihood &lt;- 0.2 * (1 - 0.2) # compute posterior probabilities, using test result as prior likelihood &lt;- c(a_likelihood, b_likelihood) prior &lt;- c(p_ap, (1 - p_ap)) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) posterior[1] ## [1] 0.5625 "],["sampling-the-imaginary.html", "3 Sampling the Imaginary 3.1 Sampling from a grid-aproximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Practice", " 3 Sampling the Imaginary 3.1 Sampling from a grid-aproximate posterior 地球儀の例について、事後確率を算出する。 9回中6回地表にコインが落ちたときを考え、grid-approximationを行う。 set.seed(100) n &lt;- 1000 n_success &lt;- 6 n_trials &lt;- 9 (d &lt;- tibble(p_grid = seq(0,1,length.out =n), prior = 1) %&gt;% mutate(likelihood = dbinom(n_success,n_trials,prob=p_grid), posterior = likelihood*prior/sum(likelihood*prior))) ## # A tibble: 1,000 × 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 ## 2 0.00100 1 8.43e-17 8.43e-19 ## 3 0.00200 1 5.38e-15 5.38e-17 ## 4 0.00300 1 6.11e-14 6.11e-16 ## 5 0.00400 1 3.42e-13 3.42e-15 ## 6 0.00501 1 1.30e-12 1.30e-14 ## 7 0.00601 1 3.87e-12 3.88e-14 ## 8 0.00701 1 9.73e-12 9.74e-14 ## 9 0.00801 1 2.16e-11 2.16e-13 ## 10 0.00901 1 4.37e-11 4.38e-13 ## # … with 990 more rows d %&gt;% ggplot(aes(x=p_grid,y=posterior))+ geom_line(size=0.1)+ geom_point(size=0.1)+ theme(panel.grid = element_blank(), text = element_text(size = 12))+ theme_bw()+ xlab(&quot;proportion of water (p)&quot;) 事後分布からサンプルを取得する。 確率密度で重みづけしたうえで、各確率（0&lt;p&lt;1）を事後分布から10000回サンプリングする。 n_samples &lt;- 1e4 set.seed(100) samples &lt;- d %&gt;% slice_sample(n=n_samples, weight_by = posterior, replace= TRUE) %&gt;% mutate(sample_number = 1:n()) glimpse(samples) ## Rows: 10,000 ## Columns: 5 ## $ p_grid &lt;dbl&gt; 0.7137137, 0.3573574, 0.5985986, 0.7177177, 0.6296296, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ likelihood &lt;dbl&gt; 0.26051046, 0.04643060, 0.24993675, 0.25825703, 0.265888… ## $ posterior &lt;dbl&gt; 0.0026077123, 0.0004647707, 0.0025018693, 0.0025851554, … ## $ sample_number &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… サンプルはp=0.6付近に集中していることが分かる。 # figure3.1 samples %&gt;% ggplot(aes(x = sample_number, y = p_grid))+ geom_point(alpha=0.1, color = &quot;blue&quot;)+ scale_y_continuous(&quot;proportion of water (p)&quot;)+ theme_classic() samples %&gt;% ggplot(aes(x=p_grid))+ geom_density(color=&quot;blue&quot;, size = 0.7)+ theme_classic()+ scale_y_continuous(breaks = seq(0,2.5,0.5))+ scale_x_continuous(&quot;proportion of water (p)&quot;,breaks = seq(0,1,0.2)) 3.2 Sampling to summarize 事後分布からのサンプリングを用いることで、事後分布の区間推定や点推定を容易に行うことができる。 # grid approximationから d %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = sum(posterior)) ## # A tibble: 1 × 1 ## sum ## &lt;dbl&gt; ## 1 0.172 # サンプリングから # 0.5未満の確率 samples %&gt;% filter(p_grid &lt; 0.5) %&gt;% summarise(sum = n()/n_samples) ## # A tibble: 1 × 1 ## sum ## &lt;dbl&gt; ## 1 0.167 # 0.5以上0.75未満の確率 samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n()/n_samples) ## # A tibble: 1 × 1 ## sum ## &lt;dbl&gt; ## 1 0.621 # Figure3.2 p1 &lt;- d %&gt;% ggplot(aes(x=p_grid, y=posterior))+ geom_line()+ geom_area(data= d %&gt;% filter(p_grid &lt; .5), fill = &quot;blue&quot;)+ labs(x=&quot;proportion of water (p)&quot;, y= &quot;Density&quot;) p2 &lt;- d %&gt;% ggplot(aes(x=p_grid, y=posterior))+ geom_line()+ geom_area(data= d %&gt;% filter(p_grid &gt;.5 &amp; p_grid &lt; .75), fill = &quot;blue&quot;)+ labs(x=&quot;proportion of water (p)&quot;, y= &quot;Density&quot;) library(patchwork) p1+p2 3.2.1 Intervals of defined mass 事後分布空のサンプルを用いて区間推定を行う。 # 60%信用区間 quantile(samples$p_grid, c(0.2,0.8)) ## 20% 80% ## 0.5185185 0.7557558 # 60$信用区間の作図 q_80 &lt;- quantile(samples$p_grid, prob = .8) q_20 &lt;- quantile(samples$p_grid, prob = .2) d %&gt;% ggplot(aes(x=p_grid, y=posterior))+ geom_line()+ geom_area(data= d %&gt;% filter(p_grid &gt; q_20 &amp; p_grid &lt; q_80), fill = &quot;blue&quot;)+ labs(x=&quot;proportion of water (p)&quot;, y= &quot;Density&quot;) 3.2.2 highly skewed example 分布が偏っているデータでは、単純に信用区間をとるとMAP推定値が含まれない。HPDIが信用区間の代わりに使われることがある。 ここでは、3回中3回地表にコインが落ちた場合を考える。 n &lt;- 1000 n_trials &lt;- 3 n_success &lt;- 3 d2 &lt;- tibble(p_grid = seq(0,1,length.out =n), prior = 1) %&gt;% mutate(likelihood = dbinom(n_success,n_trials,prob=p_grid), posterior = likelihood*prior/sum(likelihood*prior)) samples2 &lt;- d2 %&gt;% slice_sample(n=n_samples, weight_by = posterior, replace = TRUE) 50%信用区間は以下の通り。 最頻値（MAP推定値）が含まれない。 q2_25 &lt;- quantile(samples2$p_grid, prob = .25) q2_75 &lt;- quantile(samples2$p_grid, prob = .75) PI(samples2$p_grid, prob=0.5) ## 25% 75% ## 0.7047047 0.9309309 # Figure3.3 d2 %&gt;% ggplot(aes(x=p_grid, y=posterior))+ geom_line()+ geom_area(data= d2 %&gt;% filter(p_grid &gt; q2_25 &amp; p_grid &lt; q2_75), fill = &quot;blue&quot;)+ labs(x=&quot;proportion of water (p)&quot;, y= &quot;Density&quot;) 50%最高密度区間（HPDI）を求める。 ある確率になる範囲の内、最も区間が狭くなるものをHPDIという。 hdpi50 &lt;- HPDI(samples2$p_grid, prob = c(0.5)) d2 %&gt;% ggplot(aes(x=p_grid, y=posterior))+ geom_line()+ geom_area(data= d2 %&gt;% filter(p_grid &gt; hdpi50[[1]] &amp; p_grid &lt; hdpi50[[2]]), fill = &quot;blue&quot;)+ labs(x=&quot;proportion of water (p)&quot;, y= &quot;Density&quot;) tidybayesを用いて区間推定ができる。 # using tidybayes library(tidybayes) median_qi(samples2$p_grid, .width = c(.5, .8, .99)) ## y ymin ymax .width .point .interval ## 1 0.8398398 0.7047047 0.9309309 0.50 median qi ## 2 0.8398398 0.5554555 0.9749750 0.80 median qi ## 3 0.8398398 0.2522372 0.9989990 0.99 median qi mode_hdi(samples2$p_grid, .width=.5) ## y ymin ymax .width .point .interval ## 1 0.9554858 0.8388388 0.998999 0.5 mode hdi HPDI(samples2$p_grid, prob=.5) ## |0.5 0.5| ## 0.8388388 0.9989990 hdi(samples2$p_grid, .width=.5) ## [,1] [,2] ## [1,] 0.8388388 0.998999 # comparing HDPI with QI bind_rows(mean_hdi(samples$p_grid, .width = c(.8,.95)), mean_qi(samples$p_grid, .width = c(.8,.95))) %&gt;% select(.width, .interval, ymin:ymax) %&gt;% arrange(.width) %&gt;% mutate_if(is.double, round, digits = 2) ## .width .interval ymin ymax ## 1 0.80 hdi 0.46 0.82 ## 2 0.80 qi 0.45 0.81 ## 3 0.95 hdi 0.37 0.89 ## 4 0.95 qi 0.35 0.88 3.2.3 Point estimate 代表値としては、mean, median, modeなどがとれる。 分布がゆがむとこれらの値は大きく違ってくる。 # binomial(3,3)の例 samples2 %&gt;% summarise(mean=mean(p_grid), median = median(p_grid), mode = Mode(p_grid)) ## # A tibble: 1 × 3 ## mean median mode ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.798 0.840 0.955 point_estimates &lt;- bind_rows(samples2 %&gt;% mean_qi(p_grid), samples2 %&gt;% median_qi(p_grid), samples2 %&gt;% mode_qi(p_grid))%&gt;% select(p_grid, .point) %&gt;% mutate(x = p_grid + c(-.03, .03, -.03), y = c(.0005, .0012, .002)) d2 %&gt;% ggplot(aes(x = p_grid)) + geom_area(aes(y = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$p_grid) + geom_text(data = point_estimates, aes(x = x, y = y, label = .point), angle = 90) 代表値を求める手法として、損失関数を最も小さくするような値を考えるとよい。 損失関数を最小にする代表値を選ぶとき、偏差平方和を最小にするのは平均値で、絶対偏差和を最小にするのが中央値である。 # 絶対偏差和の合計 make_loss &lt;- function(our_d) { d2 %&gt;% mutate(loss = posterior * abs(our_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } l &lt;- d2 %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest(weighted_average_loss) min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() l %&gt;% ggplot(aes(x = decision, y = weighted_average_loss)) + geom_area(fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) 3.3 Sampling to simulate prediction 事後分布からのサンプリングを用いてシミュレーションを行う。 まずは、rbinomの使い方をマスターする。 dummy &lt;- rbinom(1e5,size =2,prob=0.7) tibble(n_W = dummy) %&gt;% group_by(n_W) %&gt;% count() %&gt;% mutate(prop = n/1e5) ## # A tibble: 3 × 3 ## # Groups: n_W [3] ## n_W n prop ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 9006 0.0901 ## 2 1 41696 0.417 ## 3 2 49298 0.493 # size = 9 dummy &lt;- rbinom(1e5, size =9, prob = 0.7) d_3 &lt;- tibble(draws = dummy) d_3 %&gt;% ggplot(aes(x=draws))+ geom_histogram(binwidth =0.2, center = 0, color = &quot;grey92&quot;, size = 1/10)+ scale_y_continuous(&quot;frequency&quot;, breaks = seq(0,25000,5000))+ scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(0,9,2))+ theme(text=element_text(size=15))+ coord_cartesian(xlim=c(0,9))+ theme_classic() # 色々試してみる n_draws &lt;- 1e5 simulate_binom &lt;- function(n, probability){ set.seed(3) rbinom(n_draws, size =n, prob = probability) } d_4 &lt;- crossing(n = c(3,6,9), probability = c(.3,.6,.9)) %&gt;% mutate(draws = map2(n, probability, simulate_binom)) %&gt;% ungroup() %&gt;% mutate(n= str_c(&quot;n = &quot;, n), probability = str_c(&quot;p = &quot;, probability)) %&gt;% unnest(draws) d_4 %&gt;% ggplot(aes(x=draws))+ geom_histogram(binwidth = 1, color=&quot;grey92&quot;, size = 1/10, center =0)+ scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 1))+ ylab(&quot;frequency&quot;)+ theme(panel.grid = element_blank())+ facet_grid(n~probability) 3.3.1 Model checking 試行間の相関などがシミュレーションと比較して大きくないかなどを見ることで、モデルチェックを行うことができる。 set.seed(123) samples &lt;- d %&gt;% slice_sample(n=n_samples, weight_by = posterior, replace=TRUE) %&gt;% mutate(W = map_dbl(p_grid, rbinom, n =1, size=9)) samples %&gt;% ggplot(aes(x=W))+ geom_histogram(binwidth =0.2, size=1/10, color = &quot;grey92&quot;,center=0)+ scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(0,9,3))+ theme(text = element_text(size=18))+ scale_y_continuous(NULL, breaks=NULL)+ theme_bw() ## 試行間の相関を確認 set.seed(123) samples_2 &lt;- samples %&gt;% mutate(iter = 1:n(), draws = purrr::map(p_grid, rbinom,n=9,size=1)) %&gt;% unnest(draws) ## 連続回数 samples_2 %&gt;% group_by(iter) %&gt;% summarise(longest = rle(draws)$length %&gt;% max()) %&gt;% ggplot(aes(x=longest))+ geom_histogram(aes(fill = longest ==3), binwidth = 0.2, center=0, color=&quot;grey92&quot;)+ theme_bw()+ theme(legend.position = &quot;none&quot;,)+ scale_y_continuous(NULL, breaks = NULL) samples_2 %&gt;% group_by(iter) %&gt;% summarise(longest = rle(draws)$lengths %&gt;% length()) %&gt;% ggplot(aes(x=longest))+ geom_histogram(aes(fill = longest ==7), binwidth = 0.2, center=0, color=&quot;grey92&quot;)+ theme_bw()+ theme(legend.position = &quot;none&quot;,)+ scale_y_continuous(NULL, breaks = NULL) 3.3.2 Practice with brms b3.1 &lt;- brm(data=list(W=6), family = binomial(link=&quot;identity&quot;), formula = W|trials(9) ~ 0+Intercept, prior(&quot;&quot;, class=b, lb = 0, ub=1), iter = 5000, warmup=1000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter3/b3.1&quot;, seed = 123) print(b3.1) ## Family: binomial ## Links: mu = identity ## Formula: W | trials(9) ~ 0 + Intercept ## Data: list(W = 6) (Number of observations: 1) ## Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup draws = 16000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.64 0.14 0.34 0.88 1.00 6091 6584 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). p &lt;- posterior_samples(b3.1) %&gt;% as_tibble() p %&gt;% ggplot(aes(x=b_Intercept))+ geom_density() p &lt;- p %&gt;% mutate(W = rbinom(n(),size=n_trials, prob=b_Intercept)) p %&gt;% ggplot(aes(x=W))+ geom_histogram(binwidth=0.2) 3.4 Practice 3.4.1 3E he Easy problems use the sample from the posterior distribution for the globe tossing example. The code will give you a specific set of samples, so that you can check your answers exactly. p_grid &lt;- seq(from = 0, to = 1, length.out = 1000) prior &lt;- rep(1, 1000) likelihood &lt;- dbinom(6, size = 9, prob = p_grid) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) set.seed(100) samples &lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) Use the values in samples to answer the questions that follow. # 1 mean(samples &lt; 0.2) ## [1] 4e-04 # 2 mean(samples &gt; 0.8) ## [1] 0.1116 # 3 mean(between(samples,0.2,0.8)) ## [1] 0.888 # 4 quantile(samples,prob=0.2) ## 20% ## 0.5185185 # 5 quantile(samples,prob=0.8) ## 80% ## 0.7557558 # 6 hdi(samples, 0.66) ## [,1] [,2] ## [1,] 0.5085085 0.7737738 # 7 qi(samples, 0.66) ## [,1] [,2] ## [1,] 0.5025025 0.7697698 3.4.2 3M 3.4.2.1 3M1 Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Constructe the posterior distribution, using grid approximation. Use the same flat prior as before. n &lt;- 1000 n_trials &lt;- 15 n_success &lt;- 8 e &lt;- tibble(p_grid = seq(0,1,length.out=n), prior = 1/1000) %&gt;% mutate(likelihood = dbinom(n_success, n_trials, prob = p_grid), posterior = prior*likelihood, posterior = posterior/sum(posterior)) e %&gt;% ggplot(aes(x=p_grid,y=posterior))+ geom_line() 3.4.2.2 3M2 Draw 10,000 samples from the grid approximation from above. Then use the sample to calculate the 90% HPDI for p.  set.seed(123) e_samples &lt;- e %&gt;% slice_sample(n=1e4, weight_by= posterior, replace=TRUE) %&gt;% mutate(sample_number = 1:n()) hdi(e_samples$p_grid, .width=0.9) ## [,1] [,2] ## [1,] 0.3343343 0.7217217 3.4.2.3 3M3 Construct a posterior predictive check for this model and data. The means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses? set.seed(101) e_samples &lt;- e_samples %&gt;% mutate(W = map_dbl(p_grid, rbinom, n=1,size=15)) mean(e_samples$W == 8) ## [1] 0.1469 3.4.2.4 3M4 Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses. e_samples &lt;- e_samples %&gt;% mutate(W2 = map_dbl(p_grid, rbinom, n=1, size=9)) mean(e_samples$W2 == 6) ## [1] 0.181 3.4.2.5 3M5 Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences (using both priors) to the true value p = 0.7. e2 &lt;- tibble(p_grid = seq(0,1,length.out=n), prior = case_when(p_grid &lt;0.5 ~ 0L, TRUE ~ 1L)) %&gt;% mutate(likelihood = dbinom(n_success, n_trials, prob = p_grid), posterior = prior*likelihood, posterior = posterior/sum(posterior)) e_samples2 &lt;- e2 %&gt;% slice_sample(n=1e4, weight_by= posterior, replace=TRUE) %&gt;% mutate(sample_number = 1:n()) # 事後分布 e2 %&gt;% ggplot(aes(x=p_grid, y=posterior))+ geom_line(size=1.5) # 90$HPDI hdi(e_samples2$p_grid, .width=0.9) ## [,1] [,2] ## [1,] 0.5005005 0.7137137 # サンプリング e_samples2 &lt;- e_samples2 %&gt;% mutate(W = map_dbl(p_grid, rbinom, n=1, size=9)) mean(e_samples2$W==6) ## [1] 0.2411 e_samples2 %&gt;% ggplot(aes(x=W))+ geom_histogram(binwidth=0.5,size=1/10) 3.4.2.6 3M6 Suppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this? single_sim &lt;- function(tosses, prior_type = c(&quot;uniform&quot;, &quot;step&quot;)) { prior_type &lt;- match.arg(prior_type) n_success &lt;- rbinom(1, size = tosses, prob = 0.7) e3 &lt;- tibble(p_grid =seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(likelihood = dbinom(n_success, size=tosses, prob = p_grid)) if(prior_type == &quot;uniform&quot;){ e3 &lt;- mutate(e3, prior =rep(1,1000)) } else if(prior_type == &quot;step&quot;){ e3 &lt;- mutate(e3, prior =case_when(p_grid &gt;.5 ~1L, TRUE ~0L)) } else { print(&quot;error&quot;) } e3 &lt;- e3 %&gt;% mutate(posterior = prior*likelihood, posterior = posterior/sum(posterior)) samples3 &lt;- slice_sample(e3,n=1e4, weight_by= posterior, replace=TRUE) interval &lt;- PI(samples3$p_grid, prob = 0.99) width &lt;- interval[2] - interval[1] } single_cond &lt;- function(tosses, prior_type, reps = 100) { tibble(tosses = tosses, prior_type = prior_type, width = map_dbl(seq_len(reps), ~single_sim(tosses = tosses, prior_type = prior_type))) } simulation &lt;- crossing(tosses = seq(1500, 3000, by = 100), prior_type = c(&quot;uniform&quot;, &quot;step&quot;)) %&gt;% pmap_dfr(single_cond, reps = 100) %&gt;% group_by(tosses, prior_type) %&gt;% summarise(avg = mean(width), .groups = &quot;drop&quot;) ggplot(simulation, aes(x=tosses,y=avg))+ geom_point()+ geom_line()+ facet_wrap(~prior_type) 3.4.3 3H The Hard problems here all use the data below. These data indicate the gender (male = 1, female = 0) of officially reported first and second born children in 100 two-child families. So for example, the first family in the data reported a boy (1) and then a girl (0). The second family reported a girl (0) and then a boy (1). The third family reported two girls. You can load these tow vectors into R’s memory by typing: data(homeworkch3) birth1 ## [1] 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 ## [38] 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 ## [75] 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 birth2 ## [1] 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 ## [38] 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 ## [75] 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 Use these vectors as data. So for example to compute the total number of boys born across all of these births, you could use: sum(birth1) + sum(birth2) ## [1] 111 3.4.3.1 3H1 Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability? data(&quot;homeworkch3&quot;) n_boy &lt;- sum(birth1)+sum(birth2) n_birth &lt;- length(birth1)+length(birth2) set.seed(100) n &lt;- 1000 baby &lt;- tibble(p_grid = seq(0,1,length.out =n), prior = 1) %&gt;% mutate(likelihood = dbinom(n_boy,n_birth, prob=p_grid), posterior = likelihood*prior/sum(likelihood*prior)) baby$p_grid[which.max(baby$posterior)] ## [1] 0.5545546 3.4.3.2 3H2 Using the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these sample to estimate the 50%, 89%, and 97% highest posterior density intervals. b_samples &lt;- slice_sample(baby, n=1e4, weight_by = posterior, replace=TRUE) library(tidybayes) HPDI(b_samples$p_grid,prob = c(0.5,0.89,0.97)) ## |0.97 |0.89 |0.5 0.5| 0.89| 0.97| ## 0.4824825 0.4994995 0.5265265 0.5725726 0.6076076 0.6296296 3.4.3.3 3H3 Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is prbably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome? b_samples %&gt;% mutate(boys = rbinom(1e4,200,prob=p_grid)) -&gt; b_samples b_samples %&gt;% ggplot(aes(x=boys))+ stat_histinterval(color =&quot;black&quot;, .width = c(0.66, 0.89), breaks = 100)+ geom_vline(aes(xintercept = n_boy), linetype = &quot;dashed&quot;, color = &quot;red&quot;) 3.4.3.4 3H4 Now compare 10,000 counts of boys from 100 simulated first borns only the number of boys in the first births, birth1. How does the model look in this light? b_samples %&gt;% mutate(boys = rbinom(1e4,100,prob=p_grid)) -&gt; b_samples2 b_samples2 %&gt;% ggplot(aes(x=boys))+ stat_histinterval(color =&quot;black&quot;, .width = c(0.66, 0.89), breaks = 50)+ geom_vline(aes(xintercept = sum(birth1)), linetype = &quot;dashed&quot;, color = &quot;red&quot;) 3.4.3.5 3H5 The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated conts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data? birth &lt;- as_tibble(cbind(birth1,birth2)) birth %&gt;% filter(birth1 == &quot;0&quot;) -&gt;birth set.seed(100) n &lt;- 1000 n_boy2 &lt;- sum(birth) n_birth2 &lt;- nrow(birth) b_samples3 &lt;- b_samples %&gt;% mutate(boys = rbinom(1e4, size=49,prob=p_grid)) b_samples3 %&gt;% ggplot(aes(x=boys))+ stat_histinterval(color =&quot;black&quot;, .width = c(0.66, 0.89), breaks = 50)+ geom_vline(aes(xintercept = sum(birth)), linetype = &quot;dashed&quot;, color = &quot;red&quot;) 女の子が生まれた後には、男の子が生まれやすい。 "],["geocentric-model.html", "4 Geocentric Model 4.1 Why normal distributions are normal 4.2 Normal by multiplication 4.3 Gaussian model of height 4.4 Grid approximationで事後分布を描く 4.5 Linear prediction 4.6 Curves from lines 4.7 Practice", " 4 Geocentric Model 4.1 Why normal distributions are normal ランダムウォークを繰り返すと、分布が正規分布にちかづいていく。 pos &lt;- crossing(person = 1:100, step = 0:16) %&gt;% mutate(dev = map_dbl(step, ~ifelse(.==0,0,runif(1,-1,1)))) pos &lt;- pos %&gt;% group_by(person) %&gt;% mutate(sum_dev = cumsum(dev)) %&gt;% ungroup() #Figure4.2 ##above pos %&gt;% ggplot(aes(x=step,y=sum_dev,group=person))+ geom_line(aes(color = person &lt; 2, alpha = person &lt; 2))+ geom_vline(xintercept=c(4,8,16), linetype=2)+ scale_color_manual(values = c(&quot;skyblue4&quot;, &quot;black&quot;))+ scale_alpha_manual(values = c(1/5, 1))+ scale_x_continuous(&quot;step number&quot;, breaks = seq(0,16,2))+ theme(legend.position = &quot;none&quot;) p1 &lt;- pos %&gt;% filter(step==4) %&gt;% ggplot(aes(x=sum_dev))+ geom_line(stat = &quot;density&quot;)+ labs(&quot;4 steps&quot;) p2 &lt;- pos %&gt;% filter(step==8) %&gt;% ggplot(aes(x=sum_dev))+ geom_line(stat = &quot;density&quot;)+ labs(&quot;8 steps&quot;) pos %&gt;% filter(step==16) %&gt;% summarise(sd = sd(sum_dev)) -&gt;sd p3 &lt;- pos %&gt;% filter(step==16) %&gt;% ggplot(aes(x=sum_dev))+ stat_function(fun = dnorm, args = list(mean = 0, sd = sd[[1]]), linetype = 2)+ geom_density(color = &quot;transparent&quot;, fill = &quot;dodgerblue3&quot;, alpha = 1/2) + labs(&quot;16 steps&quot;) library(patchwork) p1|p2|p3 &amp; coord_cartesian(xlim=c(-6,6)) 4.2 Normal by multiplication 掛け算になる場合でも、変化が小さければ正規分布に近づくことがある。 set.seed(4) prod(1 + runif(12, 0, 0.1)) ## [1] 1.774719 growth &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.1)))) ggplot(data = growth, aes(x = growth)) + geom_density() 成長にもたらす効果が小さいほど、効果の積が効果の和に近づくので、正規分布に近づく。 growth_big &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.5)))) growth_small &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.01)))) ggplot(data = growth_big, aes(x = growth)) + geom_density() ggplot(data = growth_small, aes(x = growth)) + geom_density() 効果が大きいときも、対数をとると正規分布に近づく。これは、対数では積と和が等しくなることによる。 log_growth_big &lt;- log(growth_big) ggplot(data = log_growth_big, aes(x = growth)) + geom_density() 4.3 Gaussian model of height クンサン族の身長データでモデリングを行う。 データは、Howellらが収集したデータ。身長は18歳以下では年齢と強く相関するので、ひとまず18歳以上を考える。 data(Howell1) (d &lt;- as_tibble(Howell1)) ## # A tibble: 544 × 4 ## height weight age male ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 152. 47.8 63 1 ## 2 140. 36.5 63 0 ## 3 137. 31.9 65 0 ## 4 157. 53.0 41 1 ## 5 145. 41.3 51 0 ## 6 164. 63.0 35 1 ## 7 149. 38.2 32 0 ## 8 169. 55.5 27 1 ## 9 148. 34.9 19 0 ## 10 165. 54.5 54 1 ## # … with 534 more rows str(d) ## tibble [544 × 4] (S3: tbl_df/tbl/data.frame) ## $ height: num [1:544] 152 140 137 157 145 ... ## $ weight: num [1:544] 47.8 36.5 31.9 53 41.3 ... ## $ age : num [1:544] 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int [1:544] 1 0 0 1 0 1 0 1 0 1 ... precis(d, hist = FALSE) ## mean sd 5.5% 94.5% ## height 138.2635963 27.6024476 81.108550 165.73500 ## weight 35.6106176 14.7191782 9.360721 54.50289 ## age 29.3443934 20.7468882 1.000000 66.13500 ## male 0.4724265 0.4996986 0.000000 1.00000 # 18歳以下に限定 d2 &lt;- d %&gt;% filter(age &gt;= 18) 身長についてモデリングを行う。 d2 %&gt;% ggplot(aes(x=height))+ geom_histogram(binwidth =5, fill=&quot;grey92&quot;,color=&quot;black&quot;)+ theme_bw() 身長は正規分布していると思われるため、モデル式は以下の通りとする。 \\(h_{i} \\sim Normal(\\mu, \\sigma)\\) 事前分布は以下のとおりとする。 \\(\\mu \\sim Normal(178, 20)\\) \\(\\sigma \\sim Uniform(0,50)\\) 事前分布を確認する。 # mu p1 &lt;- tibble(x = seq(from = 100, to = 250, by = .1)) %&gt;% ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) + geom_line() + scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) + labs(title = &quot;mu ~ dnorm(178, 20)&quot;,y = &quot;density&quot;) p1 #sigma p2 &lt;- tibble(x = seq(from = -10, to = 60, by = .1)) %&gt;% ggplot(aes(x = x, y = dunif(x, min=0,max=50))) + geom_line() + scale_x_continuous(breaks = seq(from = 0, to = 50, by = 10)) + labs(title = &quot;sigma ~ dunif(0,50)&quot;,y = &quot;density&quot;) p2 事前分布からサンプリングをしてみる。 n &lt;- 1e4 set.seed(4) sim &lt;- tibble(sample_mu = rnorm(n,178,20), sample_sigma = runif(n,0,50)) %&gt;% mutate(height = rnorm(n, sample_mu, sample_sigma)) p3 &lt;- sim %&gt;% ggplot(aes(x=height))+ geom_density(fill=&quot;grey33&quot;)+ scale_x_continuous(breaks = c(0, 73, 178, 283)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;height ~ dnorm(mu, sigma)&quot;) + theme(panel.grid = element_blank()) p3 \\(\\mu \\sim Normal(178, 100)\\) が事前分布だとすると… sim2 &lt;- tibble(sample_mu = rnorm(n, mean = 178, sd = 100), sample_sigma = runif(n, min = 0, max = 50)) %&gt;% mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma)) # compute the values we&#39;ll use to break on our x axis breaks &lt;- c(mean(sim2$height) - 3 * sd(sim2$height), 0, mean(sim2$height), mean(sim2$height) + 3 * sd(sim2$height)) %&gt;% round(digits = 0) text &lt;- tibble(height = 272-25 , y = .0013, label = &quot;tallest man&quot;, angle = 90) p4 &lt;- sim2 %&gt;% ggplot(aes(x = height)) + geom_density(fill = &quot;black&quot;, size = 0) + geom_vline(xintercept = 0, color = &quot;grey92&quot;) + geom_vline(xintercept = 272, color = &quot;grey92&quot;, linetype = 3) + geom_text(data = text, aes(y = y, label = label, angle = angle), color = &quot;grey92&quot;, size = 4) + scale_x_continuous(breaks = breaks) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;height ~ dnorm(mu, sigma)\\nmu ~ dnorm(178, 100)&quot;) + theme(panel.grid = element_blank()) p4 全部並べてみると…。 \\(\\mu\\)の事前分布の標準偏差が大きすぎると、身長の事前分布もかなり広くなってしまうことが分かる。身長が負の人や、250cmを越えるような人がいることを仮定していることになる。 (p1+xlab(&quot;mu&quot;)|p2+xlab(&quot;sigma&quot;))/(p3|p4) #身長0未満 sim2 %&gt;% count(height &lt; 0) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 × 3 ## `height &lt; 0` n percent ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 9587 95.9 ## 2 TRUE 413 4.13 #身長250以上 sim2 %&gt;% count(height &gt;=250) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 × 3 ## `height &gt;= 250` n percent ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 7443 74.4 ## 2 TRUE 2557 25.6 4.4 Grid approximationで事後分布を描く 計算には対数尤度を用い、その後戻す。 n &lt;- 200 d_grid &lt;- crossing(mu = seq(140,160,length.out=n), sigma = seq(4,9,length.out=n)) grid_function &lt;- function(mu,sigma){ dnorm(d2$height, mean = mu, sd = sigma, log = T) %&gt;% sum() } d_grid &lt;- d_grid %&gt;% mutate(LL = map2(mu, sigma, grid_function)) %&gt;% unnest(LL) %&gt;% mutate(prod = LL + dnorm(mu, 178,20,TRUE)+ dunif(sigma,0,50,TRUE)) %&gt;% mutate(prob = exp(prod-max(prod))) 事後同時分布を表すと以下のようになる。 d_grid %&gt;% ggplot(aes(x=mu, y=sigma, z = prob))+ geom_contour()+ labs(x=expression(mu),y=expression(sigma))+ coord_cartesian(xlim = range(d_grid$mu), ylim=range(d_grid$sigma))+ theme(panel.grid = element_blank()) d_grid %&gt;% ggplot(aes(x = mu, y = sigma, fill = prob)) + geom_raster(interpolate = F) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(x = expression(mu), y = expression(sigma)) + theme(panel.grid = element_blank()) #ちなみに d_grid %&gt;% filter(prob==max(prob)) ## # A tibble: 1 × 5 ## mu sigma LL prod prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 155. 7.74 -1219. -1228. 1 4.4.1 事後分布からサンプリングする set.seed(4) samples_d &lt;- d_grid %&gt;% slice_sample(n=1e4, weight_by = prob, replace= TRUE) \\(\\mu\\)と\\(\\sigma\\)の事後分布からのサンプルより # muの区間推定 bind_rows(median_hdi(samples_d$mu, .width=0.95), mean_qi(samples_d$mu, .width=0.95)) ## y ymin ymax .width .point .interval ## 1 154.5729 153.6683 155.2764 0.95 median hdi ## 2 154.6041 153.7688 155.3769 0.95 mean qi #sigmaの区間推定 bind_rows(median_hdi(samples_d$sigma, .width=0.95), mean_qi(samples_d$sigma, .width=0.95)) ## y ymin ymax .width .point .interval ## 1 7.768844 7.140704 8.296482 0.95 median hdi ## 2 7.769450 7.216080 8.371859 0.95 mean qi # 同時分布 samples_d %&gt;% ggplot(aes(x=mu, y=sigma))+ geom_point(size=0.9,alpha=1/30)+ labs(x=expression(mu[samples]),y=expression(sigma[samples]))+ theme(panel.grid = element_blank()) # それぞれの周辺事後分布 samples_d %&gt;% pivot_longer(cols = mu:sigma, names_to = &quot;param&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x=value))+ geom_histogram(binwidth=0.1,fill=&quot;grey33&quot;)+ scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank())+ facet_wrap(~param, scale=&quot;free&quot;,labeller = label_parsed) 4.4.2 rethinking set.seed(4) (d3 &lt;- sample(d2$height, size = 20)) ## [1] 147.3200 154.9400 168.9100 156.8450 165.7350 151.7650 165.7350 156.2100 ## [9] 144.7800 154.9400 151.1300 147.9550 149.8600 162.5600 161.9250 164.4650 ## [17] 160.9852 151.7650 163.8300 149.8600 n &lt;- 200 # note we&#39;ve redefined the ranges of `mu` and `sigma` d_grid &lt;- crossing(mu= seq(from = 150,to = 170,length.out= n), sigma =seq(from =4, to = 20,length.out= n)) grid_function &lt;- function(mu, sigma) { dnorm(d3, mean = mu, sd = sigma, log = T) %&gt;% sum() } d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = TRUE, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) d_grid_samples %&gt;% pivot_longer(mu:sigma) %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) 4.4.3 Finding posterior with brms quapとbrmsを用いてモデリングを行う。 # quap #flist &lt;- alist( #height ~ dnorm(mu,sigma), #mu ~ dnorm(178,20), #sigma ~ dunif(0,50) #) #m4.1 &lt;- quap(flist, data=d2) #precis(m4.1, prob=.95) # brms b4.1 &lt;- brm( data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178,20),class=Intercept), prior(cauchy(0,1), class=sigma)), iter=2000,warmup=1000,chains =4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.1&quot;) plot(b4.1) print(b4.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.61 0.41 153.79 155.40 1.00 2887 2135 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 7.76 0.29 7.21 8.33 1.00 3084 2478 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). b4.1$fit ## Inference for Stan model: model_c19008bf3f2b9c2a33b512b5e1155c58-202301301453-1-5f89bc. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## b_Intercept 154.61 0.01 0.41 153.79 154.34 154.61 154.88 155.40 ## sigma 7.76 0.01 0.29 7.21 7.56 7.75 7.94 8.33 ## lprior -9.16 0.00 0.08 -9.31 -9.21 -9.16 -9.11 -9.01 ## lp__ -1227.48 0.02 0.99 -1230.19 -1227.83 -1227.16 -1226.80 -1226.55 ## n_eff Rhat ## b_Intercept 2844 1 ## sigma 2963 1 ## lprior 2886 1 ## lp__ 1685 1 ## ## Samples were drawn using NUTS(diag_e) at 月 1 30 14:53:45 2023. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 続いて、\\(\\mu\\)の事前分布の標準偏差を0.1にしてみる。 かなり事前分布に引っ張られていることが分かる。 b4.1_2 &lt;- brm( data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178,0.1),class=Intercept), prior(cauchy(0,1), class=sigma)), iter=2000,warmup=1000,chains =4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.1_2&quot; ) plot(b4.1_2) b4.1_2$fit ## Inference for Stan model: model_103b53ece11c4e90bdd855611995b123-202301301453-1-4680b3. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## b_Intercept 177.86 0.00 0.10 177.67 177.80 177.86 177.93 178.06 ## sigma 24.56 0.02 0.94 22.83 23.91 24.54 25.19 26.50 ## lprior -6.89 0.03 1.50 -10.98 -7.54 -6.41 -5.76 -5.44 ## lp__ -1629.83 0.02 0.97 -1632.47 -1630.21 -1629.54 -1629.14 -1628.88 ## n_eff Rhat ## b_Intercept 3304 1 ## sigma 3542 1 ## lprior 2965 1 ## lp__ 1597 1 ## ## Samples were drawn using NUTS(diag_e) at 月 1 30 14:53:56 2023. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # 比較 rbind(summary(b4.1)$fixed, summary(b4.1_2)$fixed) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.6066 0.40866177 153.789 155.398 1.001184 2887.360 2135.154 ## Intercept1 177.8636 0.09863163 177.668 178.055 1.001800 3314.509 2476.818 4.4.4 事後分布からのサンプリング MCMCではすでにサンプリングを行っている。 post &lt;- posterior_samples(b4.1) # 分散・共分散行列 post %&gt;% dplyr::select(b_Intercept, sigma) %&gt;% cov() ## b_Intercept sigma ## b_Intercept 0.167004444 -0.002765798 ## sigma -0.002765798 0.081405458 # 相関行列 post %&gt;% dplyr::select(b_Intercept, sigma) %&gt;% cor() ## b_Intercept sigma ## b_Intercept 1.00000000 -0.02372082 ## sigma -0.02372082 1.00000000 # summary of the samples post %&gt;% pivot_longer(-lp__) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value), `2.5%` = quantile(value, probs = .025), `97.5%` = quantile(value, probs = .975)) ## # A tibble: 3 × 5 ## name mean sd `2.5%` `97.5%` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 155. 0.409 154. 155. ## 2 lprior -9.16 0.0767 -9.31 -9.01 ## 3 sigma 7.76 0.285 7.21 8.33 4.5 Linear prediction 身長と体重の関係をモデリングする。 両者の間には強い相関がある。 ggplot(d2,aes(x=weight,y=height))+ geom_point(shape=1,size=2)+ theme_bw()+ theme(panel.grid = element_blank()) 以下のモデルを考える。 \\(h_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\beta(x_{i}-\\bar{x})\\) \\(\\alpha \\sim Normal(172, 20)\\) \\(\\beta \\sim Normal(0, 10)\\) \\(\\sigma \\sim Uniform(0, 50)\\) 事前分布からサンプリングした\\(\\alpha\\)と\\(\\beta\\)について回帰直線を引いてみる。 set.seed(2971) n &lt;- 100 lines &lt;- tibble(n = 1:n, a = rnorm(n, mean = 178, sd = 20), b = rnorm(n, mean = 0, sd = 10)) %&gt;% tidyr::expand(nesting(n, a, b), weight = range(d2$weight)) %&gt;% mutate(height = a + b * (weight - mean(d2$weight))) head(lines) ## # A tibble: 6 × 5 ## n a b weight height ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 191. -7.06 31.1 289. ## 2 1 191. -7.06 63.0 63.5 ## 3 2 199. 0.839 31.1 187. ## 4 2 199. 0.839 63.0 214. ## 5 3 202. 3.93 31.1 147. ## 6 3 202. 3.93 63.0 272. # 直線を引く lines %&gt;% ggplot(aes(x=weight, y=height, group = n))+ geom_line(alpha=1/4)+ geom_hline(yintercept = c(0,272), linetype = c(2,1), size = 1/3)+ scale_x_continuous(&quot;weight&quot;, breaks = seq(30,60,10))+ scale_y_continuous(&quot;height&quot;, breaks = seq(-100,400,100))+ coord_cartesian(ylim = c(-100,400))+ ggtitle(&quot;b ~ dnorm(0,10)&quot;)+ theme_classic()+ theme(aspect.ratio=1) 身長と体重の関係はマイナスではないと分かっている場合、不適切である。 そこで、Log-Normalを用いる。 $ Log-Normal(0, 1)$ set.seed(4) tibble(b = rlnorm(1e4, mean =0, sd =1)) %&gt;% ggplot(aes(x=b))+ geom_density(fill=&quot;grey92&quot;)+ coord_cartesian(xlim=c(0,5))+ theme_classic()+ theme(aspect.ratio=0.8) # シミュレーション lines_2 &lt;- tibble(n = 1:n, a = rnorm(n, mean = 178, sd = 20), b = rlnorm(n, mean = 0, sd = 1)) %&gt;% tidyr::expand(nesting(n, a, b), weight = range(d2$weight)) %&gt;% mutate(height = a + b * (weight - mean(d2$weight))) text &lt;- tibble(weight = c(34, 43), height = c(0 - 25, 272 + 25), label = c(&quot;Embryo&quot;, &quot;World&#39;s tallest person (272 cm)&quot;)) lines_2 %&gt;% ggplot(aes(x=weight, y=height, group = n))+ geom_line(alpha=1/4)+ geom_hline(yintercept = c(0,272), linetype = c(2,1), size = 1/3)+ scale_x_continuous(&quot;weight&quot;, breaks = seq(30,60,10))+ scale_y_continuous(&quot;height&quot;, breaks = seq(-100,400,100))+ coord_cartesian(ylim = c(-100,400))+ ggtitle(&quot;b ~ dnorm(0,10)&quot;)+ theme_classic()+ geom_text(data = text, aes(label = label), size = 3)+ theme(aspect.ratio=1) 4.5.1 Finding the posterior distribution brmsで事後分布を求める。 quapとほとんど同じ結果が得られた。 d2 &lt;- d2 %&gt;% mutate(weight_c = weight - mean(weight)) b4.3 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20),clas =Intercept), prior(lognormal(0, 1), class = b,lb=0), prior(cauchy(0, 1), class = sigma)), iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 4, backend = &quot;cmdstanr&quot;, file=&quot;output/Chapter4/b4.3&quot;) summary(b4.3)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## Intercept 154.5946177 0.27665537 154.0470000 155.1260250 1.000155 4416.690 ## weight_c 0.9026185 0.04256141 0.8202094 0.9849504 1.000944 4799.085 ## Tail_ESS ## Intercept 3229.642 ## weight_c 3261.045 sample &lt;- posterior_samples(b4.3) sample %&gt;% dplyr::select(-lp__) %&gt;% pivot_longer(1:3) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value), `2.5%` = quantile(value, probs = .055), `97.5%` = quantile(value, probs = .945)) ## # A tibble: 3 × 5 ## name mean sd `2.5%` `97.5%` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 155. 0.277 154. 155. ## 2 b_weight_c 0.903 0.0426 0.834 0.970 ## 3 sigma 5.09 0.195 4.80 5.41 パラメータ間の相関はなさそう。 # 共分散 sample %&gt;% dplyr::select(-lp__) %&gt;% cov() %&gt;% round(digits=3) ## b_Intercept b_weight_c sigma lprior ## b_Intercept 0.077 0.000 0.000 0.004 ## b_weight_c 0.000 0.002 0.000 -0.002 ## sigma 0.000 0.000 0.038 -0.014 ## lprior 0.004 -0.002 -0.014 0.007 # ヒストグラムと散布図 pairs(b4.3) 4.5.2 Plotting posterior inference agaist the data 事後分布の平均を用いて回帰直線を引く。 d2 %&gt;% ggplot(aes(x=weight_c,y=height))+ geom_point(size=2,shape=1, color=&quot;royalblue&quot;)+ geom_abline(intercept = mean(sample$b_Intercept), slope = mean(sample$b_weight_c))+ theme_classic()+ theme(aspect.ratio=1) 最初のN行だけを用いてモデリングしてみる。 N &lt;- 10 b4.3_10 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20),class=Intercept), prior(lognormal(0,1),class =b,lb=0), prior(cauchy(0, 1),class = sigma)), iter = 11000, warmup = 10000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.3_10&quot;, cores = 4, seed = 4) N &lt;- 50 b4.3_50 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20),class=Intercept), prior(lognormal(0,1),class =b,lb=0), prior(cauchy(0, 1),class = sigma)), iter = 11000, warmup = 10000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.3_50&quot;, cores = 4, seed = 4) N &lt;- 150 b4.3_150 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20),class=Intercept), prior(lognormal(0,1),class =b,lb=0), prior(cauchy(0, 1),class = sigma)), iter = 11000, warmup = 10000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.3_150&quot;, cores = 4, seed = 4) それぞれの事後分布から20サンプルずつを抽出して回帰直線を書く。もともとのN数が多いほど、回帰直線のばらつきは小さくなっていることが分かる。 # 20サンプルを抽出 sample_10 &lt;- sample_n(posterior_samples(b4.3_10), size=20, replace=TRUE) %&gt;% mutate(n=rep(10,20)) sample_50 &lt;- sample_n(posterior_samples(b4.3_50), size=20, replace=TRUE) %&gt;% mutate(n=rep(50,20)) sample_150 &lt;- sample_n(posterior_samples(b4.3_150), size=20, replace=TRUE) %&gt;% mutate(n=rep(150,20)) sample_392 &lt;- sample_n(sample,size=20,replace=TRUE) %&gt;% mutate(n=rep(392,20)) sample_all &lt;- rbind(sample_10,sample_50,sample_150,sample_392) %&gt;% dplyr::select(-lp__,-sigma) %&gt;% rename(alpha = b_Intercept, beta = b_weight_c) for(i in c(10,50,150,392)){ sample_n &lt;- sample_all %&gt;% filter(n==i) p &lt;- d2 %&gt;% slice(1:i) %&gt;% ggplot(aes(x=weight_c,y=height))+ geom_point(size=2,shape=1, color=&quot;royalblue&quot;)+ geom_abline(intercept = sample_n$alpha, slope = sample_n$beta, size=1/3, alpha=1/3)+ theme_classic()+ theme(aspect.ratio=1)+ labs(subtitle = str_c(&quot;N =&quot;, i)) print(p) } 4.5.3 Plotting regression intervals and contours \\(\\mu\\)についての事後分布を確認する。 それぞれの体重（\\(x_{i}\\)）における\\(\\mu\\)の事後分布をサンプリングする。 sample &lt;- rename(sample,alpha=b_Intercept, beta = b_weight_c) xbar &lt;- mean(d2$weight) # xi = 50におけるmuの事後分布からのサンプル mu_at_50 &lt;- tibble(mu_50 = sample$alpha + sample$beta*(50-xbar)) (qi &lt;- mean_qi(mu_at_50, .width=.89)) ## # A tibble: 1 × 6 ## mu_50 .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 159. 159. 160. 0.89 mean qi ggplot(mu_at_50,aes(x=mu_50))+ geom_histogram() # 全てのxiについてmuの事後分布を算出 mu &lt;- fitted(b4.3, summary=F) # 任意のxiについても作成できる weight_seq &lt;- tibble(weight = 25:75) %&gt;% mutate(weight_c = weight - mean(d2$weight)) mu &lt;- fitted(b4.3, summary=F, newdata = weight_seq) %&gt;% data.frame() %&gt;% set_names(25:75) %&gt;% mutate(iter=1:4000) %&gt;% pivot_longer(-iter, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) 全サンプルをグラフ上に表すと、以下のようになる。 d2 %&gt;% ggplot(aes(x=weight, y=height))+ geom_point(data=mu %&gt;% filter(iter&lt;101), alpha=1/20, color=&quot;navyblue&quot;)+ coord_cartesian(xlim=c(30,60))+ theme(panel.grid=element_blank(), aspect.ratio =1) 89%信用区間の作図をすると mu_fit &lt;- fitted(b4.3, newdata = weight_seq, prob = c(.055, .5,.945)) %&gt;% data.frame() %&gt;% mutate(weight = 25:75) datatable(mu_fit) d2 %&gt;% ggplot(aes(x=weight,y=height))+ geom_point(color=&quot;navyblue&quot;, shape=1,size=3)+ geom_ribbon(data=mu_fit, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/5)+ geom_line(data=mu_fit, aes(x=weight,y=Estimate),size=0.5)+ coord_cartesian(xlim=c(30,65))+ theme(panel.grid = element_blank(), aspect.ratio =1)+ labs(title=&quot;89%信用区間&quot;) 4.5.4 Prediction intervals 事後分布から、新たにデータをシミュレートする。 pred &lt;- predict(b4.3, newdata = weight_seq, prob = c(0.055,0.5,0.945)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) # 作図 d2 %&gt;% ggplot(aes(x=weight,y=height))+ geom_point(color=&quot;navyblue&quot;, shape=1,size=3)+ geom_ribbon(data=pred, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/8)+ geom_ribbon(data=mu_fit, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/5)+ geom_line(data=pred, aes(x=weight,y=Estimate),size=0.5)+ coord_cartesian(xlim=c(30,65))+ theme(panel.grid = element_blank(), aspect.ratio =1)+ labs(title=&quot;89%予測区間&quot;) 4.6 Curves from lines 多項回帰やB-スプラインによって曲線回帰する方法を学ぶ。 4.6.1 Polynomial regression クンサン属の18歳以下も含めた身長・体重データを見てみると、曲線を描いていることが分かる。 (d) ## # A tibble: 544 × 4 ## height weight age male ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 152. 47.8 63 1 ## 2 140. 36.5 63 0 ## 3 137. 31.9 65 0 ## 4 157. 53.0 41 1 ## 5 145. 41.3 51 0 ## 6 164. 63.0 35 1 ## 7 149. 38.2 32 0 ## 8 169. 55.5 27 1 ## 9 148. 34.9 19 0 ## 10 165. 54.5 54 1 ## # … with 534 more rows d %&gt;% ggplot(aes(x=weight,y=height))+ geom_point(color=&quot;navyblue&quot;,size=3,shape=1,alpha=1/2)+ annotate(geom = &quot;text&quot;, x = 42, y = 115, label = &quot;This relation is\\nvisibly curved.&quot;, family = &quot;Times&quot;)+ theme(panel.grid = element_blank(), aspect.ratio=1) そこで、\\(x^2\\)項を含めたモデリングを行う。 モデル式は以下の通り。 \\(h_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x^2_{i}\\) \\(\\alpha \\sim Normal(178, 20)\\) \\(\\beta_{1} \\sim LogNormal(0,1)\\) \\(\\beta_{2} \\sim Normal(0,1)\\) \\(\\sigma \\sim Uniform(0,50)\\) 標準化を施してからモデル化をする。 d &lt;- d %&gt;% mutate(weight_s = scale(weight), weight_s2 = weight_s^2) b4.5 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + weight_s2, prior = c(set_prior(&quot;normal(178, 20)&quot;,class=&quot;Intercept&quot;), set_prior(&quot;lognormal(0, 1)&quot;,class=&quot;b&quot;, coef = &quot;weight_s&quot;), set_prior(&quot;normal(0, 1)&quot;, class = &quot;b&quot;, coef = &quot;weight_s2&quot;), set_prior(&quot;&quot;, class = &quot;sigma&quot;)), iter = 10000, warmup = 9000, chains =4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.5&quot;) plot(b4.5) summary(b4.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_s + weight_s2 ## Data: d (Number of observations: 544) ## Draws: 4 chains, each with iter = 10000; warmup = 9000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 146.05 0.37 145.30 146.77 1.00 3555 3213 ## weight_s 21.74 0.29 21.17 22.32 1.00 3211 2831 ## weight_s2 -7.79 0.28 -8.33 -7.24 1.00 3288 3067 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.81 0.18 5.47 6.17 1.00 3794 2663 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). パラメータ間の相関はありそう。 samples_b45 &lt;- posterior_samples(b4.5) pairs(b4.5) bind_rows(mean_qi(samples_b45), median_hdi(samples_b45)) %&gt;% data.frame() ## b_Intercept b_Intercept.lower b_Intercept.upper b_weight_s b_weight_s.lower ## 1 146.0455 145.304 146.773 21.74105 21.16601 ## 2 146.0455 145.298 146.760 21.74145 21.19660 ## b_weight_s.upper b_weight_s2 b_weight_s2.lower b_weight_s2.upper sigma ## 1 22.31633 -7.787424 -8.330521 -7.236676 5.809241 ## 2 22.34100 -7.786660 -8.330830 -7.238070 5.801040 ## sigma.lower sigma.upper lprior lprior.lower lprior.upper lp__ ## 1 5.465192 6.174071 -45.90548 -50.1711 -41.7887 -1772.039 ## 2 5.471860 6.178080 -45.84730 -50.1372 -41.7841 -1771.720 ## lp__.lower lp__.upper .width .point .interval ## 1 -1775.66 -1770.24 0.95 mean qi ## 2 -1774.84 -1770.07 0.95 median hdi samples_b45 %&gt;% dplyr::select(-lp__) %&gt;% cov() %&gt;% round(digits=3) ## b_Intercept b_weight_s b_weight_s2 sigma lprior ## b_Intercept 0.137 -0.045 -0.077 -0.006 -0.581 ## b_weight_s -0.045 0.086 0.043 0.002 0.321 ## b_weight_s2 -0.077 0.043 0.077 0.006 0.589 ## sigma -0.006 0.002 0.006 0.033 0.045 ## lprior -0.581 0.321 0.589 0.045 4.529 信用区間と予測区間を作図する。 weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %&gt;% mutate(weight_s2 = weight_s^2) fit_b45 &lt;- fitted(b4.5, newdata = weight_seq, prob =c(.055,0.5,.945)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) predict_b45 &lt;- predict(b4.5, newdata = weight_seq, prob =c(.055,0.5,.945)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) d %&gt;% ggplot(aes(x=weight_s,y=height))+ geom_point(color=&quot;navyblue&quot;, shape=1,size=3, alpha = 1/5)+ geom_ribbon(data=predict_b45, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/8)+ geom_ribbon(data=fit_b45, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/5)+ geom_line(data=predict_b45, aes(x=weight_s,y=Estimate),size=0.5)+ coord_cartesian(xlim=c(-2,2))+ theme(panel.grid = element_blank(), aspect.ratio =1)+ labs(title=&quot;quadratic&quot;) -&gt;p1 p1 続いて、3乗項を含めたモデルを考えてみる。 d &lt;- d %&gt;% mutate(weight_s = scale(weight), weight_s2 = weight_s^2, weight_s3 = weight_s^3) b4.5_2 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + weight_s2 + weight_s3, prior = c(set_prior(&quot;normal(178, 20)&quot;,class=&quot;Intercept&quot;), set_prior(&quot;lognormal(0, 1)&quot;,class=&quot;b&quot;, coef = &quot;weight_s&quot;), set_prior(&quot;normal(0, 1)&quot;, class = &quot;b&quot;, coef = &quot;weight_s2&quot;), set_prior(&quot;normal(0, 1)&quot;, class = &quot;b&quot;, coef = &quot;weight_s3&quot;), set_prior(&quot;&quot;, class = &quot;sigma&quot;)), iter = 10000, warmup = 9000, chains =4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.5_2&quot;) plot(b4.5_2) summary(b4.5_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_s + weight_s2 + weight_s3 ## Data: d (Number of observations: 544) ## Draws: 4 chains, each with iter = 10000; warmup = 9000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 146.39 0.31 145.78 146.99 1.00 4030 2627 ## weight_s 15.22 0.48 14.30 16.13 1.00 2750 2476 ## weight_s2 -6.19 0.26 -6.69 -5.69 1.00 3518 2604 ## weight_s3 3.58 0.22 3.15 4.03 1.00 2649 2367 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 4.86 0.15 4.58 5.15 1.00 4420 2881 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 回帰曲線を描いてみる。 weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %&gt;% mutate(weight_s2 = weight_s^2, weight_s3 = weight_s^3) fit_b45_2 &lt;- fitted(b4.5_2, newdata = weight_seq, prob =c(.055,0.5,.945)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) predict_b45_2 &lt;- predict(b4.5_2, newdata = weight_seq, prob =c(.055,0.5,.945)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) d %&gt;% ggplot(aes(x=weight_s,y=height))+ geom_point(color=&quot;navyblue&quot;, shape=1,size=3, alpha = 1/5)+ geom_ribbon(data=predict_b45_2, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/8)+ geom_ribbon(data=fit_b45_2, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/5)+ geom_line(data=predict_b45_2, aes(x=weight_s,y=Estimate),size=0.5)+ coord_cartesian(xlim=c(-2,2))+ theme(panel.grid = element_blank(), aspect.ratio =1)+ labs(title=&quot;cubic&quot;) -&gt;p2 library(patchwork) p1|p2 3乗項を含んだ方が当てはまりは良くなるが、解釈は難しい。 4.6.2 overthinking 元のスケールに戻すには？ at &lt;- c(-2, -1, 0, 1, 2) d %&gt;% ggplot(aes(x=weight_s,y=height))+ geom_point(color=&quot;navyblue&quot;, shape=1,size=3, alpha = 1/5)+ geom_ribbon(data=predict_b45_2, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/8)+ geom_ribbon(data=fit_b45_2, aes(y=Q50, ymin=Q5.5, ymax=Q94.5), fill=&#39;black&#39;, alpha=1/5)+ geom_line(data=predict_b45_2, aes(x=weight_s,y=Estimate),size=0.5)+ coord_cartesian(xlim=c(-2,2))+ theme(panel.grid = element_blank(), aspect.ratio =1)+ scale_x_continuous(&quot;元のスケールに変換&quot;, breaks = at, labels = round(at*sd(d$weight) + mean(d$weight), 1)) 4.6.3 Splines サクラの開花データを用いて学習する。 data(&quot;cherry_blossoms&quot;) d3 &lt;- cherry_blossoms head(d3) ## year doy temp temp_upper temp_lower ## 1 801 NA NA NA NA ## 2 802 NA NA NA NA ## 3 803 NA NA NA NA ## 4 804 NA NA NA NA ## 5 805 NA NA NA NA ## 6 806 NA NA NA NA precis(d3, hist = FALSE) ## mean sd 5.5% 94.5% ## year 1408.000000 350.8845964 867.77000 1948.23000 ## doy 104.540508 6.4070362 94.43000 115.00000 ## temp 6.141886 0.6636479 5.15000 7.29470 ## temp_upper 7.185151 0.9929206 5.89765 8.90235 ## temp_lower 5.098941 0.8503496 3.78765 6.37000 d3 %&gt;% ggplot(aes(x=year, y=doy))+ geom_point(size=3,shape=1,color=&quot;#ffb7c5&quot;, alpha=2/3)+ theme(panel.grid = element_blank(), panel.background = element_rect(fill = &quot;#4f455c&quot;)) ノットを15個とし、区間を16個に分割する。 rm(cherry_blossoms) detach(package:rethinking, unload = T) # 欠損値を除く d4 &lt;- d3 %&gt;% drop_na(doy) # knot数 n_knots &lt;- 15 knot_list &lt;- quantile(d4$year, probs = seq(0,1,length.out=15)) d4 %&gt;% ggplot(aes(x = year, y = doy)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_point(color = &quot;#ffb7c5&quot;, alpha = 1/2) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) 基底関数を求める。 自由度は3（3次のスプライン回帰）。 library(splines) B &lt;- bs(d4$year, knots = knot_list[-c(1,n_knots)], degree=3, intercept = TRUE) b &lt;- B %&gt;% data.frame() %&gt;% set_names(str_c(0, 1:9), 10:17) %&gt;% bind_cols(dplyr::select(d4, year)) %&gt;% pivot_longer(-year, names_to = &quot;basis_function&quot;, values_to = &quot;basis&quot;) b %&gt;% ggplot(aes(x=year,y=basis,group=basis_function))+ geom_line(color = &quot;#ffb7c5&quot;, alpha = 1/2, size = 1.5)+ geom_vline(xintercept=knot_list, color = &quot;white&quot;, alpha=1/2)+ ylab(&quot;basis value&quot;)+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank(), strip.background = element_rect(fill = scales::alpha(&quot;#ffb7c5&quot;, .25), color = &quot;transparent&quot;), strip.text = element_text(size = 8, margin = margin(0.1, 0, 0.1, 0, &quot;cm&quot;)))+ facet_wrap(~basis_function,ncol=1) モデル式は以下を考える。 \\(D_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\sum_{k=1}^K w_{k}B_{k,j}\\) \\(\\alpha \\sim Normal(100, 10)\\) \\(w_{j} \\sim Normal(0, 10)\\) \\(\\sigma \\sim Exponential(1)\\) brmsでモデリングを行う。 d5 &lt;- d4 %&gt;% mutate(B=B) glimpse(d5) ## Rows: 827 ## Columns: 6 ## $ year &lt;int&gt; 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894,… ## $ doy &lt;int&gt; 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 10… ## $ temp &lt;dbl&gt; NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.9… ## $ temp_upper &lt;dbl&gt; NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.… ## $ temp_lower &lt;dbl&gt; NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.5… ## $ B &lt;bs[,17]&gt; &lt;bs[26 x 17]&gt; b4.8 &lt;- brm(data = d5, family = gaussian, doy ~ 1+B, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.8&quot;) summary(b4.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: doy ~ 1 + B ## Data: d5 (Number of observations: 827) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 103.41 2.41 98.60 108.27 1.01 615 1127 ## B1 -3.00 3.88 -10.44 4.74 1.00 1382 2369 ## B2 -0.89 3.86 -8.39 6.76 1.01 1361 2409 ## B3 -1.09 3.58 -8.41 5.99 1.01 1283 2343 ## B4 4.74 2.89 -1.00 10.52 1.01 806 1660 ## B5 -0.85 2.92 -6.77 5.05 1.01 913 1603 ## B6 4.26 2.97 -1.68 10.13 1.01 821 1801 ## B7 -5.33 2.86 -11.24 0.29 1.01 867 1560 ## B8 7.76 2.84 2.25 13.26 1.01 779 1783 ## B9 -1.04 2.92 -6.87 4.67 1.01 867 1611 ## B10 2.96 3.00 -2.89 8.95 1.01 821 1754 ## B11 4.63 2.92 -1.37 10.26 1.01 877 1664 ## B12 -0.24 2.96 -5.87 5.50 1.01 937 1573 ## B13 5.52 2.90 -0.17 11.21 1.01 873 1815 ## B14 0.62 3.06 -5.36 6.62 1.01 1046 1674 ## B15 -0.86 3.36 -7.36 5.66 1.01 1106 1963 ## B16 -7.05 3.44 -13.60 -0.31 1.01 1146 2288 ## B17 -7.70 3.26 -14.17 -1.33 1.01 1117 1968 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.94 0.14 5.68 6.23 1.00 4215 2985 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). plot(b4.8) samples_b48 &lt;- posterior_samples(b4.8) それぞれの基底関数に重みづけしたものを作図してみる。 samples_b48 %&gt;% dplyr::select(-lp__, -sigma,-b_Intercept,-lprior) %&gt;% set_names(c(str_c(0,1:9),10:17)) %&gt;% pivot_longer(1:17, names_to = &quot;basis_function&quot;, values_to = &quot;basis_value&quot;) -&gt; samples_b48 samples_b48 %&gt;% group_by(basis_function) %&gt;% summarise(weight = mean(basis_value)) %&gt;% full_join(b, by = &quot;basis_function&quot;) -&gt; post_mean post_mean %&gt;% ggplot(aes(x=year, y = basis*weight, group=basis_function))+ geom_line(color = &quot;#ffb7c5&quot;, alpha = 1/2, size = 1.5)+ geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2)+ theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) すべてを合計して信用区間付きで図示する。 f_b48 &lt;- fitted(b4.8, prob = c(0.015,0.5,0.985)) %&gt;% data.frame() %&gt;% bind_cols(d4) f_b48 %&gt;% ggplot(aes(x=year, y = doy, ymin = Q1.5, ymax = Q98.5))+ geom_point(size=3, color = &quot;#ffb7c5&quot;, alpha = 1/2)+ geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2)+ geom_hline(yintercept = fixef(b4.8)[1, 1], color = &quot;white&quot;, linetype = 2)+ geom_ribbon(fill = &quot;white&quot;, alpha = 2/3)+ labs(x = &quot;year&quot;, y = &quot;day in year&quot;) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) 4.7 Practice 4.7.1 4M1 For the model definition below, simulate observed y values from the prior (not the posterior). $$ \\[\\begin{aligned} y_i &amp;\\sim Normal(\\mu, \\sigma)\\\\ \\mu &amp;\\sim Normal(0,10)\\\\ \\sigma &amp;\\sim Exponential(1) \\end{aligned}\\] $$ n_sim &lt;- 10000 sim &lt;- tibble(sigma_sim = rexp(n=n_sim, 1), mu_sim = rnorm(n_sim, 0, 10), y_sim = rnorm(n_sim, mu_sim,sigma_sim)) 4.7.2 4M2 Trasnlate the model just above into a brms formula. b4M2 &lt;- brm(data = sim, family = gaussian, formula = y_sim ~ 1, prior = c(prior(normal(0,10),class = Intercept), prior(exponential(1), class = sigma)), file = &quot;output/Chapter4/b4M2&quot;, backend = &quot;cmdstanr&quot;) 4.7.3 4M4 A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors. $$ \\[\\begin{aligned} h_{ij} &amp;\\sim Normal(\\mu_{ij},\\sigma)\\\\ \\mu_{ij} &amp;\\sim \\alpha + \\beta(y_j - \\bar{y})\\\\ \\alpha &amp;\\sim Normal(100,10)\\\\ \\beta &amp;\\sim Normal(0,10)\\\\ \\sigma &amp;\\sim Exponential(1) \\end{aligned}\\] $$ 身長は中央値なので，αは平均的な年（すなわち，2年目）の平均の身長を表す．身長はセンチメートルで測定され，標本はまだ成長過程にある子供たちであると仮定して，事前分布はNormal ( 100 , 10 ) を選んだ。 傾きは非常に曖昧である。ゼロを平均とし標準偏差を10とする事前分布は、成長（または縮小）の可能性の幅を表している。成長期には、身長の伸びは平均して6～13cm／年である。この標準偏差10は、成長速度が速い場合に予想される範囲を含んでいる。 最後に、σの指数事前分布は、平均0で標準偏差偏差を1と仮定している。事前の予測シミュレーションも、現在の仮定からすると、妥当な回帰直線を与えているように見える。 n &lt;- 50 tibble(group = seq_len(n), alpha = rnorm(n, 100, 10), beta = rnorm(n, 0, 10), sigma = rexp(n, 1)) %&gt;% tidyr::expand(nesting(group, alpha, beta, sigma), year = c(1, 2, 3)) %&gt;% mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma)) %&gt;% ggplot(aes(x = year, y = height, group = group)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Height&quot;) 4.7.4 4M5 Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How? 年が増えると必ず身長も増加するので，βは正になることが分かっている．したがって，事前分布はこれを反映するように，例えば対数正規分布を用いるべきである。 この事前分布では，身長の伸びの期待値は年間約3cmになりえ，89％最高密度区間は年間0.87cmから5.18cmである。 \\[ \\beta ∼ logNormal(1,0.5) \\] library(tidybayes) set.seed(123) samples &lt;- rlnorm(1e8, 1, 0.5) bounds &lt;- mean_hdi(samples, .width = 0.89) ggplot() + stat_function(data = tibble(x = c(0, 10)), mapping = aes(x = x), geom = &quot;line&quot;, fun = dlnorm, args = list(meanlog = 1, sdlog = 0.5)) + geom_ribbon(data = tibble(x = seq(bounds$ymin, bounds$ymax, 0.01)), aes(x = x, ymin = 0, ymax = dlnorm(x, 1, 0.5)), alpha = 0.8) + scale_x_continuous(breaks = seq(0, 10, 2)) + labs(x = expression(beta), y = &quot;Density&quot;) 対数正規事前分布を用いたもっともらしい直線の事前予測シミュレーションは、これらの事前分布が依然としてもっともらしい値を示していることを示している。ほとんどの線は、事前制約のために正である。しかし、平均値の周辺にばらつきがあるため、いくつかの線は低くなっている。もし、学生が身長を縮めることが本当に不可能であれば、このようなデータは測定誤差から生じるかもしれない。 n &lt;- 50 tibble(group = seq_len(n), alpha = rnorm(n, 100, 10), beta = rlnorm(n, 1, 0.5), sigma = rexp(n, 1)) %&gt;% tidyr::expand(nesting(group, alpha, beta, sigma), year = c(1, 2, 3)) %&gt;% mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma)) %&gt;% ggplot(aes(x = year, y = height, group = group)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Height&quot;) 4.7.5 4M6 Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors? 分散64cmは標準偏差8cmに相当する。現在の\\(\\sigma \\sim Exponential(1)\\) の事前分布では、確率的に8より大きい値にはならない。しかし、理論的には可能である。もしこの方法で本当に分散を抑制したいのであれば、一様分布\\(Uniform(0,8)\\)を事前分布を使用することができます。これは、64cmより大きい分散をもたらすすべての値を排除することになる。 4.7.6 4M7 Refit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models. b4.3で中心化を行わなかったら…。 結果はほとんど変わらない。 b4.3_p &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 20),clas =Intercept), prior(lognormal(0, 1), class = b,lb=0), prior(cauchy(0, 1), class = sigma)), iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 4, backend = &quot;cmdstanr&quot;, file=&quot;output/Chapter4/b4.3_p&quot;) 結果 summary(b4.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_c ## Data: d2 (Number of observations: 352) ## Draws: 4 chains, each with iter = 28000; warmup = 27000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.59 0.28 154.05 155.13 1.00 4417 3230 ## weight_c 0.90 0.04 0.82 0.98 1.00 4799 3261 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.09 0.20 4.73 5.49 1.00 4592 3189 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). summary(b4.3_p) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight ## Data: d2 (Number of observations: 352) ## Draws: 4 chains, each with iter = 28000; warmup = 27000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 114.02 1.92 110.30 117.83 1.00 3582 2723 ## weight 0.90 0.04 0.82 0.98 1.00 3584 2867 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.09 0.20 4.73 5.48 1.00 4119 2929 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 共分散 posterior_samples(b4.3) %&gt;% data.frame() %&gt;% dplyr::select(-lp__) %&gt;% cov() %&gt;% round(3) ## b_Intercept b_weight_c sigma lprior ## b_Intercept 0.077 0.000 0.000 0.004 ## b_weight_c 0.000 0.002 0.000 -0.002 ## sigma 0.000 0.000 0.038 -0.014 ## lprior 0.004 -0.002 -0.014 0.007 posterior_samples(b4.3_p) %&gt;% data.frame() %&gt;% dplyr::select(-lp__) %&gt;% cov() %&gt;% round(3) ## b_Intercept b_weight sigma lprior ## b_Intercept 3.671 -0.080 -0.006 0.086 ## b_weight -0.080 0.002 0.000 -0.002 ## sigma -0.006 0.000 0.039 -0.015 ## lprior 0.086 -0.002 -0.015 0.008 事後分布から回帰直線と信用区間、予測区間を作図。 weight_seq &lt;- tibble(weight = seq(25,75,length.out=100)) %&gt;% mutate(weight_c = weight - mean(d2$weight)) fit &lt;- bind_rows( fitted(b4.3, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% mutate(type = &quot;Centered&quot;), fitted(b4.3_p, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% mutate(type = &quot;Non-centered&quot;) ) pred &lt;- bind_rows( predict(b4.3, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% mutate(type = &quot;Centered&quot;), predict(b4.3_p, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% mutate(type = &quot;Non-centered&quot;) ) d2 %&gt;% ggplot(aes(x = weight, y = height))+ geom_point(alpha=2/3,shape=1,color=&quot;navyblue&quot;)+ geom_ribbon(data=pred, aes(y=Estimate,ymin = Q2.5, ymax =Q97.5), fill = &quot;black&quot;, alpha = 1/5)+ geom_ribbon(data=fit, aes(y=Estimate, ymin = Q2.5, ymax =Q97.5), fill = &quot;black&quot;, alpha = 3/5)+ geom_line(data=pred, aes(x=weight, y= Estimate), size=1.5)+ theme_bw()+ theme(legend.position = &quot;bottom&quot;)+ facet_wrap(~type) 4.7.7 4M8 In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights—change the standard deviation of the prior and watch what happens. What do you think the combination of know number and the prior on the weights controls? まず、この章のノット15個のSplineモデルをもう一度作成する。 library(splines) data(cherry_blossoms) cb_dat &lt;- cherry_blossoms %&gt;% drop_na(doy) # original m4.7 model knots_15 &lt;- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15)) B_15 &lt;- bs(cb_dat$year, knots = knots_15[-c(1, 15)], degree = 3, intercept = TRUE) cb_dat_15 &lt;- cb_dat %&gt;% mutate(B = B_15) b4M8 &lt;- brm(doy ~ 1 + B, data = cb_dat_15, family = gaussian, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4M8&quot;) モデルの結果を可視化する。 original_draws &lt;- cb_dat_15 %&gt;% add_epred_draws(b4M8) %&gt;% summarize(mean_hdi(.epred, .width = 0.89), .groups = &quot;drop&quot;) ggplot(original_draws, aes(x = year, y = doy)) + geom_vline(xintercept = knots_15, alpha = 0.5) + geom_hline(yintercept = fixef(b4M8)[1, 1], linetype = &quot;dashed&quot;) + geom_point(alpha = 0.5) + geom_ribbon(aes(ymin = ymin, ymax = ymax), fill = &quot;#009FB7&quot;, alpha = 0.8) + labs(x = &quot;Year&quot;, y = &quot;Day in Year&quot;) 次に2つ追加でモデリングする。一つは、ノット数を30にしたもの、もう一つは事前分布の標準偏差を狭くしたものである。 30ノット knots_30 &lt;- quantile(cb_dat$year, probs = seq(0, 1, length.out = 30)) B_30 &lt;- bs(cb_dat$year, knots = knots_30[-c(1, 30)], degree = 3, intercept = TRUE) cb_dat_30 &lt;- cb_dat %&gt;% mutate(B = B_30) b4M8_b &lt;- brm(doy ~ 1 + B, data = cb_dat_30, family = gaussian, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4M8_b&quot;) 事前分布を狭く b4M8_c &lt;- brm(doy ~ 1 + B, data = cb_dat_30, family = gaussian, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 2), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4M8_c&quot;) それぞれのモデルを可視化して比較する。ノット数を増やすとより回帰曲線がくねくねになる。また、事前分布の幅が狭くなると、事後分布の信用区間も狭くなる。 spline_15 &lt;- original_draws %&gt;% dplyr::select(-B) %&gt;% mutate(knots = &quot;15 knots (original model)&quot;) spline_30 &lt;- cb_dat_30 %&gt;% add_epred_draws(b4M8_b) %&gt;% summarize(mean_hdi(.epred, .width = 0.89), .groups = &quot;drop&quot;) %&gt;% dplyr::select(-B) %&gt;% mutate(knots = &quot;30 knots&quot;) spline_30p &lt;- cb_dat_30 %&gt;% add_epred_draws(b4M8_c) %&gt;% summarize(mean_hdi(.epred, .width = 0.89), .groups = &quot;drop&quot;) %&gt;% dplyr::select(-B) %&gt;% mutate(knots = &quot;30 knots; Tight prior&quot;) all_splines &lt;- bind_rows(spline_15, spline_30, spline_30p) # make plot ggplot(all_splines, aes(x = year, y = doy)) + geom_point(alpha = 0.5) + geom_ribbon(aes(ymin = ymin, ymax = ymax), fill = &quot;#009FB7&quot;, alpha = 0.8) + facet_wrap(~knots, ncol = 1) + labs(x = &quot;Year&quot;, y = &quot;Day in Year&quot;) 4.7.8 4H1 The weights listed below were recored in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table, below, using model-based predictions. weight_uk &lt;- tibble(weight = c(46.95, 43.72, 64.78, 32.59, 54.63)) %&gt;% mutate(weight_c = weight - mean(d2$weight)) predict(b4.3, newdata = weight_uk, prob=c(.055,.945)) ## Estimate Est.Error Q5.5 Q94.5 ## [1,] 156.2997 5.088868 148.0915 164.4584 ## [2,] 153.5451 5.034013 145.3749 161.6047 ## [3,] 172.4895 5.224621 164.2048 180.7326 ## [4,] 143.2987 5.066228 135.2979 151.3203 ## [5,] 163.2367 5.133487 155.0562 171.2982 4.7.9 4H2 Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it. 4.7.9.1 a Fit a linear regression to these data, using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets? d_y &lt;- d %&gt;% filter(age &lt; 18) %&gt;% mutate(weight_c = weight - mean(weight)) b4.3_y &lt;- brm( data =d_y, family = gaussian, formula = height ~ weight_c, prior = c(prior(normal(150,20), class= &quot;Intercept&quot;), prior(lognormal(0,1), class = &quot;b&quot;), prior(exponential(1), class = &quot;sigma&quot;)), iter = 10000, warmup=9000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.3_y&quot; ) print(b4.3_y) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ weight_c ## Data: d_y (Number of observations: 192) ## Draws: 4 chains, each with iter = 10000; warmup = 9000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 108.36 0.60 107.15 109.58 1.00 4077 3040 ## weight_c 2.72 0.07 2.59 2.85 1.00 4093 2875 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 8.35 0.42 7.60 9.23 1.00 3999 2705 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b4.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_c ## Data: d2 (Number of observations: 352) ## Draws: 4 chains, each with iter = 28000; warmup = 27000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.59 0.28 154.05 155.13 1.00 4417 3230 ## weight_c 0.90 0.04 0.82 0.98 1.00 4799 3261 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.09 0.20 4.73 5.49 1.00 4592 3189 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.7.9.2 b Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% interval for the mean. Also superimpose the 89% interval for predicted heights. range(d_y$weight) ## [1] 4.252425 44.735511 seq_weighty &lt;- tibble(weight = seq(0,50,length.out=100)) %&gt;% mutate(weight_c = weight-mean(d_y$weight)) predict_b42y &lt;- predict(b4.3_y, newdata = seq_weighty, prob = c(.055,.5,.945)) %&gt;% as_tibble() %&gt;% bind_cols(seq_weighty) fit_b42y &lt;- fitted(b4.3_y, newdata = seq_weighty, prob = c(.055,.5,.945)) %&gt;% as_tibble() %&gt;% bind_cols(seq_weighty) d_y %&gt;% ggplot(aes(x=weight,y=height))+ geom_point(size=3,shape=1,alpha=1/2, color=&quot;navyblue&quot;)+ geom_ribbon(data=fit_b42y,aes(y=Estimate, ymin=Q5.5, ymax=Q94.5), fill = &quot;black&quot;, alpha = 3/8)+ geom_ribbon(data=predict_b42y,aes(y=Estimate, ymin=Q5.5, ymax=Q94.5), fill = &quot;black&quot;, alpha = 3/8)+ geom_line(data=predict_b42y,aes(x=weight,y=Estimate), size=0.5, color=&quot;black&quot;)+ coord_cartesian(xlim = c(0,50))+ labs(title = &quot;below 18 years old&quot;)+ theme_bw()+ theme(aspect.ratio=1) 4.7.10 4H3 Suppose a colleauge of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens. 4.7.10.1 a Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Can you interpret the resulting estimates? 体重の自然対数をとってみる。 直線に近いように見える。 d_log &lt;- d %&gt;% mutate(weight_log = log(weight)) d_log %&gt;% ggplot(aes(x=weight_log,y=height))+ geom_point() モデリングする。 b4.3_log &lt;- brm( data =d_log, family = gaussian, formula = height ~ weight_log, prior = c(prior(normal(150,20), class= &quot;Intercept&quot;), prior(lognormal(0,1), class = &quot;b&quot;), prior(exponential(1), class = &quot;sigma&quot;)), iter = 10000, warmup=9000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/b4.3_log&quot; ) print(b4.3_log) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ weight_log ## Data: d_log (Number of observations: 544) ## Draws: 4 chains, each with iter = 10000; warmup = 9000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -23.77 1.33 -26.40 -21.22 1.00 3802 3125 ## weight_log 47.07 0.38 46.34 47.85 1.00 3852 3044 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.13 0.16 4.83 5.45 1.00 3977 2640 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.7.10.2 b Begin with this plot: plot( height ~ weight , data = Howell1 ). Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% interval for the mean, and (3) the 97% interval for predicted heights. 作図する。 range(d_log$weight) ## [1] 4.252425 62.992589 seq_weightl &lt;- tibble(weight = seq(1,75,length.out=120)) %&gt;% mutate(weight_log = log(weight)) predict_b42l &lt;-predict(b4.3_log,newdata = seq_weightl, prob = c(.015,.5,.985)) %&gt;% as_tibble() %&gt;% bind_cols(seq_weightl) fit_b42l &lt;- fitted(b4.3_log, newdata = seq_weightl, prob = c(.015,.5,.985)) %&gt;% as_tibble() %&gt;% bind_cols(seq_weightl) d_log %&gt;% ggplot(aes(x=weight,y=height))+ geom_point(size=1,alpha=1/2, color=&quot;navyblue&quot;)+ geom_ribbon(data=fit_b42l,aes(y=Estimate, ymin=Q1.5, ymax=Q98.5), fill = &quot;black&quot;, alpha = 1/2)+ geom_ribbon(data=predict_b42l,aes(y=Estimate, ymin=Q1.5, ymax=Q98.5), fill = &quot;black&quot;, alpha = 1/6)+ geom_line(data=predict_b42l,aes(x=weight,y=Estimate), size=0.5, color=&quot;black&quot;)+ coord_cartesian(xlim = c(0,50))+ labs(title = &quot;using logarithm of weight&quot;)+ theme_bw()+ theme(aspect.ratio=1) 4.7.11 4H4 Plot the prior predictive distribution for the parabolic polynomial regression model in the chapter. You can modify the code that plots the linear regression prior predictive distribution. Can you modify the prior distributions of \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\) so that the prior predictions stay within the biologically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to keep the curves consistent with what you know about height and weight, before seeing these exact data. 教科書での多項式モデルは以下のように定義されている。 $$ \\[\\begin{aligned} h_{i} &amp;\\sim Normal(\\mu, \\sigma)\\\\ \\mu_i &amp;\\sim \\alpha + \\beta_1 x_i + \\beta_2x_i^2\\\\ \\alpha &amp;\\sim Normal(178,20)\\\\ \\beta_1 &amp;\\sim Log-Normal(0,1)\\\\ \\beta_2 &amp;\\sim Normal(0,1)\\\\ \\sigma &amp;\\sim Uniform(0,50) \\end{aligned}\\] $$ このモデルでの事前分布からサンプリングを行って回帰曲線を描くと以下のようになる。明らかに、生態学的に妥当な範囲を超えてしまっている。 n &lt;- 1000 sim_x &lt;- tibble(x = seq(25,75,length.out=100)) param_prior &lt;- tibble(alpha = rnorm(n, 178,20), beta_1 = rlnorm(n,0,1), beta_2 = rnorm(n,0,1), sigma = runif(n,0,50)) sim &lt;- crossing(sim_x,param_prior) %&gt;% mutate(height = alpha + beta_1*x + beta_2*x^2, group = rep(1:1000, times = 100)) sim %&gt;% ggplot(aes(x=x,y=height, group = group))+ geom_line(alpha= 0.1)+ geom_hline(yintercept = c(0, 272), linetype = 2:1, color = &quot;red4&quot;) + annotate(geom = &quot;text&quot;, x = 25, y = 0, hjust = 0, vjust = 1, label = &quot;胚&quot;, fontface = &quot;bold&quot;) + annotate(geom = &quot;text&quot;, x = 25, y = 272, hjust = 0, vjust = 0, label = &quot;世界一身長が高い人 (272cm)&quot;, fontface = &quot;bold&quot;) + coord_cartesian(ylim = c(-25, 300)) + labs(x = &quot;Weight&quot;, y = &quot;Height&quot;) 例えば以下のようなモデルにすると、生態学的に妥当な範囲に収まる。 $$ \\[\\begin{aligned} h_{i} &amp;\\sim Normal(\\mu, \\sigma)\\\\ \\mu_i &amp;\\sim \\alpha + \\beta_1 x_i + \\beta_2x_i^2\\\\ \\alpha &amp;\\sim Normal(-190,5)\\\\ \\beta_1 &amp;\\sim Log-Normal(13,0.2)\\\\ \\beta_2 &amp;\\sim Normal(-0.13,0.1)\\\\ \\sigma &amp;\\sim Uniform(0,50) \\end{aligned}\\] $$ n &lt;- 1000 sim_x &lt;- tibble(x = seq(25,75,length.out=100)) param_prior &lt;- tibble(alpha = rnorm(n, -190, 5), beta_1 = rnorm(n, 13, 0.2), beta_2 = runif(n, -0.13, -0.1), sigma = runif(n,0,50)) sim &lt;- crossing(sim_x,param_prior) %&gt;% mutate(height = alpha + beta_1*x + beta_2*x^2, group = rep(1:1000, times = 100)) sim %&gt;% ggplot(aes(x=x,y=height, group = group))+ geom_line(alpha= 0.1)+ geom_hline(yintercept = c(0, 272), linetype = 2:1, color = &quot;red4&quot;) + annotate(geom = &quot;text&quot;, x = 25, y = 0, hjust = 0, vjust = 1, label = &quot;胚&quot;, fontface = &quot;bold&quot;) + annotate(geom = &quot;text&quot;, x = 25, y = 272, hjust = 0, vjust = 0, label = &quot;世界一身長が高い人 (272cm)&quot;, fontface = &quot;bold&quot;) + coord_cartesian(ylim = c(-25, 300)) + labs(x = &quot;Weight&quot;, y = &quot;Height&quot;) 4.7.12 4H5 Return to data(cherry_blossoms) and model the association between blossom date (doy) and March temperature (temp). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. How well does temperature trend predict the blossom trend? cherry_blossomのデータで、気温と開花日の関係を線形モデル、多項回帰、スプライン回帰してみる。 data(&quot;cherry_blossoms&quot;) cb_temp &lt;- cherry_blossoms %&gt;% drop_na(doy, temp) %&gt;% mutate(temp_c = temp - mean(temp), temp_s = temp_c / sd(temp), temp_s2 = temp_s ^ 2, temp_s3 = temp_s ^ 3) # 線形モデル lin_mod &lt;- brm(doy ~ 1 + temp_c, data = cb_temp, family = gaussian, prior = c(prior(normal(100,10),class=Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = (&quot;output/Chapter4/lin_mod&quot;)) # 2項モデル qad_mod &lt;- brm(doy ~ 1 + temp_s + temp_s2, data = cb_temp, family = gaussian, prior= c(prior(normal(100, 10),class=Intercept), prior(normal(0, 10), class = b, coef = &quot;temp_s&quot;), prior(normal(0, 1), class = b, coef = &quot;temp_s2&quot;), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/qad_mod&quot;) # cubic model cub_mod &lt;- brm(doy ~ 1 + temp_s + temp_s2 + temp_s3, data = cb_temp,family = gaussian, prior = c(prior(normal(100,10),class=Intercept), prior(normal(0, 10), class = b, coef = &quot;temp_s&quot;), prior(normal(0, 1), class = b, coef = &quot;temp_s2&quot;), prior(normal(0, 1), class = b, coef = &quot;temp_s3&quot;), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/cub_mod&quot;) # spline model spl_mod &lt;- brm(doy ~ 1 + s(temp, bs = &quot;bs&quot;), data = cb_temp, family = gaussian, prior=c(prior(normal(100,10),class=Intercept), prior(normal(0, 10), class = b), prior(normal(0, 10), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234, control = list(adapt_delta = 0.9999), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter4/spl_mod&quot;) モデルの結果を作図すると以下のようになる。 library(modelr) grid &lt;- cb_temp %&gt;% data_grid(temp = seq_range(temp, 100)) %&gt;% mutate(temp_c = temp - mean(cb_temp$temp), temp_s = temp_c / sd(cb_temp$temp), temp_s2 = temp_s ^ 2, temp_s3 = temp_s ^ 3) fits &lt;- bind_rows( add_epred_draws(grid, lin_mod) %&gt;% mean_qi(.width = c(0.67, 0.89, 0.97)) %&gt;% mutate(model = &quot;Linear&quot;), add_epred_draws(grid, qad_mod) %&gt;% mean_qi(.width = c(0.67, 0.89, 0.97)) %&gt;% mutate(model = &quot;Quadratic&quot;), add_epred_draws(grid, cub_mod) %&gt;% mean_qi(.width = c(0.67, 0.89, 0.97)) %&gt;% mutate(model = &quot;Cubic&quot;), add_epred_draws(grid, spl_mod) %&gt;% mean_qi(.width = c(0.67, 0.89, 0.97)) %&gt;% mutate(model = &quot;Spline&quot;) ) %&gt;% mutate(model = factor(model, levels = c(&quot;Linear&quot;, &quot;Quadratic&quot;, &quot;Cubic&quot;, &quot;Spline&quot;))) preds &lt;- bind_rows( add_predicted_draws(grid, lin_mod) %&gt;% mean_qi(.width = 0.89) %&gt;% mutate(model = &quot;Linear&quot;), add_predicted_draws(grid, qad_mod) %&gt;% mean_qi(.width = 0.89) %&gt;% mutate(model = &quot;Quadratic&quot;), add_predicted_draws(grid, cub_mod) %&gt;% mean_qi(.width = 0.89) %&gt;% mutate(model = &quot;Cubic&quot;), add_predicted_draws(grid, spl_mod) %&gt;% mean_qi(.width = 0.89) %&gt;% mutate(model = &quot;Spline&quot;) ) %&gt;% mutate(model = factor(model, levels = c(&quot;Linear&quot;, &quot;Quadratic&quot;, &quot;Cubic&quot;, &quot;Spline&quot;))) ggplot(cb_temp, aes(x = temp)) + facet_wrap(~model, nrow = 2) + geom_point(aes(y = doy), alpha = 0.2) + geom_ribbon(data = preds, aes(ymin = .lower, ymax = .upper), alpha = 0.2) + geom_lineribbon(data = fits, aes(y = .epred, ymin = .lower, ymax = .upper), size = .6) + scale_fill_brewer(palette = &quot;Blues&quot;, breaks = c(0.67, 0.89, 0.97)) + labs(x = &quot;March Temperature&quot;, y = &quot;Day in Year&quot;) + theme(legend.position = &quot;bottom&quot;) 4.7.13 4H6 Simulate the prior predictive distribution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weights is doing? cb_dat &lt;- cherry_blossoms %&gt;% drop_na(doy) knot_list &lt;- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15)) B &lt;- bs(cb_dat$year, knots = knot_list[-c(1, 15)], degree = 3, intercept = TRUE) 教科書のモデルから事前予測分布を描画すると以下のようになる。 n &lt;- 50 tibble(.draw = seq_len(n), alpha = rnorm(n, 100, 10), w = purrr::map(seq_len(n), function(x, knots) { w &lt;- rnorm(n = knots + 2, 0, 10) return(w) }, knots = 15)) %&gt;% mutate(mu = map2(alpha, w, function(alpha, w, b) { res &lt;- b %*% w res &lt;- res + alpha res &lt;- res %&gt;% as_tibble(.name_repair = ~&quot;.value&quot;) %&gt;% mutate(year = cb_dat$year, .before = 1) return(res) }, b = B)) %&gt;% unnest(cols = mu) %&gt;% ggplot(aes(x = year, y = .value)) + geom_vline(xintercept = knot_list, alpha = 0.5) + geom_line(aes(group = .draw)) + expand_limits(y = c(60, 140)) + labs(x = &quot;Year&quot;, y = &quot;Day in Year&quot;) ここで、４M８でやったようにwの事前分布を\\(Normal(0,2)\\)にする。すると、うねうねが小さくなる。 n &lt;- 50 tibble(.draw = seq_len(n), alpha = rnorm(n, 100, 10), w = purrr::map(seq_len(n), function(x, knots) { w &lt;- rnorm(n = knots + 2, 0, 1) return(w) }, knots = 15)) %&gt;% mutate(mu = map2(alpha, w, function(alpha, w, b) { res &lt;- b %*% w res &lt;- res + alpha res &lt;- res %&gt;% as_tibble(.name_repair = ~&quot;.value&quot;) %&gt;% mutate(year = cb_dat$year, .before = 1) return(res) }, b = B)) %&gt;% unnest(cols = mu) %&gt;% ggplot(aes(x = year, y = .value)) + geom_vline(xintercept = knot_list, alpha = 0.5) + geom_line(aes(group = .draw)) + expand_limits(y = c(60, 140)) + labs(x = &quot;Year&quot;, y = &quot;Day in Year&quot;) "],["the-many-variables-the-spurious-waffles.html", "5 The many variables &amp; the spurious waffles 5.1 Spurious association 5.2 Masked relationship 5.3 Categorical variables 5.4 Practice", " 5 The many variables &amp; the spurious waffles 5.1 Spurious association 結婚率と離婚率の間の因果関係を推論する。 結婚率と離婚率は正に相関するが、これは因果関係を表すだろうか？ data(WaffleDivorce) d &lt;- WaffleDivorce head(d) ## Location Loc Population MedianAgeMarriage Marriage Marriage.SE Divorce ## 1 Alabama AL 4.78 25.3 20.2 1.27 12.7 ## 2 Alaska AK 0.71 25.2 26.0 2.93 12.5 ## 3 Arizona AZ 6.33 25.8 20.3 0.98 10.8 ## 4 Arkansas AR 2.92 24.3 26.4 1.70 13.5 ## 5 California CA 37.25 26.8 19.1 0.39 8.0 ## 6 Colorado CO 5.03 25.7 23.5 1.24 11.6 ## Divorce.SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860 ## 1 0.79 128 1 435080 964201 0.45 ## 2 2.05 0 0 0 0 0.00 ## 3 0.74 18 0 0 0 0.00 ## 4 1.22 41 1 111115 435450 0.26 ## 5 0.24 0 0 0 379994 0.00 ## 6 0.94 11 0 0 34277 0.00 ちなみにWaffleHouseの店舗数と離婚率の関係 d %&gt;% ggplot(aes(x=WaffleHouses/Population, y = Divorce))+ geom_point(size=2,color = &quot;navyblue&quot;,shape=1)+ geom_text_repel(aes(label=Loc),color=&quot;black&quot;,size=3)+ geom_smooth(method = &quot;lm&quot;,color=&quot;black&quot;,size=1)+ theme_bw()+ theme(panel.grid = element_blank(), aspect.ratio = 0.8) 離婚率と結婚率、結婚年齢の関係を考える。 d %&gt;% ggplot(aes(x=Marriage,y=Divorce))+ geom_point(size=3,shape=1,color=&quot;navyblue&quot;)+ geom_smooth(method = &quot;lm&quot;,color=&quot;black&quot;,size=1)+ theme_bw()+ theme(panel.grid = element_blank(), aspect.ratio = 0.8)+ xlab(&quot;Marriage rate&quot;)+ ylab(&quot;Divorce rate&quot;) -&gt; p1 d %&gt;% ggplot(aes(x=MedianAgeMarriage,y=Divorce))+ geom_point(size=3,shape=1,color=&quot;navyblue&quot;)+ geom_smooth(method = &quot;lm&quot;,color=&quot;black&quot;,size=1)+ theme_bw()+ theme(panel.grid = element_blank(), aspect.ratio = 0.8)+ xlab(&quot;Median age marriage&quot;)+ ylab(&quot;Divorce rate&quot;) -&gt; p2 d %&gt;% ggplot(aes(x=MedianAgeMarriage,y=Marriage))+ geom_point(size=3,shape=1,color=&quot;navyblue&quot;)+ geom_smooth(method = &quot;lm&quot;,color=&quot;black&quot;,size=1)+ theme_bw()+ theme(panel.grid = element_blank(), aspect.ratio = 0.8)+ xlab(&quot;Median age marriage&quot;)+ ylab(&quot;Marriage rate&quot;) -&gt; p3 p1|p2|p3 データを標準化する。 d %&gt;% mutate(D = standardize(Divorce), M = standardize(Marriage), A = standardize(MedianAgeMarriage)) -&gt;d # ちなみに sd(d$Divorce) ## [1] 1.820814 sd(d$Marriage) ## [1] 3.797905 sd(d$MedianAgeMarriage) ## [1] 1.24363 まずは、離婚率と結婚年齢の関係を考える。 以下のモデルを考える。 \\(D_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha + \\beta_{A}A_{i}\\) \\(\\alpha \\sim Normal(0, 0.2)\\) \\(\\beta_{A} \\sim Normal(0, 0.05)\\) \\(\\sigma \\sim Exponential(1)\\) モデルを書く。 b5.1 &lt;- brm(data = d, family = gaussian, formula = D ~ 1 + A, prior = c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.1&quot;) summary(b5.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D ~ 1 + A ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.10 -0.19 0.20 1.00 4144 3159 ## A -0.57 0.12 -0.80 -0.33 1.00 4041 3081 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.82 0.08 0.68 1.00 1.00 3748 3033 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 事前分布からのサンプリングを行う。 prior_51 &lt;- prior_samples(b5.1) head(prior_51) ## Intercept b sigma ## 1 0.149090 -0.1981440 2.554740 ## 2 -0.303870 -0.0584441 1.322460 ## 3 -0.127389 -0.1347680 0.711124 ## 4 0.114389 -0.5152620 0.860517 ## 5 0.127717 -0.1790560 0.729930 ## 6 0.290030 -0.1961800 0.332693 prior_51 %&gt;% data.frame() %&gt;% slice_sample(n=50) %&gt;% rownames_to_column(&quot;draw&quot;) %&gt;% tidyr::expand(nesting(draw,Intercept,b), a=c(-2,2)) %&gt;% mutate(d = Intercept + b*a) -&gt; prior_51 prior_51 %&gt;% ggplot(aes(x=a,y=d,group=draw))+ geom_line(alpha = 1/2)+ scale_x_continuous(&quot;Median age mariage (sd)&quot;, breaks = seq(-2,2,by=1))+ scale_y_continuous(&quot;Divorce rate(std)&quot;, breaks = seq(-2,2,by=1))+theme_bw()+ theme(panel.grid = element_blank(), aspect.ratio = 1) 各パラメータの事後分布は以下の通り。 post_51 &lt;- posterior_samples(b5.1) bind_rows(mean_qi(post_51 %&gt;% gather_variables()), ggdist::median_hdi(post_51 %&gt;% gather_variables())) %&gt;% as_tibble() ## # A tibble: 17 × 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_A -0.566 -0.796 -0.333 0.95 mean qi ## 2 b_Intercept -0.000206 -0.194 0.197 0.95 mean qi ## 3 lp__ -62.2 -65.5 -60.8 0.95 mean qi ## 4 lprior -1.15 -1.91 -0.629 0.95 mean qi ## 5 prior_b 0.00271 -0.974 0.973 0.95 mean qi ## 6 prior_Intercept -0.00110 -0.384 0.395 0.95 mean qi ## 7 prior_sigma 0.988 0.0287 3.64 0.95 mean qi ## 8 sigma 0.822 0.677 1.00 0.95 mean qi ## 9 b_A -0.566 -0.800 -0.339 0.95 median hdi ## 10 b_Intercept 0.00111 -0.195 0.196 0.95 median hdi ## 11 lp__ -61.9 -64.9 -64.7 0.95 median hdi ## 12 lp__ -61.9 -64.6 -60.7 0.95 median hdi ## 13 lprior -1.11 -1.81 -0.560 0.95 median hdi ## 14 prior_b 0.00554 -0.982 0.944 0.95 median hdi ## 15 prior_Intercept 0.00329 -0.411 0.365 0.95 median hdi ## 16 prior_sigma 0.682 0.000585 2.94 0.95 median hdi ## 17 sigma 0.816 0.675 0.998 0.95 median hdi 事後分布からのサンプリングを行って回帰直線を描く。 結婚年齢が高いほど、離婚率も高いように見える。 # 信頼区間と予測区間を作図 A_seq &lt;- tibble(A = seq(-3,3,length.out=100)) fit_51 &lt;- fitted(b5.1, newdata = A_seq) %&gt;% as_tibble() %&gt;% bind_cols(A_seq) predict_51 &lt;- predict(b5.1, newdata = A_seq) %&gt;% as_tibble() %&gt;% bind_cols(A_seq) d %&gt;% ggplot(aes(x=A,y=D))+ geom_point(size=3,alpha=1/2,shape=1, color=&quot;navyblue&quot;)+ geom_ribbon(data = fit_51,aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;black&quot;, alpha = 3/8)+ geom_ribbon(data = predict_51,aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;black&quot;, alpha = 1/8)+ geom_line(data = predict_51,aes(y = Estimate, x = A))+ scale_x_continuous(&quot;Median age mariage (sd)&quot;, breaks = seq(-3,3,by=1))+ scale_y_continuous(&quot;Divorce rate(std)&quot;, breaks = seq(-3,3,by=1))+theme_bw()+ theme(panel.grid = element_blank(), aspect.ratio = 1) -&gt;p1 print(p1) 結婚率と離婚率に関しても同様に分析。 \\(D_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha + \\beta_{M}M_{i}\\) \\(\\alpha \\sim Normal(0, 0.2)\\) \\(\\beta_{M} \\sim Normal(0, 0.05)\\) \\(\\sigma \\sim Exponential(1)\\) b5.2 &lt;- brm(data = d, family = gaussian, formula = D ~ 1 + M, prior = c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.2&quot;) summary(b5.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D ~ 1 + M ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.11 -0.23 0.23 1.00 4074 2793 ## M 0.35 0.13 0.08 0.60 1.00 3732 2675 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.95 0.10 0.78 1.16 1.00 3665 2913 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 事後分布からのサンプリングを行って回帰直線を描く。 結婚率が高いほど、離婚率も高いように見える。 post_52 &lt;- posterior_samples(b5.2) bind_rows(mean_qi(post_52 %&gt;% gather_variables()), median_hdi(post_52 %&gt;% gather_variables())) %&gt;% as_tibble() ## # A tibble: 16 × 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept -0.0000870 -0.230 0.227 0.95 mean qi ## 2 b_M 0.348 0.0775 0.600 0.95 mean qi ## 3 lp__ -69.0 -72.3 -67.6 0.95 mean qi ## 4 lprior -0.922 -1.74 -0.494 0.95 mean qi ## 5 prior_b -0.0136 -0.971 0.966 0.95 mean qi ## 6 prior_Intercept 0.00628 -0.397 0.384 0.95 mean qi ## 7 prior_sigma 0.992 0.0237 3.79 0.95 mean qi ## 8 sigma 0.951 0.781 1.16 0.95 mean qi ## 9 b_Intercept -0.000115 -0.217 0.233 0.95 median hdi ## 10 b_M 0.349 0.101 0.623 0.95 median hdi ## 11 lp__ -68.7 -71.6 -67.5 0.95 median hdi ## 12 lprior -0.851 -1.54 -0.427 0.95 median hdi ## 13 prior_b -0.0162 -0.947 0.982 0.95 median hdi ## 14 prior_Intercept 0.0109 -0.372 0.401 0.95 median hdi ## 15 prior_sigma 0.683 0.000108 2.96 0.95 median hdi ## 16 sigma 0.942 0.770 1.15 0.95 median hdi # 信頼区間と予測区間を作図 M_seq &lt;- tibble(M = seq(-3,3,length.out=100)) fit_52 &lt;- fitted(b5.2, newdata = M_seq) %&gt;% as_tibble() %&gt;% bind_cols(M_seq) predict_52 &lt;- predict(b5.2, newdata = M_seq) %&gt;% as_tibble() %&gt;% bind_cols(M_seq) d %&gt;% ggplot(aes(x=M,y=D))+ geom_point(size=3,alpha=1/2,shape=1, color=&quot;navyblue&quot;)+ geom_ribbon(data = fit_52,aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;black&quot;, alpha = 3/8)+ geom_ribbon(data = predict_52,aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;black&quot;, alpha = 1/8)+ geom_line(data = predict_52,aes(y = Estimate, x = M))+ scale_x_continuous(&quot;Mariage rate (sd)&quot;, breaks = seq(-3,3,by=1))+ scale_y_continuous(&quot;Divorce rate(std)&quot;, breaks = seq(-3,3,by=1))+theme_bw()+ theme(panel.grid = element_blank(), aspect.ratio = 1) -&gt; p2 print(p2) p1|p2 これら両方は因果関係によるものだろうか？ 3つの変数の関係をグラフィカルモデルを用いて考える。 5.1.1 グラフを用いて因果モデルを考える dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) DAG5.1 &lt;- dagify(M ~ A, D ~ A + M, coords = dag_coords) DAG5.1 %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 15) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) -&gt;p4 2つの因果モデルを考える。 DAG5.2 &lt;- dagify(M ~ A, D ~ A , coords = dag_coords) DAG5.2 %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 15) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) -&gt;p5 p4|p5 d %&gt;% dplyr::select(D:A) %&gt;% cor() %&gt;% round(digits=3) ## D M A ## D 1.000 0.374 -0.597 ## M 0.374 1.000 -0.721 ## A -0.597 -0.721 1.000 dagittyパッケージを用いて、条件付き独立を調べる。 # DAG1 DAG1 &lt;- dagitty(&#39;dag{ D &lt;- A -&gt; M -&gt; D}&#39;) impliedConditionalIndependencies(DAG1) # DAG2 DAG2 &lt;- dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) impliedConditionalIndependencies(DAG2) ## D _||_ M | A DAG1はどの変数で条件づけたとしても、それぞれの変数が従属であることを表している。 一方で、DAG2はAで条件付けすれば、MとDは独立であることを示している。 このことを検証するため、以下のモデルを検討する。DAG2が正しければ、\\(\\beta_{M}\\)は0になるはずである。 \\(D_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha + \\beta_{A}A_{i} + \\beta_{M}M_{i}\\) \\(\\alpha \\sim Normal(0, 0.2)\\) \\(\\beta_{A} \\sim Normal(0, 0.05)\\) \\(\\beta_{M} \\sim Normal(0, 0.05)\\) \\(\\sigma \\sim Exponential(1)\\) モデリングする。 b5.3 &lt;- brm(data =d, family = gaussian, formula = D ~ 1 + A + M, prior= c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 4000, warmup = 3000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.3&quot;) 結果を見てみると、\\(\\beta_{M}\\)はほとんど0に近く、DAG2と一致する。 summary(b5.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D ~ 1 + A + M ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 4000; warmup = 3000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.10 -0.19 0.20 1.00 2839 2372 ## A -0.61 0.16 -0.93 -0.29 1.00 2912 2811 ## M -0.06 0.16 -0.38 0.25 1.00 2883 2865 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.83 0.09 0.68 1.01 1.00 3662 3119 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). sample_b53 &lt;-b5.3 %&gt;% posterior_samples() pairs(b5.3) bind_rows(mean_qi(sample_b53 %&gt;% gather_variables()),median_hdi(sample_b53 %&gt;% gather_variables())) %&gt;% data.frame() ## .variable .value .lower .upper .width .point ## 1 b_A -0.608597286 -0.926476750 -0.2875174 0.95 mean ## 2 b_Intercept 0.002119988 -0.192420900 0.2036938 0.95 mean ## 3 b_M -0.063691753 -0.379385975 0.2517328 0.95 mean ## 4 lp__ -62.858440000 -66.505325000 -61.0215975 0.95 mean ## 5 lprior -1.567187314 -2.726402750 -0.8577874 0.95 mean ## 6 prior_b -0.008235822 -0.982618525 0.9799550 0.95 mean ## 7 prior_Intercept -0.002222847 -0.388412525 0.3990599 0.95 mean ## 8 prior_sigma 0.985647501 0.027099925 3.6812593 0.95 mean ## 9 sigma 0.826615921 0.675801775 1.0126380 0.95 mean ## 10 b_A -0.611012500 -0.931720000 -0.2942620 0.95 median ## 11 b_Intercept -0.000616276 -0.193018000 0.2014380 0.95 median ## 12 b_M -0.062486200 -0.372572000 0.2577620 0.95 median ## 13 lp__ -62.541950000 -65.684100000 -60.8271000 0.95 median ## 14 lprior -1.484605000 -2.528500000 -0.7534570 0.95 median ## 15 prior_b -0.003533605 -0.982426000 0.9846340 0.95 median ## 16 prior_Intercept -0.005888445 -0.396181000 0.3863340 0.95 median ## 17 prior_sigma 0.681631000 0.000231375 2.9564600 0.95 median ## 18 sigma 0.821471000 0.662940000 0.9932160 0.95 median ## .interval ## 1 qi ## 2 qi ## 3 qi ## 4 qi ## 5 qi ## 6 qi ## 7 qi ## 8 qi ## 9 qi ## 10 hdi ## 11 hdi ## 12 hdi ## 13 hdi ## 14 hdi ## 15 hdi ## 16 hdi ## 17 hdi ## 18 hdi b5.1~b5.3の結果を比較してみる。 bind_cols( posterior_samples(b5.1) %&gt;% transmute(&quot;b5.1_beta[A]&quot; = b_A), posterior_samples(b5.2) %&gt;% transmute(&quot;b5.2_beta[M]&quot; = b_M), posterior_samples(b5.3) %&gt;% transmute(&quot;b5.3_beta[M]&quot; = b_M, &quot;b5.3_beta[A]&quot; = b_A) ) %&gt;% pivot_longer(1:4) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(col = name, into = c(&quot;fit&quot;,&quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit))+ geom_vline(xintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5)+ geom_pointrange(color = &quot;firebrick&quot;) + labs(x = &quot;posterior&quot;, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) b5.3の結果を際釈するために、3つの図を描く。 5.1.2 Predictor residual plots まず、結婚率を結婚年齢で回帰したのち、予測値と実測値の間の残差を調べる。残差は結婚率の内、結婚年齢では説明できない部分である。 以下のモデルを考える。 \\(M_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha + \\beta A_{i}\\) \\(\\alpha \\sim Normal(0, 0.2)\\) \\(\\beta \\sim Normal(0, 0.05)\\) \\(\\sigma \\sim Exponential(1)\\) b5.4 &lt;- brm(data = d, family = gaussian, formula = M ~ 1 + A, prior = c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.4&quot;) print(b5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: M ~ 1 + A ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.09 -0.18 0.18 1.00 3791 3108 ## A -0.69 0.10 -0.90 -0.49 1.00 3890 3151 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.71 0.08 0.59 0.88 1.00 3813 2930 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 残差を視覚化してみる。 fit_b54 &lt;- fitted(b5.4) %&gt;% data.frame() %&gt;% bind_cols(d) fit_b54 %&gt;% ggplot(aes(x=A,y=M))+ geom_point(size=3,shape=1,color=&quot;navyblue&quot;)+ geom_segment(aes(xend = A, yend = Estimate),size=1/4)+ geom_line(aes(x=A,y=Estimate))+ geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14)+ labs(x = &quot;Age at marriage (std)&quot;, y = &quot;Marriage rate (std)&quot;)+ coord_cartesian(ylim = range(d$M))+ theme_classic()+ theme(panel.grid = element_blank(), aspect.ratio = .95) -&gt; p5 p5 残差と離婚率の関係を見てみる。 ほぼ関連はないことが分かる。 red_b54 &lt;- residuals(b5.4) %&gt;% data.frame() %&gt;% bind_cols(d) p6 &lt;- red_b54 %&gt;% ggplot(aes(x=Estimate,y=D))+ geom_point(color=&quot;navyblue&quot;,size=3,shape=1)+ geom_vline(xintercept = 0, linetype = &quot;dashed&quot;)+ geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;,size=1/2)+ theme_classic()+ geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14)+ theme(aspect.ratio=1)+ labs(x = &quot;Marriage rate residuals&quot;, y = &quot;Divorce rate (std)&quot;) p6 今度は、結婚年齢を結婚率で回帰して同様の作図をしてみる。 b5.5 &lt;- brm(data = d, family = gaussian, formula = A ~ 1 + M, prior = c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.5&quot;) print(b5.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: A ~ 1 + M ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.09 -0.18 0.17 1.00 3599 3284 ## M -0.69 0.10 -0.89 -0.50 1.00 3710 3136 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.71 0.08 0.58 0.88 1.00 3363 2794 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 残差を視覚化してみる。 fit_b55 &lt;- fitted(b5.5) %&gt;% data.frame() %&gt;% bind_cols(d) fit_b55 %&gt;% ggplot(aes(x=M,y=A))+ geom_point(size=3,shape=1,color=&quot;navyblue&quot;)+ geom_segment(aes(xend = M, yend = Estimate),size=1/4)+ geom_line(aes(x=M,y=Estimate))+ geom_text_repel(data = . %&gt;% filter(Loc %in% c( &quot;ND&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14)+ labs(y = &quot;Age at marriage (std)&quot;, x = &quot;Marriage rate (std)&quot;)+ coord_cartesian(ylim = range(d$M))+ theme_classic()+ theme(panel.grid = element_blank(), aspect.ratio = .95) -&gt; p7 p7 残差と離婚率の関係を見てみる。 今度は、関連がみられることが分かる。 red_b55 &lt;- residuals(b5.5) %&gt;% data.frame() %&gt;% bind_cols(d) p8 &lt;- red_b55 %&gt;% ggplot(aes(x=Estimate,y=D))+ geom_point(color=&quot;navyblue&quot;,size=3,shape=1)+ geom_vline(xintercept = 0, linetype = &quot;dashed&quot;)+ geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;,size=1/2)+ theme_classic()+ geom_text_repel(data = . %&gt;% filter(Loc %in% c( &quot;ND&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14)+ theme(aspect.ratio=1)+ labs(x = &quot;Age at marriage residuals&quot;, y = &quot;Divorce rate (std)&quot;) p8 p5 + p6 + p7 + p8 + plot_annotation(title = &quot;Understanding multiple regression through residuals&quot;) 5.1.3 Posterior prediction plots モデルの予測値が実測値とどの程度一致しているのかを確認する。 fitted(b5.3, prob = c(.055,.945)) %&gt;% data.frame() %&gt;% mutate_all(~.*sd(d$Divorce) + mean(d$Divorce)) %&gt;% bind_cols(d) %&gt;% ggplot(aes(x=Divorce,y=Estimate,ymin = Q5.5, ymax = Q94.5))+ geom_point(color=&quot;navyblue&quot;,size=2)+ geom_abline(linetype=2, size=1/2, color=&quot;grey&quot;)+ geom_linerange(size=1/4, color = &quot;navyblue&quot;, alpha=2/3)+ geom_text(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;UT&quot;, &quot;RI&quot;, &quot;ME&quot;)), aes(label = Loc), hjust=1,nudge_x=-0.25)+ labs(x = &quot;Observed divorce&quot;, y = &quot;Predicted divorce&quot;) + theme_classic() + theme(panel.grid = element_blank(), aspect.ratio=1) 5.1.4 Counterfactual plot 反事実を用いて因果関係を推論する。 ここで、もう一度DAG1を考える。 p4 このDAGが正しいと仮定するときのモデルを考え、どのような予測が得られるかを調べる。 b4.3と異なり、AがMに与える影響も同時にモデリングする。 D_model &lt;- bf(D ~ 1 + A + M) M_model &lt;- bf(M ~ 1 + A) b5.6 &lt;- brm(data =d, family = gaussian, D_model + M_model + set_rescor(FALSE), prior= c(prior(normal(0,0.2),class=Intercept, resp = D), prior(normal(0,0.5),class=b, resp =D), prior(exponential(1),class=sigma, resp=D), prior(normal(0,0.2),class=Intercept, resp = M), prior(normal(0,0.5),class=b, resp =M), prior(exponential(1),class=sigma, resp=M) ), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.6&quot;) summary(b5.6) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: D ~ 1 + A + M ## M ~ 1 + A ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## D_Intercept -0.00 0.10 -0.20 0.20 1.00 5703 2991 ## M_Intercept 0.00 0.09 -0.17 0.18 1.00 5779 3139 ## D_A -0.61 0.16 -0.91 -0.30 1.00 3444 2961 ## D_M -0.06 0.16 -0.36 0.26 1.00 3727 3155 ## M_A -0.69 0.10 -0.89 -0.49 1.00 6504 3009 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_D 0.83 0.09 0.68 1.02 1.00 5850 3151 ## sigma_M 0.71 0.07 0.59 0.87 1.00 5915 2910 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). summary(b5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: M ~ 1 + A ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.09 -0.18 0.18 1.00 3791 3108 ## A -0.69 0.10 -0.90 -0.49 1.00 3890 3151 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.71 0.08 0.59 0.88 1.00 3813 2930 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). fixef(b5.4) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.000202642 0.09123806 -0.1760342 0.1760154 ## A -0.693199622 0.10280042 -0.8954283 -0.4863180 fixef(b5.6) ## Estimate Est.Error Q2.5 Q97.5 ## D_Intercept -0.0001033558 0.10238055 -0.2021818 0.2043600 ## M_Intercept 0.0001592893 0.09000476 -0.1747437 0.1782738 ## D_A -0.6101763775 0.15730728 -0.9142536 -0.3032206 ## D_M -0.0621125614 0.15755733 -0.3596547 0.2574711 ## M_A -0.6914509155 0.09950295 -0.8864033 -0.4947843 Aの値を動かしたときにどうなるかをシミュレートする。 A_seq &lt;- tibble(A = seq(-2,2,length.out=50), M = 0) # Dについて pred &lt;- predict(b5.6, newdata = A_seq) %&gt;% data.frame() %&gt;% bind_cols(A_seq) p1 &lt;- pred %&gt;% ggplot(aes(x=A,y = Estimate.D,ymin = Q2.5.D,ymax=Q97.5.D))+ geom_line(size=0.5)+ geom_ribbon(fill = &quot;black&quot;, alpha = 1/4)+ labs(subtitle = &quot;Total counterfactual effect of A on D&quot;, x = &quot;manipulated A&quot;, y = &quot;counterfactual D&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_classic() + theme(panel.grid = element_blank(), aspect.ratio=1) p1 p2 &lt;- pred %&gt;% ggplot(aes(x=A,y = Estimate.M,ymin = Q2.5.M,ymax=Q97.5.M))+ geom_line(size=0.5)+ geom_ribbon(fill = &quot;black&quot;, alpha = 1/4)+ labs(subtitle = &quot;Total counterfactual effect of A on D&quot;, x = &quot;manipulated A&quot;, y = &quot;counterfactual M&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_classic() + theme(panel.grid = element_blank(), aspect.ratio=1) p1+p2+ plot_annotation(title = &quot;Counterfactual plots for the multivariate divorce model&quot;) 20歳から30歳に上がった時に期待値がどの程度変化するかもシミュレートできる。 A_seq_2 &lt;- tibble(A = (c(20,30)-26.1)/1.24, M = 0) predict(b5.6, newdata = A_seq_2, resp=&quot;D&quot;,summary=F) %&gt;% data.frame() %&gt;% set_names(&quot;a20&quot;,&quot;a30&quot;) %&gt;% mutate(diff = a30 - a20) %&gt;% summarise(mean = mean(diff)) ## mean ## 1 -4.908852 次に、Mを動かしたときにDがどのように変化するかを調べる。このとき、Mを操作するので、A-&gt;Mの因果は消えることになる。 DAG5.3 &lt;- dagify(D ~ M, D ~ A , coords = dag_coords) DAG5.3 %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 15) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) シミュレートする。 seq_M &lt;- tibble(M = seq(-2,2,length.out=50), A = 0) pred_D &lt;- predict(b5.6, newdata = seq_M, resp = &quot;D&quot;) %&gt;% data.frame() %&gt;% bind_cols(seq_M) pred_D %&gt;% ggplot(aes(x=M,y = Estimate,ymin = Q2.5,ymax=Q97.5))+ geom_line(size=0.5)+ geom_ribbon(fill = &quot;black&quot;, alpha = 1/4)+ labs(subtitle = &quot;Total counterfactual effect of M on D&quot;, x = &quot;manipulated M&quot;, y = &quot;counterfactual D&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_classic() + theme(panel.grid = element_blank(), aspect.ratio=1) predict関数を使わずにやると… post &lt;- posterior_samples(b5.6) %&gt;% mutate(iter = 1:n()) %&gt;% dplyr::select(-lp__, -starts_with(&quot;prior&quot;)) %&gt;% tidyr::expand(nesting(iter, b_M_Intercept, b_M_A, sigma_M, b_D_Intercept, b_D_A, b_D_M, sigma_D), A = seq(-2,2,length.out=30)) %&gt;% mutate(M_sim = rnorm(n(), mean = b_M_Intercept+b_M_A*A, sd = sigma_M)) %&gt;% mutate(D_sim = rnorm(n(), mean = b_D_Intercept+b_D_A*A+ b_D_M*M_sim, sd = sigma_D)) %&gt;% pivot_longer(ends_with(&quot;sim&quot;)) %&gt;% group_by(A,name) %&gt;% summarise(mean = mean(value), ll = quantile(value,prob=.025), ul = quantile(value,prob=.975)) 5.2 Masked relationship 2つ以上の説明変数を用いることで、見えにくい因果関係を明らかにすることができることがある。 ここでは、霊長類の母乳1グラム当たりのカロリー（K）とメスの体重（M）、大脳新皮質の割合（N）の関係を調べ、大脳新皮質の割合が高いほど母乳の栄養価が高いのかを調べる。 data(milk) d2 &lt;- milk rm(milk) head(d2) ## clade species kcal.per.g perc.fat perc.protein ## 1 Strepsirrhine Eulemur fulvus 0.49 16.60 15.42 ## 2 Strepsirrhine E macaco 0.51 19.27 16.91 ## 3 Strepsirrhine E mongoz 0.46 14.11 16.85 ## 4 Strepsirrhine E rubriventer 0.48 14.91 13.18 ## 5 Strepsirrhine Lemur catta 0.60 27.28 19.50 ## 6 New World Monkey Alouatta seniculus 0.47 21.22 23.58 ## perc.lactose mass neocortex.perc ## 1 67.98 1.95 55.16 ## 2 63.82 2.09 NA ## 3 69.04 2.51 NA ## 4 71.91 1.62 NA ## 5 53.22 2.19 NA ## 6 55.20 5.25 64.54 # 標準化 d2 &lt;- d2 %&gt;% dplyr::select(kcal.per.g, mass, neocortex.perc) %&gt;% rename(cal = kcal.per.g,neo =neocortex.perc) %&gt;% mutate(K = scale(cal), M = scale(log(mass)), N = scale(neo)) ぱっと見たところ、関係があるのかは分かりづらい。 d2 %&gt;% dplyr::select(cal, mass,neo) %&gt;% pairs() d2_2 &lt;- d2 %&gt;% drop_na(ends_with(&quot;_s&quot;)) 5.2.1 K ~ Nモデル まずは、KとNだけの関係を考慮した単純なモデルを考える。 \\(K_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha + \\beta_{N}N_{i}\\) \\(\\alpha \\sim Normal(0, 1)\\) \\(\\beta_{N} \\sim Normal(0, 1)\\) b5.7_draft &lt;- brm(data = d2_2, family = gaussian, formula = K ~ 1 + N, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4,cores = 4, seed = 5, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.7_draft&quot;) print(b5.7_draft) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: K ~ 1 + N ## Data: d2_2 (Number of observations: 17) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.09 0.26 -0.44 0.59 1.00 3031 2316 ## N 0.16 0.27 -0.38 0.70 1.00 3402 2565 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.14 0.21 0.81 1.62 1.00 3229 2877 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). pairs(b5.7_draft) 事前分布が適切かを考える。 かなり実際とはかけ離れていそう。 prior_samples(b5.7_draft, summary = FALSE) %&gt;% data.frame() %&gt;% dplyr::select(1:2) %&gt;% slice_sample(n=50) %&gt;% mutate(iter = 1:n()) %&gt;% tidyr::expand(nesting(Intercept, b, iter), N = c(-2,2)) %&gt;% mutate(K = Intercept + b*N) %&gt;% ggplot(aes(x=N, y = K))+ geom_line(aes(group = iter), color = &quot;grey33&quot;, alpha = 3/4)+ scale_x_continuous(&quot;neocortex percent (std)&quot;, breaks = seq(-2,2,by=1))+ scale_y_continuous(&quot;kilocal per g (std)&quot;, breaks = seq(-2,2,by=1))+ coord_cartesian(ylim = c(-2,2))+ theme_classic()+ labs(subtitle = &quot;a ~ dnorm(0,1)\\nb = dnorm(0,1)&quot;)+ theme(aspect.ratio=1) -&gt; p9 p9 もう少し事前分布の幅を小さくしてみる。 今度はよさそう。 b5.7_t &lt;- brm(data = d2_2, family = gaussian, formula = K ~ 1 + N, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0,0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4,cores = 4, seed = 5, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.7_t&quot;) print(b5.7_t) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: K ~ 1 + N ## Data: d2_2 (Number of observations: 17) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.04 0.16 -0.29 0.35 1.00 3778 2736 ## N 0.12 0.25 -0.38 0.60 1.00 3510 2586 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.11 0.20 0.80 1.57 1.00 3662 2969 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b5.7_draft) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: K ~ 1 + N ## Data: d2_2 (Number of observations: 17) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.09 0.26 -0.44 0.59 1.00 3031 2316 ## N 0.16 0.27 -0.38 0.70 1.00 3402 2565 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.14 0.21 0.81 1.62 1.00 3229 2877 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). prior_samples(b5.7_t, summary = FALSE) %&gt;% data.frame() %&gt;% dplyr::select(1:2) %&gt;% slice_sample(n=50) %&gt;% mutate(iter = 1:n()) %&gt;% tidyr::expand(nesting(Intercept, b, iter), N = c(-2,2)) %&gt;% mutate(K = Intercept + b*N) %&gt;% ggplot(aes(x=N, y = K))+ geom_line(aes(group = iter), color = &quot;grey33&quot;, alpha = 3/4)+ scale_x_continuous(&quot;neocortex percent (std)&quot;, breaks = seq(-2,2,by=1))+ scale_y_continuous(&quot;kilocal per g (std)&quot;, breaks = seq(-2,2,by=1))+ coord_cartesian(ylim = c(-2,2))+ theme_classic()+ labs(subtitle = &quot;a ~ dnorm(0,0.2)\\nb = dnorm(0,5)&quot;)+ theme(aspect.ratio=1) -&gt; p10 p10 p9|p10 推定結果も少し異なり、b5.7_tの方がより事後分布の標準偏差が小さくなっている。 bind_rows( posterior_samples(b5.7_draft) %&gt;% dplyr::select(b_Intercept:sigma), posterior_samples(b5.7_t) %&gt;% dplyr::select(b_Intercept:sigma) ) %&gt;% mutate(fit = rep(c(&quot;b5.5_draft&quot;, &quot;b5.5_t&quot;), each = n() / 2)) %&gt;% pivot_longer(-fit) %&gt;% group_by(name, fit) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate(fit = factor(fit, levels = c(&quot;b5.5_draft&quot;, &quot;b5.5_t&quot;))) %&gt;% # plot ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) + geom_pointrange(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + labs(x = &quot;posterior&quot;, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank(), strip.background = element_blank()) + facet_wrap(~ name, ncol = 1) 事後分布から信用区間と予測区間を作図。 かなり微妙な変化…。 seq_N &lt;- tibble(N=seq(-2,2,length.out = 100)) fit_b57 &lt;- fitted(b5.7_t, newdata = seq_N, prob = c(.055,.945)) %&gt;% data.frame() %&gt;% bind_cols(seq_N) predict_b57 &lt;- predict(b5.7_t, newdata = seq_N, prob = c(.055,.945)) %&gt;% data.frame() %&gt;% bind_cols(seq_N) d2_2 %&gt;% ggplot(aes(x=N,y=K))+ geom_point(size=3,shape=1,color=&quot;navyblue&quot;)+ geom_ribbon(data = fit_b57,aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), fill = &quot;black&quot;, alpha = 3/8)+ geom_ribbon(data = predict_b57,aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), fill = &quot;black&quot;, alpha = 1/8)+ geom_line(data = fit_b57,aes(y = Estimate, x = N))+ scale_x_continuous(&quot;neocortex percent (std)&quot;, breaks = seq(-2,2,by=1))+ scale_y_continuous(&quot;kilocal per g (std)&quot;, breaks = seq(-1,2,by=1))+ theme_classic()+ theme(panel.grid = element_blank(), aspect.ratio = 1) -&gt; p11 p11 5.2.2 K ~ Mモデル 次に、メスの体重(M)と母乳のカロリー（K）の関係を考える。 b5.8_t &lt;- brm(data = d2_2, family = gaussian, formula = K ~ 1 + M, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0,0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4,cores = 4, seed = 5, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.8_t&quot;) print(b5.8_t) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: K ~ 1 + M ## Data: d2_2 (Number of observations: 29) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.13 -0.26 0.26 1.00 4025 2916 ## M -0.29 0.17 -0.63 0.05 1.00 3524 2380 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.98 0.14 0.75 1.28 1.00 3585 2874 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 強くはないが、負の関係がありそう？ seq_M &lt;- tibble(M=seq(-2,2,length.out = 100)) fit_b58 &lt;- fitted(b5.8_t, newdata = seq_M, prob = c(.055,.945)) %&gt;% data.frame() %&gt;% bind_cols(seq_M) predict_b58 &lt;- predict(b5.8_t, newdata = seq_M, prob = c(.055,.945)) %&gt;% data.frame() %&gt;% bind_cols(seq_M) d2_2 %&gt;% ggplot(aes(x=M,y=K))+ geom_point(size=3,shape=1,color=&quot;navyblue&quot;)+ geom_ribbon(data = fit_b58,aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), fill = &quot;black&quot;, alpha = 3/8)+ geom_ribbon(data = predict_b58,aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), fill = &quot;black&quot;, alpha = 1/8)+ geom_line(data = fit_b58,aes(y = Estimate, x = M))+ scale_x_continuous(&quot;log body mass (std)&quot;, breaks = seq(-2,2,by=1))+ scale_y_continuous(&quot;kilocal per g (std)&quot;, breaks = seq(-1,2,by=0.5))+ coord_cartesian(ylim=c(-1,2))+ theme_classic()+ theme(panel.grid = element_blank(), aspect.ratio = 1) -&gt; p12 p12 (p11|p12) 5.2.3 K ~ N + Mモデル それでは、両方の説明変数を入れるモデルを考える。 モデル式は以下の通り。 \\(K_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha + \\beta_{N}N_{i}+\\beta_{M}M_{i}\\) \\(\\alpha \\sim Normal(0, 0.2)\\) \\(\\beta_{N} \\sim Normal(0, 0.5)\\) \\(\\beta_{M} \\sim Normal(0, 0.5)\\) \\(\\sigma \\sim Exponential(1)\\) b5.9 &lt;- brm(data = d2_2, family = gaussian, formula = K ~ 1 + N + M, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0,0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains =4,cores = 4, seed = 5, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.9&quot;) それぞれNは強く正の方向に、Mは強く負の方向に影響を与えている。 2変数を入れたことによって効果が強まったよう。 print(b5.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: K ~ 1 + N + M ## Data: d2_2 (Number of observations: 17) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.14 -0.21 0.34 1.00 3167 2661 ## N 0.61 0.28 0.03 1.11 1.00 2393 2393 ## M -0.65 0.24 -1.10 -0.13 1.00 2272 2592 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.86 0.18 0.60 1.27 1.00 2290 2374 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 上記2つのモデルと比較しても一目瞭然である。 bind_cols( posterior_samples(b5.7_t) %&gt;% transmute(`b5.5_beta[N]` = b_N), posterior_samples(b5.8_t) %&gt;% transmute(`b5.6_beta[M]` = b_M), posterior_samples(b5.9) %&gt;% transmute(`b5.7_beta[N]` = b_N, `b5.7_beta[M]` = b_M) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% # complete(fit, parameter) %&gt;% ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) + geom_pointrange(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + ylab(NULL) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) この結果をどのように解釈すべきだろうか？ これは、NとMがそれぞれKに対して反対方向に影響を与えていて、かつNとMが正に相関していたため、それぞれがKに与える影響が打ち消しあっていたと考えられる。 pairs(~K+M+N, d2_2) library(GGally) na.omit(d2) %&gt;% dplyr::select(K,M,N)%&gt;% mutate_all(~ as.numeric(.)) %&gt;% ggpairs() グラフィカルモデルを使って考える。 結果に合致するグラフは3通り考えられるが、データからは区別することはできない（Marcov equivalence）。 gg_dag &lt;- function(d) { d %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) } dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;N&quot;, &quot;K&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) p1 &lt;- dagify(N ~ M, K ~ M + N, coords = dag_coords) %&gt;% gg_dag() # middle DAG p2 &lt;- dagify(M ~ N, K ~ M + N, coords = dag_coords) %&gt;% gg_dag() # right DAG dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;N&quot;, &quot;K&quot;, &quot;U&quot;), x = c(1, 3, 2, 2), y = c(2, 2, 1, 2)) p3 &lt;- dagify(M ~ U, N ~ U, K ~ M + N, coords = dag_coords) %&gt;% gg_dag() + geom_point(x = 2, y = 2, shape = 1, size = 10, stroke = 1.25, color = &quot;firebrick4&quot;) p1+p2+p3 5.2.4 Counterfactual plot を描く # K ~ N seq_N &lt;- tibble(N =seq(-2,2,length.out=50), M = 0) predict_b59_N &lt;- fitted(b5.9, newdata = seq_N, prob = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(seq_N) predict_b59_N %&gt;% ggplot(aes(x=N, y= Estimate, ymin = Q5.5, ymax = Q94.5))+ geom_line()+ geom_ribbon(alpha = 1/7, fill = &quot;black&quot;)+ scale_x_continuous(&quot;neocortex percent (std)&quot;, breaks = seq(-2,1.5,by=0.5))+ scale_y_continuous(&quot;kilocal per g (std)&quot;, breaks = seq(-1,2,by=0.5))+ coord_cartesian(ylim=c(-1,2))+ theme_classic()+ theme(panel.grid = element_blank(), aspect.ratio = 1) -&gt; p13 # K ~ M seq_M &lt;- tibble(M =seq(-2,2,length.out=50), N = 0) predict_b59_M &lt;- fitted(b5.9, newdata = seq_M, prob = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(seq_M) predict_b59_M %&gt;% ggplot(aes(x=M, y= Estimate, ymin = Q5.5, ymax = Q94.5))+ geom_line()+ geom_ribbon(alpha = 1/7, fill = &quot;black&quot;)+ scale_x_continuous(&quot;neocortex percent (std)&quot;, breaks = seq(-2,2,by=1))+ scale_y_continuous(&quot;kilocal per g (std)&quot;, breaks = seq(-1,2,by=0.5))+ coord_cartesian(ylim=c(-1,2))+ theme_classic()+ theme(panel.grid = element_blank(), aspect.ratio = 1) -&gt; p14 p13+p14+ plot_annotation(title = &quot;Figure 5.9 [bottom row]. Milk energy and neocortex among primates.&quot;) 5.2.5 Overthinking Marcov evuivalentなグラフィカルモデルはどれ？ dag5.7 &lt;- dagitty(&quot;dag{ M -&gt; K &lt;- N M -&gt; N }&quot; ) coordinates(dag5.7) &lt;- list(x = c(M = 0, K = 1, N =2), y=c(M =0.5,K = 1,N = 0.5)) ggdag_equivalent_dags(dag5.7) 5.3 Categorical variables 5.3.1 Two categories 説明変数に質的変数を含む場合を考える。 再びクンサン族のデータを考える。 data(Howell1) d3 &lt;- Howell1 str(d3) ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... まずは、身長が性別によって違うというモデルを考える。 \\(h_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha + \\beta_{M}m_{i}\\) \\(\\alpha \\sim Normal(178, 20)\\) \\(\\beta_{M} \\sim Normal(0, 10)\\) \\(\\sigma \\sim Exponential(1)\\) しかし、このモデルでは、男女の事前分布で違いが出てきてしまう（男は\\(\\alpha\\)と\\(\\beta_{M}\\)という2つのパラメータを使っているため）。 mu_f &lt;- rnorm(1e4, 178,20) # alphaに相当 mu_m &lt;- rnorm(1e4, 178,20) + rnorm(1e4,0,10) precis(data.frame(mu_f,mu_m),hist = F) ## mean sd 5.5% 94.5% ## mu_f 178.4736 19.88366 146.6287 210.1023 ## mu_m 178.0379 22.32357 142.2385 214.0687 そこで、以下のモデルを考える。 \\(h_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} \\sim \\alpha_{SEX[i]}\\) \\(\\alpha_{j} \\sim Normal(178, 20)\\;\\; for\\,j=1,2\\) \\(\\sigma \\sim Exponential(1)\\) d3 &lt;- d3 %&gt;% mutate(sex = factor(ifelse(male==1,2,1))) str(d3) ## &#39;data.frame&#39;: 544 obs. of 5 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... ## $ sex : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 2 1 1 2 1 2 1 2 1 2 ... brmsでモデリングする。 b5.10 &lt;- brm( data = d3, family = gaussian, formula = height ~ 0 + sex, prior = c(prior(normal(178, 20), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.10_2&quot; ) print(b5.10) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 0 + sex ## Data: d3 (Number of observations: 544) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sex1 134.86 1.54 131.83 137.87 1.00 3815 3177 ## sex2 142.62 1.67 139.33 145.92 1.00 3173 2687 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 26.76 0.77 25.31 28.35 1.00 4534 2777 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). #posterior_samples(b5.10,summary=F) %&gt;% #data.frame() %&gt;% #mutate(diff = b_sex1 - b_sex2) %&gt;% #dplyr::select(-lp__) %&gt;% #map_df(mean_qi, .width = .89) 5.3.2 Many Categories 複数のカテゴリーがある場合を考える。 data(milk) d4 &lt;- milk str(milk) ## &#39;data.frame&#39;: 29 obs. of 8 variables: ## $ clade : Factor w/ 4 levels &quot;Ape&quot;,&quot;New World Monkey&quot;,..: 4 4 4 4 4 2 2 2 2 2 ... ## $ species : Factor w/ 29 levels &quot;A palliata&quot;,&quot;Alouatta seniculus&quot;,..: 11 8 9 10 16 2 1 6 28 27 ... ## $ kcal.per.g : num 0.49 0.51 0.46 0.48 0.6 0.47 0.56 0.89 0.91 0.92 ... ## $ perc.fat : num 16.6 19.3 14.1 14.9 27.3 ... ## $ perc.protein : num 15.4 16.9 16.9 13.2 19.5 ... ## $ perc.lactose : num 68 63.8 69 71.9 53.2 ... ## $ mass : num 1.95 2.09 2.51 1.62 2.19 5.25 5.37 2.51 0.71 0.68 ... ## $ neocortex.perc: num 55.2 NA NA NA NA ... d4 %&gt;% distinct(clade) ## clade ## 1 Strepsirrhine ## 2 New World Monkey ## 3 Old World Monkey ## 4 Ape d4 &lt;- d4 %&gt;% mutate(K = scale(kcal.per.g)) 以下のモデルを考える。 \\(K_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha_{clade[i]}\\) \\(\\alpha_{j} \\sim Normal(0, 0.5)\\) \\(\\sigma \\sim Exponential(1)\\) b5.11 &lt;- brm(data=d4, family = gaussian, formula = K ~ 0 + clade, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000,warmup = 1000,chains = 4, cores = 4, seed = 5, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b5.11&quot;) mcmc_plot(b5.11, pars = &quot;^b&quot;) library(bayesplot) library(ggmcmc) posterior_samples(b5.11) %&gt;% dplyr::select(starts_with(&quot;b_&quot;)) %&gt;% mcmc_intervals(prob = .5, prob_outer=.89)+ labs(title = &quot;My fancy bayesplot-based coefficient plot&quot;) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) 5.4 Practice 5.4.1 5M1 Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced). 気温（T）とアイスクリームの売り上げ（I）、プールでの死亡事故の件数（P）の関係を考える。 想定する因果関係は以下の通り。 gg_dag &lt;- function(d) { d %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) } dag_coords &lt;- tibble(name = c(&quot;T&quot;, &quot;I&quot;, &quot;P&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) dagify(I ~ T, P ~ T, coords = dag_coords) %&gt;% gg_dag() シミュレーションする。 IとPの間には正の相関がある n &lt;- 100 T &lt;- rnorm(n, 25, 5) I &lt;- rnorm(n,0.5*T+4, 2) P &lt;- rnorm(n,0.3*T+2, 3) d &lt;- tibble(T = T, I=I, P=P) pairs(d) d %&gt;% ggplot(aes(x=I, y = P))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, color=&quot;black&quot;)+ theme_classic()+ theme(panel.grid=element_blank(),aspect.ratio=1) cor.test(d$I,d$P) ## ## Pearson&#39;s product-moment correlation ## ## data: d$I and d$P ## t = 5.1296, df = 98, p-value = 1.467e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2898452 0.6020814 ## sample estimates: ## cor ## 0.4600711 モデルを考える。 modIT &lt;- brm(data = d, family=gaussian, formula = I ~ 1+T, prior = c(prior(normal(0,10),class=Intercept), prior(normal(0,10),class= b)), iter=4000,warmup=3000,chains=4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/mod_IT&quot; ) modIP &lt;- brm(data = d, family=gaussian, formula = I ~ 1+P, prior = c(prior(normal(0,10),class=Intercept), prior(normal(0,10),class= b)), iter=4000,warmup=3000,chains=4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/mod_IP&quot; ) modIPT &lt;- brm(data = d, family=gaussian, formula = I ~ 1+P+T, prior = c(prior(normal(0,10),class=Intercept), prior(normal(0,10),class= b)), iter=4000,warmup=3000,chains=4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/mod_IPT&quot; ) 結果を見てみる。 print(modIT) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: I ~ 1 + T ## Data: d (Number of observations: 100) ## Draws: 4 chains, each with iter = 4000; warmup = 3000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.66 1.00 1.75 5.67 1.00 3754 2569 ## T 0.51 0.04 0.43 0.59 1.00 3825 2555 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.92 0.13 1.68 2.21 1.00 3754 2494 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(modIP) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: I ~ 1 + P ## Data: d (Number of observations: 100) ## Draws: 4 chains, each with iter = 4000; warmup = 3000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 13.19 1.00 11.18 15.18 1.00 3823 2768 ## P 0.32 0.10 0.14 0.51 1.00 3820 2986 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 3.01 0.22 2.62 3.48 1.00 3579 2733 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(modIPT) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: I ~ 1 + P + T ## Data: d (Number of observations: 100) ## Draws: 4 chains, each with iter = 4000; warmup = 3000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.76 1.04 1.71 5.80 1.00 5273 3077 ## P -0.03 0.07 -0.16 0.11 1.00 3462 2738 ## T 0.52 0.04 0.43 0.60 1.00 3477 2780 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.92 0.14 1.68 2.21 1.00 3730 2822 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). bind_cols( posterior_samples(modIT) %&gt;% transmute(&quot;modIT_[T]&quot;= b_T), posterior_samples(modIP) %&gt;% transmute(&quot;modIP_[P]&quot; = b_P), posterior_samples(modIPT) %&gt;% transmute(&quot;modIPT_[T]&quot; = b_T, &quot;modIPT_[P]&quot; = b_P) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% ggplot(aes(x = mean, y = fit,xmin = ll,xmax = ul)) + geom_pointrange(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + ylab(NULL) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~ parameter, ncol = 1) 5.4.2 5M2 Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another. 自分への信頼（C）と勉強時間（S）、慢心（M）、成績（G）の関係を考える。 想定する因果関係は以下の通り。 dag_coords &lt;- tibble(name = c(&quot;S&quot;, &quot;M&quot;, &quot;G&quot;, &quot;C&quot;), x = c(1, 3, 2, 2), y = c(2, 2, 1, 2)) dagify(S ~ C, M ~ C, G ~ S + M, coords = dag_coords) %&gt;% gg_dag() + geom_point(x = 2, y = 2, shape = 1, size = 10, stroke = 1.25, color = &quot;firebrick4&quot;) シミュレートする。 n &lt;- 100 C &lt;- rnorm(n) S &lt;- rnorm(n, 1.2*C , 0.4) M &lt;- rnorm(n, 1.0*C , 0.3) G &lt;- rnorm(n, 0.5*S - 0.6*M , 0.8) d2 &lt;- tibble(S = S, M = M, G = G) pairs(d2) cor(d2) ## S M G ## S 1.00000000 0.9168803 -0.05796131 ## M 0.91688025 1.0000000 -0.19285146 ## G -0.05796131 -0.1928515 1.00000000 cor.test(d2$S,d2$G) ## ## Pearson&#39;s product-moment correlation ## ## data: d2$S and d2$G ## t = -0.57475, df = 98, p-value = 0.5668 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2515160 0.1400512 ## sample estimates: ## cor ## -0.05796131 モデリング。 modGS &lt;- brm(data = d2, family=gaussian, formula = G ~ 1+S, prior = c(prior(normal(0,1),class=Intercept), prior(normal(0,1),class= b)), iter=4000,warmup=3000,chains=4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/modGS&quot; ) modGM &lt;- brm(data = d2, family=gaussian, formula = G ~ 1+M, prior = c(prior(normal(0,1),class=Intercept), prior(normal(0,1),class= b)), iter=4000,warmup=3000,chains=4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/modGM&quot; ) modGSM &lt;- brm(data = d2, family=gaussian, formula = G ~ 1+S + M, prior = c(prior(normal(0,1),class=Intercept), prior(normal(0,1),class= b)), iter=4000,warmup=3000,chains=4,seed=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/modGSM&quot; ) print(modGM) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 1 + M ## Data: d2 (Number of observations: 100) ## Draws: 4 chains, each with iter = 4000; warmup = 3000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.06 0.08 -0.09 0.21 1.00 3984 2763 ## M 0.05 0.07 -0.09 0.18 1.00 4048 3210 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.76 0.06 0.66 0.88 1.00 4013 2954 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(modGS) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 1 + S ## Data: d2 (Number of observations: 100) ## Draws: 4 chains, each with iter = 4000; warmup = 3000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.03 0.07 -0.11 0.18 1.00 4104 2701 ## S 0.14 0.06 0.02 0.25 1.00 3954 3018 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.74 0.05 0.64 0.86 1.00 4109 3192 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(modGSM) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 1 + S + M ## Data: d2 (Number of observations: 100) ## Draws: 4 chains, each with iter = 4000; warmup = 3000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.06 0.08 -0.21 0.09 1.00 2917 2274 ## S 0.57 0.13 0.30 0.83 1.00 2340 2048 ## M -0.56 0.16 -0.87 -0.23 1.00 2325 2074 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.70 0.05 0.60 0.81 1.00 3112 2629 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). bind_cols( posterior_samples(modGS) %&gt;% transmute(&quot;modGS_[S]&quot;= b_S), posterior_samples(modGM) %&gt;% transmute(&quot;modGM_[M]&quot; = b_M), posterior_samples(modGSM) %&gt;% transmute(&quot;modGSM_[S]&quot; = b_S, &quot;modGSM_[M]&quot; = b_M) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% ggplot(aes(x = mean, y = fit,xmin = ll,xmax = ul)) + geom_pointrange(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + ylab(NULL) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~ parameter, ncol = 1) 5.4.3 5M4 In the divorce data, States with high numbers of members of the Church of Jesus Christ of Latter-day Saints (LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardized). You may want to consider transformations of the raw percent LDS variable. data(&quot;WaffleDivorce&quot;) lds &lt;- read_csv(&quot;data/lds-data.csv&quot;) lds &lt;- lds %&gt;% mutate(ldsrate = log(members/population)) %&gt;% dplyr::select(state, ldsrate) %&gt;% rename(Location = state) lds %&gt;% ggplot(aes(x=ldsrate))+ geom_histogram() d4 &lt;- WaffleDivorce %&gt;% left_join(lds, by = &quot;Location&quot;) %&gt;% dplyr::select(MedianAgeMarriage, Divorce, Marriage,ldsrate,South) %&gt;% mutate(across(1:4,standardize)) %&gt;% rename(A = MedianAgeMarriage, D = Divorce, M = Marriage, S=South, L = ldsrate) datatable(d4) pairs(d4) modelling b.5M4 &lt;- brm(data = d4, family = gaussian, formula = D ~ 1 + A + M + L, prior= c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, backend = &quot;cmdstanr&quot;, chains = 4, sample_prior = T, file = &quot;output/Chapter5/b.5M4&quot;) print(b.5M4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D ~ 1 + A + M + L ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.10 -0.20 0.21 1.00 3999 2466 ## A -0.69 0.16 -0.99 -0.37 1.00 3257 2947 ## M 0.08 0.17 -0.24 0.42 1.00 3068 3018 ## L -0.29 0.16 -0.59 0.02 1.00 3199 2816 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.80 0.09 0.65 0.99 1.00 3561 2782 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). b.5M4_2 &lt;- brm(data = d4, family = gaussian, formula = M ~ 1 + A + L, prior= c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5M4_2&quot;) print(b.5M4_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: M ~ 1 + A + L ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.08 -0.17 0.17 1.00 3796 2327 ## A -0.44 0.11 -0.67 -0.22 1.00 3163 2902 ## L 0.42 0.11 0.19 0.64 1.00 3332 3089 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.63 0.07 0.52 0.78 1.00 3609 2898 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). b.5M4_3&lt;- brm(data = d4, family = gaussian, formula = A ~ 1 + M + L, prior= c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5M4_3&quot;) print(b.5M4_3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: A ~ 1 + M + L ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.09 -0.17 0.17 1.00 3742 2832 ## M -0.51 0.13 -0.77 -0.25 1.00 2603 2777 ## L -0.26 0.13 -0.53 -0.01 1.00 2517 2746 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.69 0.08 0.57 0.85 1.00 3757 3123 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). b.5M4_4&lt;- brm(data = d4, family = gaussian, formula = L ~ 1 + A + M, prior= c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5M4_4&quot;) print(b.5M4_4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: L ~ 1 + A + M ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.09 -0.17 0.17 1.00 3967 2519 ## A -0.27 0.13 -0.53 -0.01 1.00 2896 2913 ## M 0.50 0.14 0.23 0.77 1.00 3000 2825 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.70 0.07 0.57 0.86 1.00 3993 3024 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). b.5M4_5&lt;- brm(data = d4, family = gaussian, formula = M ~ 1 + A + L + D, prior= c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5M4_5&quot;) print(b.5M4_5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: M ~ 1 + A + L + D ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.08 -0.17 0.16 1.00 4281 2508 ## A -0.40 0.14 -0.69 -0.13 1.00 2097 2265 ## L 0.43 0.12 0.20 0.66 1.00 2510 2688 ## D 0.05 0.12 -0.18 0.28 1.00 3043 2803 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.64 0.07 0.53 0.79 1.00 3391 2947 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # all b1 &lt;- bf(D ~ 1 + A + L) b2 &lt;- bf(M ~ 1 + A + L) b3 &lt;- bf(A ~ 1 + L) b.5M4_all&lt;- brm(data = d4, family = gaussian, formula = b1 + b2 +b3+ set_rescor(FALSE), prior= c(prior(normal(0,0.2),class=Intercept, resp = D), prior(normal(0,0.5),class=b,resp=D), prior(exponential(1),class=sigma,resp=D), prior(normal(0,0.2),class=Intercept,resp=A), prior(normal(0,0.5),class=b,resp=A), prior(exponential(1),class=sigma,resp=A), prior(normal(0,0.2),class=Intercept,resp=M), prior(normal(0,0.5),class=b,resp=M), prior(exponential(1),class=sigma, resp=M)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5M4_all&quot;) print(b.5M4_all) ## Family: MV(gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: D ~ 1 + A + L ## M ~ 1 + A + L ## A ~ 1 + L ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## D_Intercept 0.00 0.10 -0.19 0.20 1.00 8041 2770 ## M_Intercept 0.00 0.08 -0.16 0.17 1.00 8274 2960 ## A_Intercept -0.00 0.10 -0.19 0.18 1.00 7400 2906 ## D_A -0.73 0.14 -0.99 -0.45 1.00 4533 3290 ## D_L -0.26 0.14 -0.53 0.02 1.00 4510 3388 ## M_A -0.44 0.11 -0.66 -0.21 1.00 4511 2830 ## M_L 0.42 0.11 0.20 0.64 1.00 4550 3271 ## A_L -0.61 0.11 -0.82 -0.39 1.00 8292 3027 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_D 0.79 0.08 0.65 0.97 1.00 6552 2899 ## sigma_M 0.64 0.07 0.52 0.78 1.00 7181 3337 ## sigma_A 0.79 0.08 0.65 0.98 1.00 6647 2817 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). library(bayesplot) posterior_samples(b.5M4_all) %&gt;% dplyr::select(-lp__) %&gt;% dplyr::select(starts_with(&quot;b_&quot;)) %&gt;% mcmc_intervals() posterior_samples(b.5M4_all) %&gt;% dplyr::select(starts_with(&quot;b_&quot;)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_hdi() %&gt;% data.frame() ## name value .lower .upper .width .point .interval ## 1 b_A_Intercept -0.0012076800 -0.185829 0.18290800 0.95 median hdi ## 2 b_A_L -0.6104695000 -0.832102 -0.39804600 0.95 median hdi ## 3 b_D_A -0.7285490000 -0.984386 -0.45322400 0.95 median hdi ## 4 b_D_Intercept 0.0011336100 -0.183601 0.20455000 0.95 median hdi ## 5 b_D_L -0.2561405000 -0.536132 0.00771803 0.95 median hdi ## 6 b_M_A -0.4346445000 -0.661619 -0.20759500 0.95 median hdi ## 7 b_M_Intercept 0.0002934045 -0.166698 0.16098400 0.95 median hdi ## 8 b_M_L 0.4217880000 0.202541 0.64777700 0.95 median hdi Graphical model library(dagitty) mad_dag &lt;- dagitty(&quot;dag{ D&lt;-A -&gt; M &lt;- L-&gt;D; A &lt;- L }&quot;) ggdag(mad_dag) impliedConditionalIndependencies(mad_dag) ## D _||_ M | A, L ggdag_equivalent_dags(mad_dag) 5.4.4 5H1 In the divorce example, suppose the DAG is: M→A→D. What are the implied conditional independencies of the graph? Are the data consistent with it? library(dagitty) mad_dag &lt;- dagitty(&quot;dag{M -&gt; A -&gt; D}&quot;) impliedConditionalIndependencies(mad_dag) ## D _||_ M | A データに適合する因果ダイアグラムは以下の通り。 equivalentDAGs(mad_dag) ## [[1]] ## dag { ## A ## D ## M ## A -&gt; D ## M -&gt; A ## } ## ## [[2]] ## dag { ## A ## D ## M ## A -&gt; D ## A -&gt; M ## } ## ## [[3]] ## dag { ## A ## D ## M ## A -&gt; M ## D -&gt; A ## } 5.4.5 5H2 Assuming that the DAG for the divorce example is indeed M→A→D, fit a new model and use it to estimate the counterfactual effect of halving a State’s marriage rate M. Using the counterfactual example from the chapter (starting on page 140) as a template. mad_dag &lt;- dagitty(&quot;dag{ M -&gt; A -&gt; D}&quot;) ggdag_equivalent_dags(mad_dag) b1 &lt;- bf(D ~ 1 + A + M) b2 &lt;- bf(A ~ 1 + M) b.5H2 &lt;- brm(data = d4, family = gaussian, formula = b1 + b2 + set_rescor(rescor = FALSE), prior = c(prior(normal(0,0.2),class=Intercept,resp=D), prior(normal(0,0.2),class=Intercept,resp=A), prior(normal(0,0.5),class=b,resp=A), prior(normal(0,0.5),class=b,resp=D), prior(exponential(1),class=sigma, resp = D), prior(exponential(1),class=sigma, resp = A)), seed = 5, iter = 2000, warmup = 1000, chains = 4, sample_prior = T, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5H2&quot;) print(b.5H2) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: D ~ 1 + A + M ## A ~ 1 + M ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## D_Intercept 0.00 0.10 -0.20 0.20 1.00 6273 3012 ## A_Intercept -0.00 0.09 -0.18 0.18 1.00 5121 2775 ## D_A -0.60 0.16 -0.92 -0.29 1.00 3693 3016 ## D_M -0.06 0.16 -0.37 0.25 1.00 3797 2818 ## A_M -0.69 0.10 -0.89 -0.49 1.00 5113 2805 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_D 0.83 0.09 0.68 1.02 1.00 5012 3145 ## sigma_A 0.71 0.07 0.58 0.88 1.00 4978 3004 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). posterior_samples(b.5H2) %&gt;% dplyr::select(starts_with(&quot;b_&quot;)) %&gt;% mcmc_intervals(prob = 0.5,prob_outer=.95) posterior_samples(b.5H2) %&gt;% data.frame() %&gt;% dplyr::select(starts_with(&quot;b_&quot;),starts_with(&quot;sigma&quot;)) %&gt;% tidyr::expand(nesting(b_D_Intercept, b_A_Intercept, b_D_A, b_D_M, b_A_M,sigma_D, sigma_A), M = seq(-2,2,length.out=50)) %&gt;% mutate(a_sim = rnorm(n(),mean=b_A_Intercept+ b_A_M*M, sigma_A), d_sim = rnorm(n(),mean=b_D_Intercept+ b_D_M*M + b_A_M*a_sim, sigma_D)) %&gt;% dplyr::select(a_sim, M, d_sim) %&gt;% pivot_longer(c(a_sim,d_sim)) %&gt;% group_by(name,M) %&gt;% mean_qi(value,.width = c(.89)) %&gt;% ggplot(aes(x=M,y=value, ymin = .lower, ymax = .upper))+ geom_line()+ geom_ribbon(alpha=3/8)+ theme_classic()+ theme(aspect.ratio = 1)+ facet_wrap(~name) 5.4.6 5H3 Return to the milk energy model, m5.7. Suppose that the true causal relationship among the variables is: dagify(K ~ M + N, N ~ M) %&gt;% ggdag()+ theme_dag() Now compute the counterfactual effect on K of doubling M. You will need to account for both the direct and indirect paths of causation. Use the counterfactual example from the chapter (starting on page 140) as a template. b1 &lt;- bf(N ~ 1 + M) b2 &lt;- bf(K ~ 1 + M + N) b.5H3 &lt;- brm(formula = b1 + b2 + set_rescor(FALSE), data = d2_2, family = gaussian, prior = c(prior(normal(0, 0.2), class = Intercept, resp = N), prior(normal(0, 0.2), class = Intercept, resp = K), prior(normal(0, 0.5), class = b, resp = N), prior(normal(0, 0.5), class = b, resp = K), prior(exponential(1), class = sigma, resp = N), prior(exponential(1), class = sigma, resp = K)), seed =5, iter = 4000, warmup = 2000, chains =4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5H3&quot;) print(b.5H3) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: N ~ 1 + M ## K ~ 1 + M + N ## Data: d2_2 (Number of observations: 17) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## N_Intercept -0.02 0.13 -0.29 0.23 1.00 10735 5770 ## K_Intercept 0.07 0.14 -0.22 0.35 1.00 11199 6075 ## N_M 0.60 0.16 0.28 0.90 1.00 8398 5129 ## K_M -0.64 0.25 -1.11 -0.12 1.00 5971 5279 ## K_N 0.61 0.28 0.04 1.13 1.00 5973 5529 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_N 0.72 0.14 0.50 1.04 1.00 8317 5584 ## sigma_K 0.87 0.18 0.60 1.28 1.00 6616 5730 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). pairs(b.5H3) seq_M &lt;- tibble(M = seq(0,80,length.out=200)) %&gt;% mutate(M_log = log(M), M_sim = standardize(M_log)) sim &lt;- posterior_samples(b.5H3) %&gt;% rownames_to_column(var = &quot;iter&quot;) %&gt;% rename(I_N = b_N_Intercept, I_K = b_K_Intercept) %&gt;% tidyr::expand(nesting(iter,I_N,I_K,b_N_M, b_K_M, b_K_N, sigma_N, sigma_K), M = seq(0.5,80,length.out=200)) %&gt;% mutate(M_sim = (log(M) - mean(log(d2_2$mass)))/sd(log(d2_2$mass))) %&gt;% mutate(sim_N = rnorm(n(),mean = I_N + b_N_M*M_sim,sigma_N), sim_K = rnorm(n(),mean = I_K + b_K_M*M_sim + b_K_N*sim_N,sigma_K)) sim %&gt;% dplyr::select(M, M_sim,sim_K) %&gt;% group_by(M) %&gt;% mean_qi(sim_K) %&gt;% ggplot(aes(x=M, y = sim_K, ymin = .lower, ymax = .upper))+ geom_ribbon(alpha=3/8)+ geom_line(color = &quot;black&quot;)+ labs(x = &quot;mass&quot;, y= &quot;counterfactual K&quot;) 5.4.7 5H4 Here is an open practice problem to engage your imagination. In the divorce data, States in the southern United States have many of the highest divorce rates. Add the South indicator variable to the analysis. First, draw one or more DAGs that represent your ideas for how Southern American culture might influence any of the other three variables (D, M, or A). Then list the testable implications of your DAGs, if there are any, and fit one or more models to evaluate the implications. What do you think the influence of “Southerness” is? データの分布をみてみる。南部のほうが離婚率が高い。 data(&quot;WaffleDivorce&quot;) d &lt;- WaffleDivorce d %&gt;% ggplot(aes(x=as.factor(South),y=Divorce))+ geom_violin()+ stat_summary(geom = &quot;point&quot;, fun = &quot;median&quot;) d %&gt;% rename(A = MedianAgeMarriage, D = Divorce, M = Marriage, S = South) %&gt;% mutate(M = standardize(M), A = standardize(A), D = standardize(D)) -&gt;d 以下のような因果モデルを考える。 mad_dag &lt;- dagitty(&quot;dag{ M -&gt; A -&gt; D &lt;-M ; M &lt;- S -&gt; A}&quot;) ggdag(mad_dag)+ theme_dag() モデリングを行う。 b1 &lt;- bf(D ~ 1 + A + M + S) b2&lt;- bf(M ~ 1 + A + S) b3 &lt;- bf(A ~ 1 + M + S) b.5H4 &lt;- brm(family=gaussian, data =d, formula = b1 + b2 + b3 + set_rescor(FALSE), prior = c(prior(normal(0,0.2),class=Intercept, resp = D), prior(normal(0,0.5),class=b, resp = D), prior(exponential(1),class=sigma, resp = D), prior(normal(0,0.2),class=Intercept, resp = M), prior(normal(0,0.5),class=b, resp = M), prior(exponential(1),class=sigma, resp = M), prior(normal(0,0.2),class=Intercept, resp = A), prior(normal(0,0.5),class=b, resp = A), prior(exponential(1),class=sigma, resp = A)), seed = 123, iter=4000,warmup=2000,chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5H4&quot;) summary(b.5H4) ## Family: MV(gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: D ~ 1 + A + M + S ## M ~ 1 + A + S ## A ~ 1 + M + S ## Data: d (Number of observations: 50) ## Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup draws = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## D_Intercept -0.10 0.12 -0.33 0.13 1.00 12250 5790 ## M_Intercept 0.05 0.11 -0.17 0.26 1.00 13042 5840 ## A_Intercept 0.10 0.10 -0.10 0.30 1.00 11197 5669 ## D_A -0.56 0.16 -0.86 -0.24 1.00 6887 6388 ## D_M -0.04 0.15 -0.35 0.26 1.00 6858 6283 ## D_S 0.37 0.24 -0.11 0.82 1.00 11215 5985 ## M_A -0.71 0.10 -0.91 -0.50 1.00 11273 6246 ## M_S -0.17 0.21 -0.58 0.25 1.00 12003 6100 ## A_M -0.68 0.10 -0.88 -0.48 1.00 11990 6153 ## A_S -0.36 0.20 -0.75 0.03 1.00 11809 6006 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_D 0.81 0.09 0.66 1.00 1.00 11478 6089 ## sigma_M 0.71 0.08 0.58 0.88 1.00 11514 6271 ## sigma_A 0.69 0.07 0.57 0.85 1.00 11642 6268 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). library(bayesplot) posterior_samples(b.5H4) %&gt;% dplyr::select(-lp__) %&gt;% mcmc_intervals() Lも含むモデルを考える。 b.5H4_2 &lt;- brm(data = d4, family = gaussian, formula = D ~ 1 + A + L + S + M, prior= c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), seed = 5, iter = 2000, warmup = 1000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter5/b.5H4_2&quot;) print(b.5H4_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D ~ 1 + A + L + S + M ## Data: d4 (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.07 0.12 -0.31 0.18 1.00 3294 2797 ## A -0.64 0.17 -0.97 -0.31 1.00 2929 2670 ## L -0.23 0.17 -0.56 0.11 1.00 2852 2822 ## S 0.24 0.26 -0.25 0.75 1.00 3048 2947 ## M 0.07 0.17 -0.28 0.39 1.00 3055 2751 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.80 0.08 0.66 0.98 1.00 3035 2621 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). "],["the-haunted-dag-the-causal-terror.html", "6 The Haunted DAG &amp; the Causal Terror 6.1 Multicollinearity 6.2 Post-treatment bias 6.3 Collider bias 6.4 Confronting confoundings 6.5 Practice 6.6 おまけ ランダム効果を入れてみる", " 6 The Haunted DAG &amp; the Causal Terror Preface 合流点を固定してしまうと、あるいは合流点の値で選抜を行ってしまうと、疑相関が生じてしまう。 例えば、交付金申請に通った研究内のみで「研究の信頼性」と「研究の話題性」の相関を調べる場合。 本来それらには相関がなくても、申請に通った研究（両者の合計点が高い研究）のみに絞ると、負の相関が生じる。 theme_set(theme_classic()) set.seed(200) n &lt;- 500 # number of proposal p &lt;- 0.1 # prop to select d &lt;- tibble(newsworthiness = rnorm(n), trustworthiness = rnorm(n)) d %&gt;% ggplot(aes(x=newsworthiness,y=trustworthiness))+ geom_point()+ theme(aspect.ratio=1)+ geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) 合計点が上位10%のみの研究に絞ると…。 d %&gt;% mutate(total = trustworthiness + newsworthiness) %&gt;% mutate(selected = ifelse(total &gt;= quantile(total, 1-p), TRUE, FALSE)) -&gt; d head(d) ## # A tibble: 6 × 4 ## newsworthiness trustworthiness total selected ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 0.0848 -0.354 -0.269 FALSE ## 2 0.226 -1.93 -1.70 FALSE ## 3 0.433 -0.761 -0.328 FALSE ## 4 0.558 2.34 2.90 TRUE ## 5 0.0598 -2.07 -2.01 FALSE ## 6 -0.115 -1.04 -1.15 FALSE text &lt;- tibble(newsworthiness = c(2, 1), trustworthiness = c(2.25, -2.5), selected = c(TRUE, FALSE), label = c(&quot;selected&quot;, &quot;rejected&quot;)) d %&gt;% ggplot(aes(x = newsworthiness, y=trustworthiness, color = selected))+ geom_point(aes(shape = selected), alpha =3/4)+ geom_text(data = text, aes(label = label))+ scale_shape_manual(values=c(1,19))+ scale_color_manual(values=c(&quot;black&quot;, &quot;navyblue&quot;))+ geom_smooth(data = . %&gt;% filter(selected == &quot;TRUE&quot;), method = &quot;lm&quot;, color = &quot;navyblue&quot;, se = F, size = 1/2, fullrange = T)+ theme(legend.position = &quot;none&quot;, aspect.ratio=1) 本章では、回帰モデルを使用する際にどの変数を加えるべきで、どの変数を加えるべきでないかを検討する。 6.1 Multicollinearity 両脚の長さから身長を予測するモデルを考える。 set.seed(909) n &lt;- 100 d &lt;- tibble(height = rnorm(n, 10, 2), leg_prop = runif(n, 0.4, 0.5)) %&gt;% mutate(leg_left = leg_prop*height + rnorm(n,0,0.02), leg_right = leg_prop*height + rnorm(n, 0, 0.02)) 当然、左右の脚の長さ同士は強く相関するし、どちらも身長と強く相関する。 d %&gt;% dplyr::select(-leg_prop) %&gt;% cor() ## height leg_left leg_right ## height 1.0000000 0.9557084 0.9559273 ## leg_left 0.9557084 1.0000000 0.9997458 ## leg_right 0.9559273 0.9997458 1.0000000 d %&gt;% ggplot(aes(x=leg_left, y=leg_right))+ geom_point()+ theme(aspect.ratio=1)-&gt; p1 d %&gt;% ggplot(aes(x=leg_left, y=height))+ geom_point()+ theme(aspect.ratio=1) -&gt; p2 d %&gt;% ggplot(aes(x=leg_right, y=height))+ geom_point()+ theme(aspect.ratio=1) -&gt; p3 p1+p2+p3 そこで、モデルを回してみると…。 どちらも身長にほとんど影響していないように見えるし、両脚が異なる影響を与えているような推定結果になってしまう。 b6.1 &lt;- brm(data = d, family = gaussian, formula = height ~ 1 + leg_left + leg_right, prior = c(prior(normal(10,100),class=Intercept), prior(normal(2,10),class=b), prior(exponential(1),class = sigma)), iter=2000,warmup=1000,chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.1&quot;) posterior_summary(b6.1) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0.98 0.29 0.4 1.56 ## 2 b_leg_left 0.17 2.59 -4.94 5.17 ## 3 b_leg_right 1.83 2.59 -3.17 6.96 ## 4 sigma 0.63 0.05 0.55 0.73 ## 5 lprior -12.7 0.12 -13.0 -12.6 ## 6 lp__ -109. 1.48 -113. -107. posterior_samples(b6.1) %&gt;% dplyr::select(-lp__) %&gt;% mcmc_intervals() 繰り返しやってみる。 推定結果にかなりばらつきが出る。 sim_and_fit &lt;- function(seed, n = 100) { n &lt;- n set.seed(seed) d &lt;- tibble(height = rnorm(n, mean = 10, sd = 2), leg_prop = runif(n, min = 0.4, max = 0.5)) %&gt;% mutate(leg_left = leg_prop * height + rnorm(n, mean = 0, sd = 0.02), leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02)) fit &lt;- update(b6.1, newdata = d) } #sim &lt;- # tibble(seed = 1:4) %&gt;% #mutate(post = purrr::map(seed, ~sim_and_fit(.) %&gt;% # posterior_samples())) sim &lt;- readRDS(&quot;output/Chapter6/sim.rds&quot;) sim %&gt;% unnest(post) %&gt;% pivot_longer(b_Intercept:sigma) %&gt;% mutate(seed = str_c(&quot;seed &quot;, seed)) %&gt;% ggplot(aes(x = value, y = name)) + stat_pointinterval(.width = .95, color = &quot;forestgreen&quot;) + labs(x = &quot;posterior&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), panel.border = element_rect(color = &quot;black&quot;, fill = &quot;transparent&quot;), panel.grid.minor = element_blank(), strip.text = element_text(hjust = 0)) + facet_wrap(~ seed, ncol = 1) 両脚の係数は強く相関する。 posterior_samples(b6.1) %&gt;% dplyr::select(b_leg_right,b_leg_left) %&gt;% ggplot(aes(x=b_leg_right,y=b_leg_left))+ geom_point() また、両者の合計はおおよそどちらか一つの変数のみを説明変数に入れたときの係数(2程度)と一致する。 posterior_samples(b6.1) %&gt;% mutate(sum_b = b_leg_left+b_leg_right) %&gt;% dplyr::select(sum_b) %&gt;% ggplot(aes(x=sum_b))+ geom_histogram() これは、左右の脚の長さが強く相関しているため、モデルが以下の式のように近似できると考えると上手く説明できる。 $ \\[\\begin{aligned} y_{i} &amp;\\sim Normal(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp;= \\alpha + (\\beta_{1} + \\beta_{2})x_{i} \\end{aligned}\\] $ b6.1_2 &lt;- brm(data = d, family = gaussian, formula = height ~ 1 + leg_right, prior = c(prior(normal(10,100),class=Intercept), prior(normal(2,10),class=b), prior(exponential(1),class = sigma)), iter=2000,warmup=1000,chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.1_2&quot;) posterior_summary(b6.1_2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 5 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0.98 0.29 0.4 1.56 ## 2 b_leg_right 2 0.06 1.87 2.12 ## 3 sigma 0.63 0.05 0.55 0.72 ## 4 lprior -9.38 0.05 -9.47 -9.3 ## 5 lp__ -105. 1.24 -108. -104. 6.1.1 Multicollinear milk 霊長類の母乳のデータを用いて考える。 脂肪分の割合（F）とラクトースの割合（L）でカロリー（K）を予測することを考える。 FとLは強く負に相関する。 data(milk) d2 &lt;- milk %&gt;% mutate(K = standardize(kcal.per.g), F = standardize(perc.fat), L = standardize(perc.lactose)) head(d2) ## clade species kcal.per.g perc.fat perc.protein ## 1 Strepsirrhine Eulemur fulvus 0.49 16.60 15.42 ## 2 Strepsirrhine E macaco 0.51 19.27 16.91 ## 3 Strepsirrhine E mongoz 0.46 14.11 16.85 ## 4 Strepsirrhine E rubriventer 0.48 14.91 13.18 ## 5 Strepsirrhine Lemur catta 0.60 27.28 19.50 ## 6 New World Monkey Alouatta seniculus 0.47 21.22 23.58 ## perc.lactose mass neocortex.perc K F L ## 1 67.98 1.95 55.16 -0.9400408 -1.2172427 1.3072619 ## 2 63.82 2.09 NA -0.8161263 -1.0303552 1.0112855 ## 3 69.04 2.51 NA -1.1259125 -1.3915310 1.3826790 ## 4 71.91 1.62 NA -1.0019980 -1.3355347 1.5868743 ## 5 53.22 2.19 NA -0.2585112 -0.4696927 0.2571148 ## 6 55.20 5.25 64.54 -1.0639553 -0.8938643 0.3979882 d2 %&gt;% dplyr::select(F,K,L) %&gt;% pairs() library(GGally) ggpairs(data = d2, columns = 9:11) それぞれの変数でモデリングしてみる。 b6.2_F &lt;- brm(data = d2, family = gaussian, formula = K ~ 1 + F, prior = c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), iter=4000, warmup = 3000, chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.2_F&quot;) b6.2_L &lt;- brm(data = d2, family = gaussian, formula = K ~ 1 + L, prior = c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), iter=4000, warmup = 3000, chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.2_L&quot;) posterior_summary(b6.2_F) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 5 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.08 -0.17 0.15 ## 2 b_F 0.86 0.1 0.66 1.04 ## 3 sigma 0.49 0.07 0.38 0.65 ## 4 lprior -1.6 0.36 -2.37 -0.98 ## 5 lp__ -22.1 1.3 -25.5 -20.6 posterior_summary(b6.2_L) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 5 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.07 -0.14 0.14 ## 2 b_L -0.9 0.08 -1.05 -0.74 ## 3 sigma 0.41 0.06 0.32 0.55 ## 4 lprior -1.63 0.3 -2.29 -1.12 ## 5 lp__ -17.4 1.3 -20.8 -15.9 次に、両方の変数を説明変数に入れてモデリング。係数が変化しており、ばらつきも大きくなっている。 b6.2_LF &lt;- brm(data = d2, family = gaussian, formula = K ~ 1 + L + F, prior = c(prior(normal(0,0.2),class=Intercept), prior(normal(0,0.5),class=b), prior(exponential(1),class=sigma)), iter=4000, warmup = 3000, chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.2_LF&quot;) posterior_summary(b6.2_LF) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.07 -0.14 0.14 ## 2 b_L -0.67 0.2 -1.06 -0.25 ## 3 b_F 0.25 0.19 -0.14 0.64 ## 4 sigma 0.42 0.06 0.32 0.54 ## 5 lprior -1.42 0.43 -2.56 -0.87 ## 6 lp__ -17.3 1.44 -20.9 -15.4 posterior_summary(b6.2_LF) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.07 -0.14 0.14 ## 2 b_L -0.67 0.2 -1.06 -0.25 ## 3 b_F 0.25 0.19 -0.14 0.64 ## 4 sigma 0.42 0.06 0.32 0.54 ## 5 lprior -1.42 0.43 -2.56 -0.87 ## 6 lp__ -17.3 1.44 -20.9 -15.4 およらく、データには以下のような因果関係がある。 library(ggdag) dag_coords &lt;- tibble(name = c(&quot;L&quot;, &quot;D&quot;, &quot;F&quot;, &quot;K&quot;), x = c(1, 2, 3, 2), y = c(2, 2, 2, 1)) dagify(L ~ D, F ~ D, K ~ L + F, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;D&quot;), alpha = 1/2, size = 6.5, show.legend = F) + geom_point(x = 2, y = 2, size = 6.5, shape = 1, stroke = 1, color = &quot;orange&quot;) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges() + scale_color_manual(values = c(&quot;steelblue&quot;, &quot;orange&quot;)) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) 6.2 Post-treatment bias 土中の真菌に対する処置を行った場合に植物の成長が早くなるかを検討する。 ここでは、処置前の植物の高さ(h0)、処置の有無(T)、処置後の真菌の量(F)、処置後の植物の高さ(h1)を考える。どの変数をモデルに含めるべきだろうか? もし処置の結果を知りたいのであれば、Fを説明変数に入れてはいけない。それはpost-treatment effectだからである。 set.seed(71) n &lt;- 100 d3 &lt;- tibble( h0 = rnorm(n, 10,2), T = rep(0:1,each=n/2), F = rbinom(n, size = 1, prob = .5 - T*0.4), h1 = h0 + rnorm(n, mean = 5-3*F, sd=1)) d3 %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mean_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 F 0.23 0 1 0.89 mean qi ## 2 h0 9.96 6.57 13.1 0.89 mean qi ## 3 h1 14.4 10.6 17.9 0.89 mean qi ## 4 T 0.5 0 1 0.89 mean qi まず、処置前の高さのみを入れた以下のモデルを考える。 このとき、\\(p\\)は必ず0以上になるので、事前分布として対数正規分布を用いる。 \\(h_{1,i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = h_{0,i}×p\\) \\(p \\sim Lognormal(0, 0.25)\\) \\(\\sigma \\sim Exponential(1)\\) 推定結果は以下の通り。 40%強の成長が見込まれる。 b6.3_h &lt;- brm(data=d3, family = gaussian, formula = h1 ~ 0 + h0, prior = c(prior(lognormal(0, 0.25),class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.3_h&quot;) posterior_summary(b6.3_h) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 4 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_h0 1.43 0.02 1.39 1.46 ## 2 sigma 1.83 0.13 1.59 2.11 ## 3 lprior -2.72 0.16 -3.07 -2.44 ## 4 lp__ -204. 1.02 -207. -203. 続いて、処置の有無(T)と処置後の真菌の有無(F)もいれた以下のモデルを考える。 \\(h_{1,i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = h_{0,i}×p\\) \\(p = \\alpha + \\beta_{T}T_{i} + \\beta_{F}F_{i}\\) \\(\\alpha \\sim Lognormal(0, 0.25)\\) \\(\\beta \\sim Normal(0, 0.5)\\) \\(\\sigma \\sim Exponential(1)\\) 結果、処置の結果が0に近いと推定される。 b6.3_htf &lt;- brm( data = d3, family = gaussian, formula = bf(h1 ~ h0*(a + t*T + f*F), a+t+f ~ 1, nl = TRUE), prior=c(prior(lognormal(0,0.2),nlpar = a,lb = 0), prior(normal(0, 0.5), nlpar = t), prior(normal(0, 0.5), nlpar = f), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, seed=6, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.3_htf&quot; ) posterior_summary(b6.3_htf) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_a_Intercept 1.48 0.03 1.43 1.53 ## 2 b_t_Intercept 0 0.03 -0.06 0.06 ## 3 b_f_Intercept -0.27 0.04 -0.34 -0.19 ## 4 sigma 1.45 0.1 1.26 1.67 ## 5 lprior -3.68 0.23 -4.17 -3.26 ## 6 lp__ -182. 1.5 -186. -180. pairs(b6.3_htf) 6.2.1 Blocked by consequence これは、処置後の真菌を入れたことによって、T -&gt; h1へのパスがブロックされたからである。（h1とTについてバックドア基準を満たさない） よって、Fを除外してモデリングする必要がある。 b6.3_ht &lt;- brm( data = d3, family = gaussian, formula = bf(h1 ~ h0*(a + t*T), a+t ~ 1, nl = TRUE), prior=c(prior(lognormal(0,0.2),nlpar = a,lb = 0), prior(normal(0, 0.5), nlpar = t), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, seed =6, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.3_ht&quot; ) posterior_summary(b6.3_ht) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 5 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_a_Intercept 1.38 0.03 1.33 1.43 ## 2 b_t_Intercept 0.08 0.04 0.02 0.15 ## 3 sigma 1.79 0.13 1.56 2.05 ## 4 lprior -2.97 0.2 -3.39 -2.6 ## 5 lp__ -202. 1.23 -205. -201. 6.2.2 Fungus and d-separation DAGを用いて考えてみる。 dag_coords &lt;- tibble(name = c(&quot;H0&quot;, &quot;T&quot;, &quot;F&quot;, &quot;H1&quot;), x = c(1, 5, 4, 3), y = c(2, 2, 1.5, 1)) dag &lt;- dagify(F ~ T, H1 ~ H0 + F, coords = dag_coords) 上記の因果モデルにおいて、条件付き独立を求めると、Fで条件づけたときにH1とTが独立になってしまうことが分かる。 このとき、H1とTはFで条件づけたときにd分離しているという。 impliedConditionalIndependencies(dag) ## F _||_ H0 ## H0 _||_ T ## H1 _||_ T | F 続いて、以下のような因果モデルを考える。 このモデルでは、H0と水分(M、非観測変数)のみがH1に影響を与え、MとTがFに影響を与えるとする。このとき, Tだけを説明変数に入れてもH1との関連は見られないが、Fを入れるとTとH1に関連がみられる。 dag_coords &lt;- tibble(name = c(&quot;H0&quot;, &quot;H1&quot;, &quot;M&quot;, &quot;F&quot;, &quot;T&quot;), x = c(1, 2, 2.5, 3, 4), y = c(2, 2, 1, 2, 2)) # save our DAG dag &lt;- dagify(F ~ M + T, H1 ~ H0 + M, coords = dag_coords) dag %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;M&quot;), alpha = 1/2, size = 6.5, show.legend = F) + geom_point(x = 2.5, y = 1, size = 6.5, shape = 1, stroke = 1, color = &quot;orange&quot;) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges() + scale_color_manual(values = c(&quot;steelblue&quot;, &quot;orange&quot;)) + theme_dag() set.seed(71) n &lt;- 1000 d4 &lt;- tibble( h0 = rnorm(n, 10,2), T = rep(0:1, each = n/2), M = rbern(n), F = rbinom(n, size=1, prob = 0.5-T*0.4+0.4*M), h1 = h0 + rnorm(n, mean =5 + 3*M, sd=1) ) head(d4) ## # A tibble: 6 × 5 ## h0 T M F h1 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9.14 0 0 0 14.8 ## 2 9.11 0 0 0 15.3 ## 3 9.04 0 1 1 16.4 ## 4 10.8 0 1 1 19.1 ## 5 9.16 0 1 1 17.2 ## 6 7.63 0 0 0 13.4 b6.3_ht2 &lt;- update(b6.3_ht, newdata = d4, seed =6, file = &quot;output/Chapter6/b6.3_ht2&quot;) b6.3_htf2 &lt;- update(b6.3_htf, newdata = d4, seed =6, file = &quot;output/Chapter6/b6.3_htf2&quot;) posterior_summary(b6.3_ht2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 5 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_a_Intercept 1.62 0.01 1.6 1.64 ## 2 b_t_Intercept -0.01 0.01 -0.04 0.02 ## 3 sigma 2.21 0.05 2.12 2.31 ## 4 lprior -5.17 0.09 -5.36 -4.99 ## 5 lp__ -2216. 1.21 -2219. -2215. posterior_summary(b6.3_htf2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_a_Intercept 1.52 0.01 1.5 1.55 ## 2 b_t_Intercept 0.05 0.01 0.02 0.08 ## 3 b_f_Intercept 0.14 0.01 0.12 0.17 ## 4 sigma 2.11 0.05 2.02 2.2 ## 5 lprior -4.54 0.11 -4.75 -4.34 ## 6 lp__ -2169. 1.41 -2172. -2167. これはなぜだろうか？ 次節でこの問題を見ていく。 6.3 Collider bias 再び、本章冒頭の例を考える。 DAGは以下のようになる。 dag_coords &lt;- tibble(name = c(&quot;T&quot;, &quot;S&quot;, &quot;N&quot;), x = c(1, 2, 3), y = c(2, 1, 2)) dag &lt;- dagify(S ~ T, S ~ N, coords = dag_coords) gg_simple_dag(dag) このとき、Sについて条件づけると、TとNの間に疑似相関が生じてしまう。 6.3.1 Collider of false sorrow 年齢(A)が幸福(H)にどう影響するかを検討したいとする。 ここでは、幸せは生まれた瞬間に決定し、年齢とは関係ないと仮定する。一方で、年齢と幸福が結婚(M)に影響を与えるとする。 DAGは以下の通り。 dag_coords &lt;- tibble(name = c(&quot;H&quot;, &quot;M&quot;, &quot;A&quot;), x = c(1, 2, 3), y = c(2, 1, 2)) dag &lt;- dagify(M ~ H, M ~ A, coords = dag_coords) gg_simple_dag(dag) シミュレーションをする。 d5 &lt;- sim_happiness(seed = 1977, N_years = 1000) posterior_summary(d5) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 3 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 33 18.8 2 64 ## 2 married 0.3 0.46 0 1 ## 3 happiness 0 1.21 -2 2 d5 %&gt;% ggplot(aes(x = age, y = happiness, fill= as.factor(married)))+ geom_point(size=2.3, shape=21, color = &quot;black&quot;) + scale_fill_manual(values = c(&quot;white&quot;,&quot;blue&quot;))+ theme_bw()+ theme(legend.position = &quot;none&quot;) まず、以下のモデルを考え、結婚状態をコントロールうえで年齢が幸福度に与える影響を考える。 \\(\\mu_{i} = \\alpha_{MID[i]} + \\beta_{A}A_{i}\\) d5 %&gt;% filter(age &gt;17) %&gt;% mutate(A = (age-18)/(65-18)) %&gt;% mutate(M = factor(married + 1, labels = c(&quot;single&quot;, &quot;married&quot;))) -&gt; d6 モデルを回すと、年齢が幸福と負に関連していることになる。 b6.4 &lt;- brm(data = d6, family = gaussian, formula = happiness ~0 + M + A, prior = c(prior(normal(0,1),class = b, coef=Mmarried), prior(normal(0,1),class = b, coef = Msingle), prior(normal(0,2),class = b, coef = A), prior(exponential(1), class = sigma)), iter = 2000, warmup=1000, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.4&quot;) posterior_summary(b6.4) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Msingle -0.24 0.06 -0.36 -0.11 ## 2 b_Mmarried 1.26 0.09 1.09 1.43 ## 3 b_A -0.75 0.12 -0.97 -0.53 ## 4 sigma 0.99 0.02 0.95 1.04 ## 5 lprior -5.34 0.12 -5.6 -5.12 ## 6 lp__ -1360. 1.43 -1364. -1359. 一方で、年齢のみを説明変数に入れると、年齢の影響はなくなる。 つまり、結婚の有無を変数に入れたことによって年齢と幸福度の間に疑似相関が生じていた。 b6.4_2 &lt;- brm(data = d6, family = gaussian, formula = happiness ~1 + A, prior = c(prior(normal(0,1),class = b), prior(normal(0,2),class = Intercept), prior(exponential(1), class = sigma)), iter = 2000, warmup=1000, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6.4_2&quot;) posterior_summary(b6.4_2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 5 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.08 -0.15 0.15 ## 2 b_A 0 0.13 -0.26 0.25 ## 3 sigma 1.22 0.03 1.16 1.27 ## 4 lprior -3.76 0.03 -3.82 -3.7 ## 5 lp__ -1553. 1.23 -1556. -1552. 6.3.2 The haunted DAG 以下の例を考える。 両親(P)と祖父母(G)が子どもの学業成績(C)に直接影響を与えているが、GはPを通してもCに影響を与えているとする。 また、未観測の変数(U)がPとCに影響を与えているとする。 dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;, &quot;U&quot;), x = c(1, 2, 2, 2.5), y = c(2, 2, 1, 1.5)) dag &lt;- dagify(C ~ G + P + U, P ~ G + U, coords = dag_coords) gg_fancy_dag(dag, x=2.5,y=1.5,circle = &quot;U&quot;) 以下のようにシミュレーションする。 ここで、GがCに直接与える影響は0と仮定する。 n &lt;- 200 b_GC &lt;- 0 b_GP &lt;- 1 b_PC &lt;- 1 b_U &lt;- 2 set.seed(1) d7 &lt;- tibble( U = 2*rbern(n, 0.5)-1, G = rnorm(n), P = rnorm(n, b_GP*G + b_U*U,sd=1), C = rnorm(n, b_PC*P + b_U*U + b_GC*G,sd=1) ) head(d7) ## # A tibble: 6 × 4 ## U G P C ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1 -0.620 -1.73 -3.65 ## 2 -1 0.0421 -3.01 -5.30 ## 3 1 -0.911 3.06 3.88 ## 4 1 0.158 1.77 3.79 ## 5 -1 -0.655 -1.00 -2.01 ## 6 1 1.77 5.28 8.87 ggpairs(d7) C ~ G + Pのモデリングを行うと…。 Pの影響が2倍近くになっており、Gが負の影響を与えているという結果になってしまっている。 b6.5 &lt;- brm(data = d7, family = gaussian, formula = C ~ 0 + Intercept+P+G, prior = c(prior(normal(0,1),class = b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =6, file = &quot;output/Chapter6/b6.5&quot;) posterior_summary(b6.5) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept -0.12 0.1 -0.32 0.07 ## 2 b_P 1.79 0.04 1.7 1.87 ## 3 b_G -0.84 0.11 -1.06 -0.62 ## 4 sigma 1.43 0.07 1.29 1.58 ## 5 lprior -6.15 0.16 -6.47 -5.85 ## 6 lp__ -361. 1.43 -365. -359. これは、Pを固定（例えば40-60%のものにデータを限定）してみるとよくわかる。 Pを固定したとき、Gが低ければUは高くなるし、Gが高ければUは低くなる（U -&gt; P &lt;- Gなので）。そして、Uが高いほどはCも高いので、結果的にGが低いほどCも高いという結果が得られることになる。 d7 %&gt;% mutate(C = standardize(C), G = standardize(G)) %&gt;% mutate(P40_60 = ifelse(P &lt; quantile(P,probs = .6)&amp; P &gt; quantile(P, probs = .4), TRUE,FALSE)) %&gt;% ggplot(aes(x=G, y=C))+ geom_point(aes(shape = as.factor(P40_60), color = as.factor(U)), size=3.5, stroke=3/4)+ scale_color_manual(values = c(&quot;black&quot;,&quot;navy&quot;))+ scale_shape_manual(values =c(1,19))+ geom_smooth(data = . %&gt;% filter(P40_60==&quot;TRUE&quot;), method = &quot;lm&quot;, se = FALSE, fullrange=T, color = &quot;black&quot;, size=1/2)+ labs(x=&quot;granpa education (G)&quot;, y = &quot;grandchild eduaction (C)&quot;, title = &quot;Parents in 45th and 60th centiles&quot;)+ theme(aspect.ratio=1, legend.position = &quot;none&quot;)+ annotate(geom=&quot;text&quot;, x=-2,y=2,label=&quot;U = 1&quot;, color = &quot;navy&quot;)+ annotate(geom=&quot;text&quot;, x=2,y=-2,label=&quot;U = -1&quot;, color = &quot;black&quot;) ここで、Uが観測できたとしてモデリングを行うと、Gの効果はなくなり、Pの効果も正しく推定できるようになる。 b6.5_2 &lt;- brm(data = d7, family = gaussian, formula = C ~ 0 + Intercept+P+G+U, prior = c(prior(normal(0,1),class = b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =6, file = &quot;output/Chapter6/b6.5_2&quot;) posterior_summary(b6.5_2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 7 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept -0.12 0.07 -0.26 0.02 ## 2 b_P 1.02 0.07 0.88 1.14 ## 3 b_G -0.05 0.1 -0.24 0.15 ## 4 b_U 1.98 0.15 1.69 2.27 ## 5 sigma 1.04 0.05 0.94 1.14 ## 6 lprior -7.23 0.24 -7.74 -6.8 ## 7 lp__ -298. 1.61 -302. -296. 6.4 Confronting confoundings 上記のように、モデルに入れる変数を身長に選ばなければ交絡が生じてしまう。 ゆえに、バックドア基準を満たすような変数を入れることが肝要である。 因果モデルはどんなに複雑であろうと、以下の4つの関係の組み合わせからなる。 The fork Zで条件づけられれば、XとYは独立になる。 The Pipe Zで条件づけるとXからYへのパスがふさがれてしまう。 The Collider Zで条件づけると、XとYの間に疑似相関が生じてしまう。 The Descendant Dで条件づけるとZも部分的に条件づけられてしまうため、XとYの間に疑似相関が生じてしまう。 6.4.1 Two roads 以下のDAGを考える。 バックドアをふさぐためには、AかCのすくなくともどちらかについて条件づける必要がある。Cをいれれば推定の制度も上がるので、Cを入れる方がよりよい（両方入れてもよいが、あまり意味はない）。 dag_6.1 &lt;- dagitty( &quot;dag { U [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C }&quot; ) adjustmentSets(dag_6.1, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) ## { C } ## { A } 6.4.2 Backdoor Waffles 次に、Waffle House Divorce につい以下のモデルを考える。 AとM、もしくはSで条件づければWとDの間に余計な関係は生じない。 dag_6.2 &lt;- dagitty( &quot;dag { A -&gt; D A -&gt; M -&gt; D A &lt;- S -&gt; M S -&gt; W -&gt; D }&quot; ) adjustmentSets(dag_6.2, exposure = &quot;W&quot;, outcome = &quot;D&quot;) ## { A, M } ## { S } 6.5 Practice 6.5.1 6M1 Modify the DAG on page 186 to include the variable V, an unobserved cause of C and Y: C←V→Y. Reanalyze the DAG. How many paths connect X to Y? Which must be closed? Which variables should you condition on now? new_dag &lt;- dagitty(&quot;dag { U [unobserved] V [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C C &lt;- V -&gt; Y }&quot;) adjustmentSets(new_dag, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) ## { A } 6.5.2 6M2 Sometimes, in order to avoid multicollinearity, people inspect pairwise correlations among predictors before including them in a model. This is a bad procedure, because what matters is the conditional association, not the association before the variables are included in the model. To highlight this, consider the DAG X→Z→Y. Simulate data from this DAG so that the correlation between X and Z is very large. Then include both in a model prediction Y . Do you observe any multicollinearity? Why or why not? What is different from the legs example in the chapter? set.seed(1984) n &lt;- 100 dat &lt;- tibble(x = rnorm(n,0,1)) %&gt;% mutate(z = rnorm(n,x,0.4), y = rnorm(n,z,1)) cor(dat) ## x z y ## x 1.0000000 0.9325230 0.7431466 ## z 0.9325230 1.0000000 0.8063858 ## y 0.7431466 0.8063858 1.0000000 pairs(dat) 上手く推定できてると思われる。 b6M2 &lt;- brm( data = dat, family = gaussian, formula = y ~ 1 + z + x, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0,0.5),class =b), prior(exponential(1),class=sigma)), iter=4000, warmup=2000, chains =4, seed=1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6M2&quot; ) posterior_summary(b6M2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept -0.12 0.09 -0.29 0.06 ## 2 b_z 1.08 0.21 0.66 1.49 ## 3 b_x 0.14 0.23 -0.31 0.6 ## 4 sigma 0.96 0.07 0.84 1.11 ## 5 lprior -3.54 0.86 -5.6 -2.29 ## 6 lp__ -142. 1.45 -145. -140. posterior_samples(b6M2) %&gt;% as_tibble() %&gt;% dplyr::select(-lp__) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + stat_halfeye(.width = c(0.67, 0.89, 0.97)) pairs(b6M2) 繰り返してみる。 推定結果も両脚と身長の例ほどはばらついていない。 sim_and_fit &lt;- function(seed, n = 100) { n &lt;- n set.seed(seed) dat &lt;- tibble(x = rnorm(n,0,1)) %&gt;% mutate(z = rnorm(n,x,0.4), y = rnorm(n,z,1)) fit &lt;- update(b6M2, newdata = dat) } #sim2 &lt;- # tibble(seed = 1:4) %&gt;% #mutate(post = purrr::map(seed, ~sim_and_fit(.) %&gt;% # posterior_samples())) sim2 &lt;- readRDS(&quot;output/Chapter6/sim2.rds&quot;) sim2 %&gt;% unnest(post) %&gt;% pivot_longer(b_Intercept:sigma) %&gt;% mutate(seed = str_c(&quot;seed &quot;, seed)) %&gt;% ggplot(aes(x = value, y = name)) + stat_pointinterval(.width = .95, color = &quot;forestgreen&quot;) + labs(x = &quot;posterior&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), panel.border = element_rect(color = &quot;black&quot;, fill = &quot;transparent&quot;), panel.grid.minor = element_blank(), strip.text = element_text(hjust = 0)) + facet_wrap(~ seed, ncol = 1) これは、身長の例では両脚(L,R)がどちらも身長(H)を予測するのに対して、今回の例ではZのみがYを予測し、XからYへのパスはブロックされているから。 このように、多重共線性が生じるかは単純に変数間の相関だけではわからず、因果モデルを考える必要がある。 6.5.3 6H1 Use the Waffle House data, data(WaffleDivorce), to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph. data(&quot;WaffleDivorce&quot;) dat2 &lt;- WaffleDivorce %&gt;% mutate(M = standardize(Marriage), A = standardize(MedianAgeMarriage), D = standardize(Divorce), S = factor(South + 1, labels = c(&quot;North&quot;, &quot;South&quot;)), W = standardize(WaffleHouses)) Sを入れてモデリング。 Wはほとんど影響していないことが分かる。 b6H1 &lt;- brm(data = dat2, family = gaussian, formula = D ~ 0 + W + S, prior = c(prior(normal(0,0.5),class =b), prior(exponential(1),class = sigma)), iter=4000, warmup=2000, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6H1&quot;) posterior_summary(b6H1) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_W 0.09 0.16 -0.23 0.41 ## 2 b_SNorth -0.16 0.17 -0.49 0.17 ## 3 b_SSouth 0.36 0.27 -0.17 0.88 ## 4 sigma 0.97 0.1 0.79 1.19 ## 5 lprior -2.22 0.47 -3.48 -1.67 ## 6 lp__ -71.4 1.42 -75.0 -69.6 6.5.4 6H2 Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be ammended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren’t in the data. DAGの条件付き独立がどのような時に起こるかを調べてみる。 以下の3つをモデリングしてみる。 dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;D&quot;, &quot;M&quot;, &quot;S&quot;, &quot;W&quot;), x = c(1, 3, 2, 1, 3), y = c(1, 1, 2, 3, 3)) waffle_dag &lt;- dagify(A ~ S, D ~ A + M + W, M ~ A + S, W ~ S, coords = dag_coords) impliedConditionalIndependencies((waffle_dag)) ## A _||_ W | S ## D _||_ S | A, M, W ## M _||_ W | S モデルの結果、1と3については条件付き独立を支持する結果が得られたた。 2については信用区間が0にまたがっているが若干あいまいな結果だった。これはおそらくまだ他の要因が関わっているからだと思われる。 b6H2_1 &lt;- brm(data = dat2, family = gaussian, formula = A ~ 1 + W + South, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0,0.5),class =b), prior(exponential(1),class = sigma)), iter=4000, warmup=2000, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6H2_1&quot;) b6H2_2 &lt;- brm(data = dat2, family = gaussian, formula = D ~ 1 + A + M + W + South, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0,0.5),class =b), prior(exponential(1),class = sigma)), iter=4000, warmup=2000, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6H2_2&quot;) b6H2_3 &lt;- brm(data = dat2, family = gaussian, formula = M ~ 1 + W + South, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0,0.5),class =b), prior(exponential(1),class = sigma)), iter=4000, warmup=2000, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6H2_3&quot;) posterior_summary(b6H2_1) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0.11 0.15 -0.17 0.4 ## 2 b_W 0.01 0.17 -0.33 0.33 ## 3 b_South -0.4 0.32 -1.02 0.25 ## 4 sigma 1 0.1 0.82 1.23 ## 5 lprior -1.51 0.66 -3.22 -0.75 ## 6 lp__ -72.1 1.44 -75.7 -70.3 posterior_summary(b6H2_2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 8 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept -0.07 0.13 -0.32 0.18 ## 2 b_A -0.55 0.16 -0.86 -0.23 ## 3 b_M -0.04 0.16 -0.34 0.27 ## 4 b_W 0.11 0.14 -0.17 0.38 ## 5 b_South 0.23 0.29 -0.33 0.8 ## 6 sigma 0.81 0.09 0.66 1 ## 7 lprior -2.21 0.54 -3.53 -1.43 ## 8 lp__ -62.7 1.81 -67.2 -60.2 posterior_summary(b6H2_3) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept -0.04 0.15 -0.33 0.25 ## 2 b_W -0.02 0.17 -0.35 0.31 ## 3 b_South 0.15 0.32 -0.48 0.76 ## 4 sigma 1.02 0.11 0.84 1.26 ## 5 lprior -1.27 0.48 -2.49 -0.72 ## 6 lp__ -73.1 1.44 -76.8 -71.3 6.5.5 6H3 Use a model to infer the total causal influence of area on weight. Would increasing the area available to each fox make it heavier (healthier)? You might want to standardize the variables. Regardless, use prior predictive simulation to show that your model’s prior predictions stay within the possible outcome range. areaとweightは他の変数を条件づけなくてもバックドア基準を満たすため、どの変数も入れなくてよい。 dag_fox &lt;- dagitty( &quot;dag{ area -&gt; avgfood -&gt; weight &lt;- groupsize ; avgfood -&gt; groupsize }&quot; ) adjustmentSets(dag_fox, exposure = &quot;area&quot;, outcome =&quot;weight&quot;) ## {} data(foxes) dat3 &lt;- foxes %&gt;% as_tibble() %&gt;% dplyr::select(area, avgfood, weight, groupsize) %&gt;% mutate(across(everything(),standardize)) head(dat3) ## # A tibble: 6 × 4 ## area avgfood weight groupsize ## &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; ## 1 -2.239596 -1.924829 0.4141347 -1.524089 ## 2 -2.239596 -1.924829 -1.4270464 -1.524089 ## 3 -1.205508 -1.118035 0.6759540 -1.524089 ## 4 -1.205508 -1.118035 1.3009421 -1.524089 ## 5 -1.130106 -1.319734 1.1151348 -1.524089 ## 6 -1.130106 -1.319734 -1.0807692 -1.524089 pairs(dat3) b6H3 &lt;- brm(data = dat3, family = gaussian, formula = weight ~ 1 + area, prior = c(prior(normal(0, 0.2),class=Intercept), prior(normal(0, 0.5), class = b,), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6H3&quot;) areaはほとんど影響していなかった。 posterior_samples(b6H3) %&gt;% as_tibble() %&gt;% dplyr::select(-lp__) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Intercept&quot;, &quot;b_area&quot;, &quot;sigma&quot;))) %&gt;% ggplot(aes(x = value, y = fct_rev(name))) + stat_halfeye(.width = c(0.67, 0.89, 0.97)) + labs(x = &quot;Parameter Estimate&quot;, y = &quot;Parameter&quot;) 6.5.6 6H4 Now infer the causal impact of adding food to a territory. Would this make foxes heavier? Which covariates do you need to adjust for to estimate the total causal influence of food? 続いて、えさの量が体重に与える影響を考える。 えさの量が体重に与える影響を評価するために他の変数を加える必要はない。 adjustmentSets(dag_fox, exposure = &quot;avgfood&quot;, outcome =&quot;weight&quot;) ## {} えさの量も影響せず。 b6H4 &lt;- brm(data = dat3, family = gaussian, formula = weight ~ 1 + avgfood, prior = c(prior(normal(0, 0.2),class=Intercept), prior(normal(0, 0.5), class = b,), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6H4&quot;) posterior_samples(b6H4) %&gt;% as_tibble() %&gt;% dplyr::select(-lp__) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Intercept&quot;, &quot;b_avgfood&quot;, &quot;sigma&quot;))) %&gt;% ggplot(aes(x = value, y = fct_rev(name))) + stat_halfeye(.width = c(0.67, 0.89, 0.97)) + labs(x = &quot;Parameter Estimate&quot;, y = &quot;Parameter&quot;) 6.5.7 6H5 Now infer the causal impact of group size. Which covariates do you need to adjust for? Looking at the posterior distribution of the resulting model, what do you think explains these data? That is, can you explain the estimates for all three problems? How do they go together? 最後にgroupsizeの影響を見る。その際には、avgfoodも説明変数に入れる必要がある。 その結果、groupsizeはavgfoodを固定したときに負の影響を与えていることが分かった。また、avgfoodはgroupsizeを固定したときに正の影響を与えることが分かった。 ただし、avgfoodがweightに与える全体的な影響は前問で見たように0である。これは、avgfoodが増えればgroupsizeも増えてしまうからである。 b6H5 &lt;- brm(data = dat3, family = gaussian, formula = weight ~ 1 + avgfood + groupsize, prior = c(prior(normal(0, 0.2),class=Intercept), prior(normal(0, 0.5), class = b,), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, file = &quot;output/Chapter6/b6H5&quot;) posterior_samples(b6H5) %&gt;% as_tibble() %&gt;% dplyr::select(-lp__) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Intercept&quot;, &quot;b_avgfood&quot;,&quot;b_groupsize&quot;, &quot;sigma&quot;))) %&gt;% ggplot(aes(x = value, y = fct_rev(name))) + stat_halfeye(.width = c(0.67, 0.89, 0.97)) + labs(x = &quot;Parameter Estimate&quot;, y = &quot;Parameter&quot;) posterior_summary(b6H5) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.08 -0.16 0.16 ## 2 b_avgfood 0.47 0.18 0.1 0.82 ## 3 b_groupsize -0.56 0.18 -0.91 -0.2 ## 4 sigma 0.96 0.07 0.85 1.1 ## 5 lprior -2.01 0.74 -3.75 -0.92 ## 6 lp__ -162. 1.41 -166. -160. 6.6 おまけ ランダム効果を入れてみる dat3 &lt;- foxes %&gt;% as_tibble() %&gt;% mutate(across(2:5,standardize)) head(dat3) ## # A tibble: 6 × 5 ## group avgfood groupsize area weight ## &lt;int&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; ## 1 1 -1.924829 -1.524089 -2.239596 0.4141347 ## 2 1 -1.924829 -1.524089 -2.239596 -1.4270464 ## 3 2 -1.118035 -1.524089 -1.205508 0.6759540 ## 4 2 -1.118035 -1.524089 -1.205508 1.3009421 ## 5 3 -1.319734 -1.524089 -1.130106 1.1151348 ## 6 3 -1.319734 -1.524089 -1.130106 -1.0807692 b6H5_2 &lt;- brm(data = dat3, family = gaussian, formula = weight ~ 1 + avgfood + groupsize + (1|group), prior = c(prior(normal(0, 0.2),class=Intercept), prior(normal(0, 0.5), class = b,), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b6H5_2&quot;) posterior_summary(b6H5_2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% as_tibble() ## # A tibble: 37 × 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.09 -0.17 0.18 ## 2 b_avgfood 0.46 0.2 0.07 0.84 ## 3 b_groupsize -0.55 0.2 -0.93 -0.16 ## 4 sd_group__Intercept 0.18 0.12 0.01 0.44 ## 5 sigma 0.95 0.07 0.83 1.09 ## 6 r_group[1,Intercept] -0.04 0.2 -0.5 0.35 ## 7 r_group[2,Intercept] 0.06 0.2 -0.29 0.53 ## 8 r_group[3,Intercept] -0.02 0.2 -0.48 0.38 ## 9 r_group[4,Intercept] -0.02 0.19 -0.46 0.38 ## 10 r_group[5,Intercept] 0.08 0.2 -0.27 0.59 ## # … with 27 more rows "],["ulysses-compass.html", "7 Ulysses’ Compass 7.1 The problems with parameters 7.2 Entropy and accuracy 7.3 Golem taming: regularization 7.4 Predicting predictive accuracy 7.5 Model comparison 7.6 Practice", " 7 Ulysses’ Compass 7.1 The problems with parameters 説明変数が多ければ多いほどモデルのデータへの当てはまりは良くなる。しかし、既存のデータへの当てはまりがよくなりすぎると(overfitting)、新たなデータを予測する制度が低くなってしまう。一方で、シンプルすぎるモデルもうまく予測ができない(underfitting)。 7.1.1 More parameters always improve fit ホミニンの脳容量と体重の関係を考える。 (d &lt;- tibble(species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;, &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), brain= c(438, 452, 612, 521, 752, 871, 1350), mass = c(37.0, 35.5,34.5,41.5,55.5,61.0,53.5))) ## # A tibble: 7 × 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 africanus 452 35.5 ## 3 habilis 612 34.5 ## 4 boisei 521 41.5 ## 5 rudolfensis 752 55.5 ## 6 ergaster 871 61 ## 7 sapiens 1350 53.5 d %&gt;% ggplot(aes(x=mass,y=brain))+ geom_point(size=3.5,shape=1,color=&quot;navy&quot;)+ geom_text_repel(aes(label = species))+ scale_y_continuous(breaks = seq(600,1200,200))+ scale_x_continuous(limits = c(30,70),breaks=seq(30,70,10))+ theme(aspect.ratio=1)+ labs(x=&quot;body mass (kg)&quot;,y=&quot;brain volume (cc)&quot;, subtitle = &quot;Average brain volume by body\\nmass for six hominin species&quot;) 標準化する。 d %&gt;% mutate(M = (mass-mean(mass))/sd(mass), B = brain/max(brain)) -&gt; d 7.1.1.1 単回帰 まずは単純な線形回帰を考える。モデルは以下の通り。 \\(B_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\beta M_{i}\\) \\(\\alpha \\sim Normal(0.5,1)\\) \\(\\beta \\sim Normal(0,10)\\) \\(\\sigma \\sim Lognormal(0,1)\\) b7.1 &lt;- brm(data = d, family = gaussian, formula = B ~ 1 + M, prior = c(prior(normal(0.5, 1),class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), seed =7, file = &quot;output/Chapter7/b7.1&quot;) posterior_summary(b7.1) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% as_tibble() ## # A tibble: 5 × 5 ## parameters Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0.53 0.12 0.32 0.76 ## 2 b_M 0.16 0.12 -0.09 0.39 ## 3 sigma 0.27 0.12 0.13 0.58 ## 4 lprior -4.72 0.15 -5.11 -4.56 ## 5 lp__ -5.63 1.66 -10.1 -3.78 \\(R^2\\)を計算する。 R2 &lt;- function(brm_fit, seed = 7, ...) { set.seed(seed) p &lt;- predict(brm_fit, summary = F, ...) r &lt;- apply(p, 2, mean) - d$B 1 - rethinking::var2(r)/rethinking::var2(d$B) } R2(b7.1,seed=7) ## [1] 0.4947021 7.1.1.2 多項回帰 続いて、2乗項を入れたモデルを考える。 \\(B_{i} \\sim Normal(\\mu_{i}, \\sigma)\\) \\(\\mu_{i} = \\alpha + \\beta_{1} M{i} + \\beta_{2} M_{i}^2\\) \\(\\alpha \\sim Normal(0.5,1)\\) \\(\\beta_{j} \\sim Normal(0,10) \\;\\; j =1,2\\) \\(\\sigma \\sim Lognormal(0,1)\\) b7.2 &lt;- brm(data = d, family = gaussian, formula = B ~ 1 + M + I(M^2), prior = c(prior(normal(0.5, 1),class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), seed =7, file = &quot;output/Chapter7/b7.2&quot;) posterior_summary(b7.2) %&gt;% round(2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% as_tibble() ## # A tibble: 6 × 5 ## parameters Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0.61 0.26 0.1 1.15 ## 2 b_M 0.19 0.16 -0.13 0.52 ## 3 b_IME2 -0.1 0.26 -0.62 0.43 ## 4 sigma 0.31 0.15 0.14 0.7 ## 5 lprior -7.92 0.15 -8.31 -7.78 ## 6 lp__ -9.54 1.99 -14.4 -7.01 R2(b7.2) ## [1] 0.540159 同様に、3乗項～6乗項を順番に足したモデルを考える。 b7.3 &lt;- brm(data = d, family = gaussian, formula = B ~ 1 + M + I(M^2)+I(M^3), prior = c(prior(normal(0.5, 1),class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), control = list(adapt_delta = .99), seed =7, file = &quot;output/Chapter7/b7.3&quot;) b7.4 &lt;- brm(data = d, family = gaussian, formula = B ~ 1 + M + I(M^2)+I(M^3)+I(M^4), prior = c(prior(normal(0.5, 1),class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), iter =18000, warmup=10000,thin=4, control = list(adapt_delta = .995, max_treedepth = 15), seed =7, file = &quot;output/Chapter7/b7.4&quot;) b7.5 &lt;- brm(data = d, family = gaussian, formula = B ~ 1 + M + I(M^2)+I(M^3)+I(M^4)+I(M^5), prior = c(prior(normal(0.5, 1),class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), iter=20000,warmup=18000,thin=1, control = list(adapt_delta = .999, max_treedepth = 15), seed =7, file = &quot;output/Chapter7/b7.5&quot;) 6乗項を入れるときには、少し手間が必要。 \\(\\sigma\\)を0.001に固定する。 custom_normal &lt;- custom_family( &quot;custom_normal&quot;, dpars = &quot;mu&quot;, links = &quot;identity&quot;, type = &quot;real&quot; ) stan_funs &lt;- &quot;real custom_normal_lpdf(real y, real mu) { return normal_lpdf(y | mu, 0.001); } real custom_normal_rng(real mu) { return normal_rng(mu, 0.001); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) b7.6 &lt;- brm(data = d, family = custom_normal, formula = B ~ 1 + M + I(M^2)+I(M^3)+I(M^4)+I(M^5)+I(M^6), prior = c(prior(normal(0.5, 1),class = Intercept), prior(normal(0, 10), class = b)), iter=8000,warmup=5000, control = list(adapt_delta = .999, max_treedepth = 15), stanvars = stanvars, seed =7, file = &quot;output/Chapter7/b7.6&quot;) 7.1.1.3 plot 全ての結果を作図する。 複雑なモデルほどフィットするようになる(R2も大きくなる)ことが分かる。 #make_figure &lt;- function(brms_fit, ylim = range(d$B)){ # r2 &lt;- R2(brms_fit) #nd &lt;- tibble(M = seq(-2,2,length.out=200)) #fitted(brms_fit,newdata = nd, probs = c(0.055,0.945)) %&gt;% #data.frame() %&gt;% #bind_cols(nd) %&gt;% #ggplot()+ #geom_lineribbon(aes(x=M, y = Estimate, # ymin = Q5.5,ymax=Q94.5), # color = &quot;black&quot;, fill=&quot;black&quot;,alpha=3/8, # size=1)+ #geom_point(data=d, aes(x=M,y=B), # color=&quot;navy&quot;,size=2)+ #labs(subtitle = bquote(italic(R)^2==.(round(r2, digits = 2))), # x = &quot;body mass (std)&quot;, # y = &quot;brain volume (std)&quot;) + #coord_cartesian(xlim = c(-1.2, 1.5), # ylim = ylim)+ #theme_classic()+ #theme(aspect.ratio=1) #} #p1 &lt;- make_figure(b7.1) #p2 &lt;- make_figure(b7.2) #p3 &lt;- make_figure(b7.3) #p4 &lt;- make_figure(b7.4, ylim =c(.25,1.1)) #p5 &lt;- make_figure(b7.5, ylim =c(.1,1.4)) #p6 &lt;- make_figure(b7.6,ylim = c(-0.25,1.5))+ # geom_hline(yintercept = 0, color = &quot;grey&quot;, # linetype=2)+ #labs(subtitle = bquote(italic(R)^2== &quot;1&quot;)) #((p1|p2|p3))/((p4|p5|p6))+ # plot_annotation(title = &quot;Figure7.3. Polynomial linear models of increasing\\ndegree for the hominin data.&quot;) -&gt;p7 p7 &lt;- readRDS(&quot;output/Chapter7/p7.rds&quot;) p7 7.1.2 Too few parameters hurts too 複雑なモデルほどデータを1つ抜いた時に結果が大きく変わってしまう。一方で、シンプルなモデルは1つデータを抜いても大きく推定結果は変わらない。 brain_loo_lines &lt;- function(brms_fit, row, ...) { nd &lt;- tibble(M = seq(-2,2,length.out=200)) # refit the model new_fit &lt;- update(brms_fit, newdata = filter(d, row_number() != row), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, refresh = 0, ...) # pull the lines values fitted(new_fit, newdata = nd) %&gt;% data.frame() %&gt;% dplyr::select(Estimate) %&gt;% bind_cols(nd) } #b7.1_fits &lt;- # tibble(row = 1:7) %&gt;% #mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = b7.1, row = .))) %&gt;% #unnest(post) b7.1_fits &lt;- readRDS(&quot;output/Chapter7/b7.1_fits.rds&quot;) #b7.4_fits &lt;- # tibble(row = 1:7) %&gt;% #mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = b7.4, row = ., # control = list(adapt_delta = .999)))) %&gt;% #unnest(post) b7.4_fits &lt;-readRDS(&quot;output/Chapter7/b7.4_fits.rds&quot;) p1 &lt;- b7.1_fits %&gt;% ggplot(aes(x = M)) + geom_line(aes(y = Estimate, group = row), size = 1/2, alpha = 1/2) + geom_point(data = d, aes(y = B),) + labs(subtitle = &quot;b7.1&quot;, x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = range(d$M), ylim = range(d$B))+ theme_classic()+ theme(aspect.ratio=1) # right p2 &lt;- b7.4_fits %&gt;% ggplot(aes(x = M, y = Estimate)) + geom_line(aes(group = row), size = 1/2, alpha = 1/2) + geom_point(data = d, aes(y = B)) + labs(subtitle = &quot;b7.4&quot;, x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = range(d$M), ylim = c(-0.1, 1.4))+ theme_classic()+ theme(aspect.ratio=1) # combine p1 + p2 7.2 Entropy and accuracy 7.2.1 Information and uncertainty どのようにモデルを評価すればいいだろうか。 Information theoryでは、不確実性を評価するうえで以下の指標を用いる(= Information entropy)。 ただし、\\(n\\)個の異なる出来事が起きる可能性がある場合、\\(p_{i}\\)はイベント\\(i\\)が起きる確率である。 \\(H(p) = -Elog(p_{i}) = - \\sum_{i=1}^n log(p_{i})\\) 例えば、晴れと雨になる確率がそれぞれ0.3と0.7であれば、以下のようになる。 p &lt;- c(0.3, 0.7) -sum(p*log(p)) ## [1] 0.6108643 7.2.2 From entropy to acculacy 実際にモデルが真のモデル(target)からどの程度外れているかを評価する際には、Kullback-Leibler divergenceを用いる。KL divergenceは以下の式で与えられる。 なお、イベント\\(i\\)が起こる真の確率を\\(p_{i}\\)、あるモデルにおいてイベント\\(i\\)が起きるとされる確率を\\(q_{i}\\)とおく。 \\(D_{KL}(p,q) = \\sum_{i} p_{i}(log(p_{i})-log(q_{i})) = \\sum_{i} p_{i}log(\\frac{p_{i}}{q_{i}})\\) 例えば、p = {0.3, 0.7}のとき、qによってdivergenceがどのように変化するかを調べると以下のようになり、真の値から遠ざかるほど大きくなることが分かる。 D &lt;- tibble( p_1 = 0.3, p_2 = 0.7, q_1 = seq(0.01,0.99,by = 0.01) ) %&gt;% mutate(q_2 = 1 - q_1, d_kl = (p_1*log(p_1/q_1))+(p_2*log(p_2/q_2))) D %&gt;% ggplot(aes(x=q_1,y=d_kl))+ geom_line(color=&quot;navy&quot;,size=1)+ geom_vline(aes(xintercept = p_1),linetype=&quot;dashed&quot;)+ annotate(geom=&quot;text&quot;,x=0.4,y=1.5,label=&quot;q = p&quot;)+ theme(aspect.ratio=1)+ labs(x = &quot;q[1]&quot;, y = &quot;Divergence of q from p&quot;) 7.2.3 Estimating divergence 真のモデルが未知な状態(\\(p_{i}\\)が未知)で\\(divergence\\)は推定できないが、異なるモデル同士の\\(divergence\\)の差を求めることはできる(\\(p_{i}\\)が打ち消されるので)。その際、そのモデルの平均対数尤度さえわかればよい。 そこで、以下の値を求める(\\(\\frac{1}{n}\\)倍していないので、平均対数尤度のn倍値である)。 \\(S(q) = \\sum_{i} log(q_{i})\\) ベイズ統計では、事後分布を用いてlog-pointwise-predictive-densityを用いることで対数尤度を求める。 なお、以下で\\(y\\)はデータを、\\(\\theta\\)はパラメータを、Sはiterationの数を表す。 \\(lppd(y, \\theta) = \\sum_{i} log\\frac{1}{S} \\sum_{s}p(y_{i}|\\theta_{s})\\) lppd &lt;- function(brms_fit) { log_lik(brms_fit) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% mutate(prob = exp(value)) %&gt;% group_by(name) %&gt;% summarise(log_prob_score = mean(prob) %&gt;% log()) %&gt;% summarise(lppd = sum(log_prob_score)) } lppd(b7.1) ## # A tibble: 1 × 1 ## lppd ## &lt;dbl&gt; ## 1 1.36 7.2.4 Scoring the right data 対数尤度も\\(R^2\\)と同様にモデルが複雑なほど大きくなってしまう。-&gt; brmsの結果だと少し変…? #tibble(name = str_c(&quot;b7.&quot;, 1:6)) %&gt;% # mutate(brms_fit = purrr::map(name,get)) %&gt;% #mutate(lppd = purrr::map(brms_fit, lppd)) %&gt;% #unnest(lppd) -&gt; t_lppd readRDS(&quot;output/Chapter7/t_lppd.rds&quot;) ## # A tibble: 6 × 3 ## name brms_fit lppd ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 b7.1 &lt;brmsfit&gt; 1.36 ## 2 b7.2 &lt;brmsfit&gt; 0.722 ## 3 b7.3 &lt;brmsfit&gt; 0.601 ## 4 b7.4 &lt;brmsfit&gt; 0.0125 ## 5 b7.5 &lt;brmsfit&gt; 1.22 ## 6 b7.6 &lt;brmsfit&gt; 25.9 ここで、以下のシミュレーションを考える。 1. ある真のモデル(パラメータは3個)を仮定し、そのモデルからデータを10000個生成する。 2. 生成したデータに対してパラメータをそれぞれ1~5個としたモデルをあてはめ、Devianceを計算する。これをN回繰り返す(training)。 3. 真のモデルから新たに10000回データを生成し、2で用いたモデルをあてはめ、Devianceを計算する(test)。これをN回繰り返す。 すると、training dataに関しては、パラメータの数が大きいほどDevianceも小さくなる(当てはまりがよくなる)が、test dataに関してはパラメータが3のときに最もDevianceが小さくなる。 7.3 Golem taming: regularization overfittingを防ぐ方法の一つは、データの過学習を防ぐような事前分布を用いることである。そのような事前分布の一つがregularizing priorである。 幅の狭い事前分布を用いれば、とくにデータが少ないときにはoverfittingをある程度は防ぐことができる。ただし、あまりに狭いとunderfittingになるので注意が必要である。 7.4 Predicting predictive accuracy 実際にはtest sampleは存在しない。それでは、どのようにモデルを評価すればいいだろうか。 主要な方法としては、cross validationとinformation criteriaがある。 7.4.1 Cross validation 全てのサンプルについてLOOCVを行うのは計算が大変なため、各サンプルの重要度(事後分布に影響を与える程度)に応じて重みづけをするPareto-smoothed importance sampling(PSIS)がよく用いられる。 #lppd &lt;- tibble(name = str_c(&quot;b7.&quot;, 1:2)) %&gt;% # mutate(brms_fit = purrr::map(name,get)) %&gt;% #mutate(lppd = purrr::map(brms_fit, loo, reloo= &quot;T&quot;)) lppd &lt;- readRDS(&quot;output/Chapter7/lppd.rds&quot;) lppd$lppd[[1]] ## ## Computed from 4000 by 7 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2.0 2.7 ## p_loo 3.4 1.7 ## looic 4.0 5.4 ## ------ ## Monte Carlo SE of elpd_loo is 0.2. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 6 85.7% 43 ## (0.5, 0.7] (ok) 1 14.3% 383 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. lppd$lppd[[2]] ## ## Computed from 4000 by 7 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2.9 1.9 ## p_loo 3.6 1.3 ## looic 5.8 3.7 ## ------ ## Monte Carlo SE of elpd_loo is 0.2. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 5 71.4% 36 ## (0.5, 0.7] (ok) 2 28.6% 132 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. 7.4.2 Information criteria もう一つが情報量基準である。 事前分布がflatなとき、overfittingによってDevianceがパラメータの2倍分だけ向上することが理論的に分かっている(see figure7.8)。AICはこれを利用して以下のようにモデルを評価する(ただし、pはパラメータの数)。 \\(AIC = D_{train} + 2p = -2lppd + 2p\\) ただし、AICは以下の仮定を満たしているときにしか使えない。 1. 事前分布がflatであるか、事後分布にほとんど影響を与えないとき。 2. 事後分布が多変量正規分布に従うとき。 3. サンプル数Nがパラメータ数pよりも十分大きいとき。 ＊DICは1の仮定が不要であるが、2と3の仮定は必要。 WAIC 1と2の仮定を必要とせず、Nが大きいときにはLOOCVの結果と一致する。 \\(WAIC(y,\\theta) = -2(lppd - \\sum_{i} var_{\\theta} log p(y_{i}|\\theta))\\) tibble(name = str_c(&quot;b7.&quot;, 1:5)) %&gt;% mutate(brms_fit = purrr::map(name,get)) %&gt;% mutate(WAIC = purrr::map(brms_fit, WAIC)) -&gt;r r$WAIC ## [[1]] ## ## Computed from 4000 by 7 log-likelihood matrix ## ## Estimate SE ## elpd_waic -0.9 1.7 ## p_waic 2.2 0.7 ## waic 1.8 3.3 ## ## 1 (14.3%) p_waic estimates greater than 0.4. We recommend trying loo instead. ## ## [[2]] ## ## Computed from 4000 by 7 log-likelihood matrix ## ## Estimate SE ## elpd_waic -2.0 1.1 ## p_waic 2.7 0.6 ## waic 3.9 2.3 ## ## 2 (28.6%) p_waic estimates greater than 0.4. We recommend trying loo instead. ## ## [[3]] ## ## Computed from 4000 by 7 log-likelihood matrix ## ## Estimate SE ## elpd_waic -3.0 0.9 ## p_waic 3.6 0.4 ## waic 6.0 1.8 ## ## 5 (71.4%) p_waic estimates greater than 0.4. We recommend trying loo instead. ## ## [[4]] ## ## Computed from 8000 by 7 log-likelihood matrix ## ## Estimate SE ## elpd_waic -4.4 0.5 ## p_waic 4.4 0.4 ## waic 8.9 1.1 ## ## 7 (100.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. ## ## [[5]] ## ## Computed from 8000 by 7 log-likelihood matrix ## ## Estimate SE ## elpd_waic -5.1 0.5 ## p_waic 6.3 0.4 ## waic 10.2 1.1 ## ## 7 (100.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. 7.4.3 overthinking: WAIC calculation 以下の例を考える。 data(cars) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 b7.m &lt;- brm( data = cars, family = gaussian, formula = dist ~ 1+ speed, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed=7, file = &quot;output/Chapter7/b7.m&quot; ) posterior_summary(b7.m) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters !=&quot;lp__&quot;) %&gt;% as_tibble() ## # A tibble: 4 × 5 ## parameters Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept -17.5 6.06 -29.3 -5.51 ## 2 b_speed 3.92 0.377 3.16 4.65 ## 3 sigma 13.9 1.23 11.7 16.5 ## 4 lprior -22.8 1.23 -25.4 -20.6 WAICを計算する。 関数を使った値とほとんど一致する。 lppd &lt;- lppd(b7.m)[[1]] pwaic &lt;- log_lik(b7.m) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(var_log = var(value)) %&gt;% summarise(pwaic = sum(var_log)) %&gt;% pull() hand_waic &lt;- -2*(lppd-pwaic) hand_waic ## [1] -145.3186 WAIC(b7.m) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_waic -210.8 8.2 ## p_waic 4.2 1.6 ## waic 421.5 16.4 ## ## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. loo(b7.m) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_loo -210.9 8.3 ## p_loo 4.3 1.7 ## looic 421.8 16.5 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 49 98.0% 855 ## (0.5, 0.7] (ok) 1 2.0% 252 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. 7.4.4 Comparing CV, PSIS, and WAIC それぞれを比較して使うことが重要である。 正しく推定できていれば、ほとんど同じになるはずである。 7.5 Model comparison 7.5.1 Model mis-selection WAICやPSISはあくまでもモデルの予測に関する指標であって、因果関係についてはなにも教えてくれない。 例えば、Chapter6での植物の成長に関するモデルを再び考える。処置後の真菌の情報を入れたモデルは、処置が植物の成長に与える影響を推測する際には不適切だが、WAICはもっとも小さい。 b6.3_h &lt;- readRDS(&quot;output/Chapter6/b6.3_h.rds&quot;) b6.3_htf &lt;- readRDS(&quot;output/Chapter6/b6.3_htf.rds&quot;) b6.3_ht &lt;- readRDS(&quot;output/Chapter6/b6.3_ht.rds&quot;) # WAIC b6.3_h &lt;- add_criterion(b6.3_h, criterion = &quot;waic&quot;) b6.3_htf &lt;- add_criterion(b6.3_htf,criterion = &quot;waic&quot;) b6.3_ht &lt;- add_criterion(b6.3_ht, criterion = &quot;waic&quot;) w &lt;- loo_compare(b6.3_h, b6.3_htf, b6.3_ht, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic ## b6.3_htf 0.0 0.0 -180.6 6.7 3.4 0.5 361.3 ## b6.3_ht -20.6 4.9 -201.3 5.4 2.5 0.3 402.5 ## b6.3_h -22.4 5.8 -203.0 5.7 1.6 0.2 406.0 ## se_waic ## b6.3_htf 13.4 ## b6.3_ht 10.8 ## b6.3_h 11.3 #PSIS b6.3_h_loo &lt;- add_criterion(b6.3_h, criterion = &quot;loo&quot;) b6.3_htf_loo &lt;- add_criterion(b6.3_htf,criterion = &quot;loo&quot;) b6.3_ht_loo &lt;- add_criterion(b6.3_ht, criterion = &quot;loo&quot;) w_loo &lt;- loo_compare(b6.3_h_loo, b6.3_htf_loo, b6.3_ht_loo, criterion = &quot;loo&quot;) print(w_loo, simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic ## b6.3_htf_loo 0.0 0.0 -180.7 6.7 3.4 0.5 361.3 ## b6.3_ht_loo -20.6 4.9 -201.3 5.4 2.5 0.3 402.5 ## b6.3_h_loo -22.3 5.8 -203.0 5.7 1.6 0.2 406.0 ## se_looic ## b6.3_htf_loo 13.5 ## b6.3_ht_loo 10.8 ## b6.3_h_loo 11.3 WAICの差が正規分布すると仮定すると、b6.3_htfとb6.3_htのWAICの差の99%信頼区間は以下の通り。 (w[2, 1] * -2) + c(-1, 1) * (w[2, 2] * 2) * 2.6 ## [1] 15.56371 66.88758 WAICを標準誤差と共にプロットすると…。 w[,7:8] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;model_name&quot;) %&gt;% mutate(model_name = fct_reorder(model_name,waic,.desc=T)) %&gt;% ggplot(aes(x = waic, y = model_name, xmin = waic - se_waic, xmax = waic + se_waic))+ geom_pointrange(shape=21, fill = &quot;black&quot;,color=&quot;black&quot;)+ labs(title = &quot;WAIC with standard error&quot;, y = NULL)+ theme(axis.ticks.y = element_blank()) b6.3_hとb6.3_htのWAICの差の標準誤差は差そのものよりも大きい。よってこれらのモデルのWAICの間に差はほとんどないといえる。 loo_compare(b6.3_h, b6.3_ht, criterion = &quot;waic&quot;)[2,2]*2 ## [1] 4.652457 各モデルのWAICに基づくweightは以下のように算出する。 なお、\\(\\varDelta_{i}\\)はWAICが最小のモデルとのWAICの差である。 \\(w_{i} = \\frac {exp(-0.5\\varDelta_{i})}{\\sum_{j} exp(-0.5\\varDelta_{j})}\\) model_weights(b6.3_htf,b6.3_h, b6.3_ht, weights = &quot;waic&quot;) %&gt;% round(2) ## b6.3_htf b6.3_h b6.3_ht ## 1 0 0 7.5.2 Outlier and other illusions Waffle Divorceの例を再び考える。 data(&quot;WaffleDivorce&quot;) d &lt;- WaffleDivorce %&gt;% mutate(D = standardize(Divorce), M = standardize(Marriage), A = standardize(MedianAgeMarriage)) rm(WaffleDivorce) Aのみが説明変数のモデルb5.1が最もPSISが小さいが、b5.3との差はわずかである。つまり、Mはほとんど予測を向上させない。 b5.1 &lt;- readRDS(&quot;output/Chapter5/b5.1.rds&quot;) b5.2 &lt;- readRDS(&quot;output/Chapter5/b5.2.rds&quot;) b5.3 &lt;- readRDS(&quot;output/Chapter5/b5.3.rds&quot;) b5.1 &lt;- add_criterion(b5.1, criterion = &quot;loo&quot;) b5.2 &lt;- add_criterion(b5.2, criterion = &quot;loo&quot;) b5.3 &lt;- add_criterion(b5.3, criterion = &quot;loo&quot;) loo_compare(b5.1,b5.2,b5.3, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b5.1 0.0 0.0 -62.9 6.4 3.7 1.8 125.9 12.8 ## b5.3 -0.9 0.4 -63.8 6.4 4.8 1.9 127.7 12.9 ## b5.2 -6.7 4.6 -69.6 4.9 2.9 0.9 139.2 9.8 教科書と違い、\\(k\\)が大きすぎるデータは出なかった。 0.5以上となったのは1点のみ。 library(loo) loo(b5.3) %&gt;% pareto_k_ids(threshold = .5) ## [1] 13 d %&gt;% slice(13) %&gt;% dplyr::select(Location,Loc) ## Location Loc ## 1 Idaho ID pareto_k_values(loo(b5.3))[13] ## [1] 0.5123012 WAICのpwaicと比較してみる。 k &lt;- pareto_k_values(loo(b5.3)) b5.3 &lt;- add_criterion(b5.3, &quot;waic&quot;) tibble(k = k, p_waic = b5.3$criteria$waic$pointwise[,2], Loc = pull(d,Loc)) %&gt;% ggplot(aes(x = k, y = p_waic, color = Loc == &quot;ID&quot;))+ geom_vline(xintercept = 0.5, linetype = &quot;dashed&quot;, alpha =1/2)+ geom_point(aes(shape = Loc == &quot;ID&quot;),size=2, stroke=1.5)+ geom_text(data = . %&gt;% filter(p_waic &gt;.5), aes(x = k-0.05,label=Loc))+ scale_shape_manual(values=c(1,19))+ scale_color_manual(values=c(&quot;navy&quot;,&quot;red&quot;))+ theme(legend.position=&quot;none&quot;, aspect.ratio=1)+ labs(title = &quot;b5.3&quot;) -&gt;p1 p1 外れ値があった場合、正規分布よりも裾の広い分布を用いることでより頑強な結果が得られる。 b5.3t &lt;- brm(data =d, family = student, formula = bf(D ~ 1 + A + M, nu=2), prior = c(prior(normal(0, 0.2),class= Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed =5, file = &quot;output/Chapter7/b5.3t&quot;) posterior_summary(b5.3) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;b_Intercept&quot;,&quot;b_A&quot;,&quot;b_M&quot;,&quot;sigma&quot;)) %&gt;% as_tibble() ## # A tibble: 4 × 5 ## parameters Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0 0.1 -0.19 0.2 ## 2 b_A -0.61 0.16 -0.93 -0.29 ## 3 b_M -0.06 0.16 -0.38 0.25 ## 4 sigma 0.83 0.09 0.68 1.01 posterior_summary(b5.3t) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;b_Intercept&quot;,&quot;b_A&quot;,&quot;b_M&quot;,&quot;sigma&quot;)) %&gt;% as_tibble() ## # A tibble: 4 × 5 ## parameters Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0.03 0.1 -0.18 0.23 ## 2 b_A -0.7 0.15 -1 -0.41 ## 3 b_M 0.05 0.2 -0.32 0.47 ## 4 sigma 0.58 0.09 0.43 0.78 b5.3t &lt;- add_criterion(b5.3t,c(&quot;waic&quot;,&quot;loo&quot;)) かなり改善したことが分かる。 k_t &lt;- pareto_k_values(loo(b5.3t)) tibble(k = k_t, p_waic = b5.3t$criteria$waic$pointwise[,2], Loc = pull(d,Loc)) %&gt;% ggplot(aes(x = k, y = p_waic, color = Loc == &quot;ID&quot;))+ geom_point(aes(shape = Loc == &quot;ID&quot;),size=2, stroke=1.5)+ geom_text(data = . %&gt;% filter(Loc %in%c(&quot;ME&quot;,&quot;ID&quot;)), aes(x = k-0.02,y = p_waic+0.02,label=Loc))+ scale_shape_manual(values=c(1,19))+ scale_color_manual(values=c(&quot;navy&quot;,&quot;red&quot;))+ theme(legend.position=&quot;none&quot;, aspect.ratio=1)+ labs(title = &quot;b5.3t&quot;) -&gt;p2 p1|p2 7.6 Practice 7.6.1 7E2 Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin? -(0.7*log(0.7)+0.3*log(0.3)) ## [1] 0.6108643 7.6.2 7E3 Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die? x &lt;- c(0.2,0.25,0.25,0.3) ent &lt;- function(x) { ent &lt;- tibble(p = x) %&gt;% mutate(plog = p*log(p)) %&gt;% summarise(entropy = -sum(plog)) print(ent) } ent(x) ## # A tibble: 1 × 1 ## entropy ## &lt;dbl&gt; ## 1 1.38 7.6.3 7E4 Suppose another four-sided die is loaded such that it never shows “4.” The other three sides show equally often. What is the entropy of this die? #4 x &lt;- c(1/3,1/3,1/3) ent(x) ## # A tibble: 1 × 1 ## entropy ## &lt;dbl&gt; ## 1 1.10 7.6.4 7M4 What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure. 事前分布が狭いほど事後分布の対数尤度のばらつきも小さくなるので、WAICもPSISも小さくなる。 set.seed(2000) library(glue) #prior_sim &lt;- tibble(prior_sd=rep(c(0.1,1,10),each=50)) %&gt;% #mutate(sample = purrr::map(1:n(), #function(x){ #n &lt;- 20 #tibble(x1 = rnorm(n = n), #x2 = rnorm(n = n), #x3 = rnorm(n = n)) %&gt;% mutate(y = rnorm(n = n, mean =0.3 + #0.8 * x1 + 0.6 * x2 + 1.2 * x3), #across(everything(), standardize)) #}), #model = purrr::map2(sample,prior_sd, #function(df, p_sd){ #mod &lt;- brm(y ~ 1+x1+x2+x3, #data=df, #family=gaussian, #prior = c(prior(normal(0, 0.2), class = Intercept), #prior_string(glue(&quot;normal(0, {p_sd})&quot;), class = &quot;b&quot;), #prior(exponential(1), class = sigma)), #iter=4000,warmup=3000,seed=1234) #return(mod) #})) #prior_sim %&gt;% #mutate(infc = purrr::map(model, # function(mod){ # w &lt;- suppressWarnings(brms::waic(mod)) # p &lt;- suppressWarnings(brms::loo(mod)) #tibble(p_waic = w$estimates[&quot;p_waic&quot;, &quot;Estimate&quot;], # p_loo = p$estimates[&quot;p_loo&quot;, &quot;Estimate&quot;]) # })) %&gt;% #unnest(infc) -&gt; prior_sim prior_sim &lt;- readRDS(&quot;output/Chapter7/prior_sim.rds&quot;) prior_sim %&gt;% pivot_longer(cols = c(p_waic, p_loo)) %&gt;% mutate(prior_sd = glue(&quot;sd~&#39;=&#39;~{prior_sd}&quot;), prior_sd = fct_inorder(prior_sd), name = factor(name, levels = c(&quot;p_waic&quot;, &quot;p_loo&quot;), labels = c(&quot;p[WAIC]&quot;, &quot;p[PSIS]&quot;))) %&gt;% ggplot(aes(x = value)) + facet_grid(rows = vars(prior_sd), cols = vars(name), labeller = label_parsed) + geom_histogram(aes(y = after_stat(density)), binwidth = 0.2) + labs(x = &quot;Value&quot;, y = &quot;Density&quot;) + theme(panel.border = element_rect(fill = NA)) 7.6.5 7H1 In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corportate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in (reconstructed at right), seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in data(Laffer). Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue. data(Laffer) d &lt;- Laffer %&gt;% mutate(tax_rate2 = tax_rate^2, across(everything(),standardize)) Laffer %&gt;% ggplot(aes(x=tax_rate,y=tax_revenue))+ geom_point(size=3,shape=21,color=&quot;navy&quot;)+ theme_classic()+ theme(aspect.ratio=1) laf_line &lt;- brm(data = d, family = gaussian, formula = tax_revenue ~ tax_rate, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000,seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter7/laf_line&quot;) laf_curve &lt;- brm(data = d, family = gaussian, formula = tax_revenue ~ 1+tax_rate + tax_rate2, prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000,seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter7/laf_curve&quot;) laf_spline &lt;- brm(data = d, family = gaussian, formula = tax_revenue ~ 1 + s(tax_rate), prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 10000, warmup =6000,seed = 1234, thin = 2, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter7/laf_spline&quot;) conditional_effects(laf_line, method = &quot;fitted&quot;) %&gt;% plot(points = TRUE,theme =theme(aspect.ratio=1)) -&gt; p_line conditional_effects(laf_curve, method = &quot;fitted&quot;,effects = &quot;tax_rate&quot;) %&gt;% plot(points = TRUE,theme =theme(aspect.ratio=1))-&gt; p_curve conditional_effects(laf_spline, method = &quot;fitted&quot;) %&gt;% plot(points = TRUE,theme =theme(aspect.ratio=1))-&gt; p_spline p_line$tax_rate + p_curve$tax_rate + p_spline$tax_rate WAICとPSISを計算してみると…。 差はほとんどなく、どのモデルも大きな違いはない。 laf_line &lt;- add_criterion(laf_line, criterion = c(&quot;waic&quot;,&quot;loo&quot;)) laf_curve &lt;- add_criterion(laf_curve, criterion = c(&quot;waic&quot;,&quot;loo&quot;)) laf_spline &lt;- add_criterion(laf_spline, criterion = c(&quot;waic&quot;,&quot;loo&quot;)) #waic loo_compare(laf_line, laf_curve, laf_spline, criterion = &quot;waic&quot;) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic ## laf_spline 0.0 0.0 -42.2 9.1 5.3 3.6 84.4 ## laf_line -1.0 0.8 -43.2 9.3 4.6 3.4 86.3 ## laf_curve -1.1 0.8 -43.3 9.4 4.9 3.6 86.6 ## se_waic ## laf_spline 18.2 ## laf_line 18.5 ## laf_curve 18.9 #loo loo_compare(laf_line, laf_curve, laf_spline, criterion = &quot;loo&quot;) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## laf_spline 0.0 0.0 -42.6 9.4 5.7 3.9 85.2 18.9 ## laf_line -1.0 0.8 -43.6 9.6 5.0 3.8 87.2 19.3 ## laf_curve -2.0 1.4 -44.6 10.7 6.2 4.8 89.1 21.3 7.6.6 7H2 In the Laffer data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Student’s t distribution to revist the curve fitting problem. How much does a curved relationship depend upon the outlier point. 一方で、pareto_kが0.7以上の点があり、外れ値があったことがうかがえる。 library(gghighlight) criteria_influence &lt;- function(mod) { tibble(pareto_k = mod$criteria$loo$diagnostics$pareto_k, p_waic = mod$criteria$waic$pointwise[, &quot;p_waic&quot;]) %&gt;% rowid_to_column(var = &quot;obs&quot;) } influ &lt;- bind_rows( criteria_influence(laf_line) %&gt;% mutate(type = &quot;Linear&quot;), criteria_influence(laf_curve) %&gt;% mutate(type = &quot;Curve&quot;), criteria_influence(laf_spline) %&gt;% mutate(type = &quot;Spline&quot;) ) ggplot(influ, aes(x = pareto_k, y = p_waic)) + facet_wrap(~type, ncol = 2) + geom_vline(xintercept = 0.7, linetype = &quot;dashed&quot;) + geom_hline(yintercept = 0.4, linetype = &quot;dashed&quot;) + geom_point() + gghighlight(pareto_k &gt; 0.7 | p_waic &gt; 0.4, n = 1, label_key = obs, label_params = list(size = 3)) + labs(x = expression(Pareto~italic(k)), y = expression(p[WAIC])) + theme(panel.border = element_rect(fill = NA)) Student_tでより頑強なモデルをフィットさせる。 laf_line_st &lt;- brm(data = d, family = student, formula = bf(tax_revenue ~ tax_rate,nu=2), prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000,seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter7/laf_line_st&quot;) laf_curve_st &lt;- brm(data = d, family = student, formula = bf(tax_revenue ~ 1+tax_rate + tax_rate2,nu=2), prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000,seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter7/laf_curve_st&quot;) laf_spline_st &lt;- brm(data = d, family = student, formula = bf(tax_revenue ~ 1 + s(tax_rate),nu=2), prior = c(prior(normal(0,0.2),class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 50000, warmup =40000,seed = 1234, thin = 5, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter7/laf_spline_st&quot;) conditional_effects(laf_line_st, method = &quot;fitted&quot;) %&gt;% plot(points = TRUE, theme=theme(aspect.ratio=1)) -&gt; pline_st conditional_effects(laf_curve_st, method = &quot;fitted&quot;) %&gt;% plot(points = TRUE) -&gt; pcurve_st conditional_effects(laf_spline_st, method = &quot;fitted&quot;) %&gt;% plot(points = TRUE) -&gt;pspline_st pline_st$tax_rate+pcurve_st$tax_rate+pspline_st$tax_rate WAICとPSISを計算しても、エラーは出ない。 また、WAICとPSISの値もほぼ一致した。 laf_line_st &lt;- add_criterion(laf_line_st, criterion = c(&quot;waic&quot;,&quot;loo&quot;)) laf_curve_st &lt;- add_criterion(laf_curve_st, criterion = c(&quot;waic&quot;,&quot;loo&quot;)) laf_spline_st &lt;- add_criterion(laf_spline_st, criterion = c(&quot;waic&quot;,&quot;loo&quot;)) #waic loo_compare(laf_line_st, laf_curve_st, laf_spline_st, criterion = &quot;waic&quot;) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic ## laf_spline_st 0.0 0.0 -35.9 6.3 5.0 1.0 71.7 ## laf_curve_st -0.6 0.8 -36.5 6.3 3.6 0.8 72.9 ## laf_line_st -0.9 1.1 -36.8 6.2 3.1 0.6 73.6 ## se_waic ## laf_spline_st 12.5 ## laf_curve_st 12.5 ## laf_line_st 12.5 #loo loo_compare(laf_line_st, laf_curve_st, laf_spline_st, criterion = &quot;loo&quot;) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic ## laf_spline_st 0.0 0.0 -35.8 6.3 4.9 1.0 71.7 ## laf_curve_st -0.6 0.8 -36.4 6.3 3.6 0.8 72.8 ## laf_line_st -0.9 1.1 -36.8 6.2 3.1 0.6 73.6 ## se_looic ## laf_spline_st 12.5 ## laf_curve_st 12.5 ## laf_line_st 12.5 pwaicは高い点があるものの、かなり改善している。 influ &lt;- bind_rows( criteria_influence(laf_line_st) %&gt;% mutate(type = &quot;Linear&quot;), criteria_influence(laf_curve_st) %&gt;% mutate(type = &quot;Curve&quot;), criteria_influence(laf_spline_st) %&gt;% mutate(type = &quot;Spline&quot;) ) ggplot(influ, aes(x = pareto_k, y = p_waic)) + facet_wrap(~type, ncol = 2) + geom_vline(xintercept = 0.7, linetype = &quot;dashed&quot;) + geom_hline(yintercept = 0.4, linetype = &quot;dashed&quot;) + geom_point() + gghighlight(pareto_k &gt; 0.7 | p_waic &gt; 0.4, n = 1, label_key = obs, label_params = list(size = 3)) + labs(x = expression(Pareto~italic(k)), y = expression(p[WAIC])) + theme(panel.border = element_rect(fill = NA)) 7.6.7 7H3 Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species: tibble(island = paste(&quot;Island&quot;, 1:3), speciesA = c(0.2, 0.8, 0.05), speciesB = c(0.2, 0.1, 0.15), speciesC = c(0.2, 0.05, 0.7), speciesD = c(0.2, 0.025, 0.05), speciesE = c(0.2, 0.025, 0.05)) %&gt;% datatable() Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution. Interpret these entropy values. Second, use each island’s bird distribution to predict the other two. This means to compute the KL divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different KL divergence values. Which island predicts the others best? Why? Island1ではもっともエントロピーが大きい。 よって、Island1で他のデータを予測するときに最もKLdivergenceが小さくなる。 islands &lt;- tibble(island = paste(&quot;Island&quot;, 1:3), a = c(0.2, 0.8, 0.05), b = c(0.2, 0.1, 0.15), c = c(0.2, 0.05, 0.7), d = c(0.2, 0.025, 0.05), e = c(0.2, 0.025, 0.05)) %&gt;% pivot_longer(-island, names_to = &quot;species&quot;, values_to = &quot;prop&quot;) islands %&gt;% group_by(island) %&gt;% summarise(prop=list(prop),.groups=&quot;drop&quot;) %&gt;% mutate(entropy = purrr::map(prop,ent)) %&gt;% unnest(entropy) -&gt; islands ## # A tibble: 1 × 1 ## entropy ## &lt;dbl&gt; ## 1 1.61 ## # A tibble: 1 × 1 ## entropy ## &lt;dbl&gt; ## 1 0.743 ## # A tibble: 1 × 1 ## entropy ## &lt;dbl&gt; ## 1 0.984 islands ## # A tibble: 3 × 3 ## island prop entropy ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 Island 1 &lt;dbl [5]&gt; 1.61 ## 2 Island 2 &lt;dbl [5]&gt; 0.743 ## 3 Island 3 &lt;dbl [5]&gt; 0.984 ##KL divergence d_KL &lt;- function(p,q){ sum(p*(log(p)-log(q))) } crossing(model = paste(&quot;Island&quot;,1:3), predicts = paste(&quot;Island&quot;,1:3)) %&gt;% filter(model!=predicts) %&gt;% left_join(islands, by=c(&quot;model&quot;=&quot;island&quot;)) %&gt;% dplyr::select(-entropy) %&gt;% rename(model_prop = prop) %&gt;% left_join(islands, by = c(&quot;predicts&quot;=&quot;island&quot;)) %&gt;% rename(predict_prop = prop) %&gt;% mutate(d_KL = map2(predict_prop, model_prop,d_KL)) %&gt;% unnest(d_KL) ## # A tibble: 6 × 6 ## model predicts model_prop predict_prop entropy d_KL ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Island 1 Island 2 &lt;dbl [5]&gt; &lt;dbl [5]&gt; 0.743 0.866 ## 2 Island 1 Island 3 &lt;dbl [5]&gt; &lt;dbl [5]&gt; 0.984 0.626 ## 3 Island 2 Island 1 &lt;dbl [5]&gt; &lt;dbl [5]&gt; 1.61 0.970 ## 4 Island 2 Island 3 &lt;dbl [5]&gt; &lt;dbl [5]&gt; 0.984 1.84 ## 5 Island 3 Island 1 &lt;dbl [5]&gt; &lt;dbl [5]&gt; 1.61 0.639 ## 6 Island 3 Island 2 &lt;dbl [5]&gt; &lt;dbl [5]&gt; 0.743 2.01 7.6.8 7H4 Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again (page 178). Compare these two models using WAIC (or PSIS, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree? Chapter6の結婚と年齢が幸福度に与える影響についてもモデルを再度考える。 b6.4の方がWAICもPSISも小さいが、このモデルでは因果関係は推論できない。 b6.4 &lt;- readRDS(&quot;output/Chapter6/b6.4.rds&quot;) b6.4_2 &lt;- readRDS(&quot;output/Chapter6/b6.4_2.rds&quot;) b6.4 &lt;- add_criterion(b6.4, criterio =c(&quot;waic&quot;,&quot;loo&quot;)) b6.4_2 &lt;- add_criterion(b6.4_2, criterio =c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b6.4,b6.4_2) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b6.4 0.0 0.0 -1356.9 18.6 3.7 0.2 2713.9 37.3 ## b6.4_2 -194.0 17.6 -1551.0 13.8 2.3 0.1 3101.9 27.6 7.6.9 7H5 Revisit the urban fox data, data(foxes), from the previous chapter’s practice problems. Use WAIC or PSIS based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables: avgfood + groupsize + area avgfood + groupsize groupsize + area avgfood area Can you explain the relative differences in WAIC scores, using the fox DAG from the previous chapter? Be sure to pay attention to the standard error of the score differences (dSE). b7H5_5 &lt;- readRDS(&quot;output/Chapter6/b6H3.rds&quot;) %&gt;% add_criterion(c(&quot;waic&quot;)) b7H5_4 &lt;- readRDS(&quot;output/Chapter6/b6H4.rds&quot;) %&gt;% add_criterion(c(&quot;waic&quot;)) b7H5_2 &lt;- readRDS(&quot;output/Chapter6/b6H5.rds&quot;) %&gt;% add_criterion(c(&quot;waic&quot;)) data(foxes) dat3 &lt;- foxes %&gt;% as_tibble() %&gt;% dplyr::select(area, avgfood, weight, groupsize) %&gt;% mutate(across(everything(),standardize)) ## 1 b7H5_1 &lt;- brm(data = dat3, family = gaussian, formula = weight ~ 1 + area + groupsize + avgfood, prior = c(prior(normal(0, 0.2),class=Intercept), prior(normal(0, 0.5), class = b,), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b7H5_1&quot;) b7H5_1 &lt;- add_criterion(b7H5_1, &quot;waic&quot;) ## 3 b7H5_3 &lt;- brm(data = dat3, family = gaussian, formula = weight ~ 1 + area + groupsize + avgfood, prior = c(prior(normal(0, 0.2),class=Intercept), prior(normal(0, 0.5), class = b,), prior(exponential(1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter6/b7H5_3&quot;) b7H5_3 &lt;- add_criterion(b7H5_3, &quot;waic&quot;) 1,2,3はほとんど変わらない。 areaがweightに与える影響はすべてavgfoodを経由しているため、１と2は全く同じとなる。また、3も同様の理由でほとんど変わらない。 4と5も同様である。 loo_compare(b7H5_1, b7H5_2, b7H5_3,b7H5_4,b7H5_5, criterion=&quot;waic&quot;) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b7H5_1 0.0 0.0 -161.4 7.7 4.4 0.7 322.8 15.5 ## b7H5_3 0.0 0.0 -161.4 7.7 4.4 0.7 322.8 15.5 ## b7H5_2 -0.4 1.7 -161.8 7.7 3.5 0.6 323.6 15.4 ## b7H5_4 -5.3 3.4 -166.7 6.7 2.3 0.3 333.3 13.4 ## b7H5_5 -5.3 3.4 -166.7 6.7 2.4 0.4 333.4 13.3 "],["conditional-manatees.html", "8 Conditional Manatees 8.1 Building an interaction 8.2 Symmetry of interaction 8.3 Continuous interaction 8.4 Practice", " 8 Conditional Manatees 8.1 Building an interaction アフリカとそれ以外の地域で地形の悪さ(terrain ruggedness)とGDPの関係を調べる。地形が悪いほど経済活動が阻害され、GDPも小さくなると予想されるが、アフリカではむしろ逆の関係がみられる。 ## データ読み込み data(rugged) d &lt;- rugged head(rugged) ## isocode isonum country rugged rugged_popw rugged_slope rugged_lsd ## 1 ABW 533 Aruba 0.462 0.380 1.226 0.144 ## 2 AFG 4 Afghanistan 2.518 1.469 7.414 0.720 ## 3 AGO 24 Angola 0.858 0.714 2.274 0.228 ## 4 AIA 660 Anguilla 0.013 0.010 0.026 0.006 ## 5 ALB 8 Albania 3.427 1.597 10.451 1.006 ## 6 AND 20 Andorra 5.717 6.722 17.774 1.616 ## rugged_pc land_area lat lon soil desert tropical dist_coast ## 1 0.000 18 12.508 -69.970 21.324 0.000 100.000 0.001 ## 2 39.004 65209 33.833 66.026 27.849 4.356 0.000 0.922 ## 3 4.906 124670 -12.299 17.551 26.676 0.425 44.346 0.428 ## 4 0.000 9 18.231 -63.064 100.000 0.000 100.000 0.000 ## 5 62.133 2740 41.143 20.070 68.088 0.000 0.000 0.048 ## 6 99.064 47 42.551 1.576 0.000 0.000 0.000 0.134 ## near_coast gemstones rgdppc_2000 rgdppc_1950_m rgdppc_1975_m rgdppc_2000_m ## 1 100.0000 0 NA NA NA NA ## 2 0.0000 0 NA 644.756 720.633 565.231 ## 3 13.1587 47756 1794.729 1051.822 1073.036 765.215 ## 4 100.0000 0 NA NA NA NA ## 5 94.6919 0 3703.113 1001.339 2289.472 2741.420 ## 6 0.0000 0 NA NA NA NA ## rgdppc_1950_2000_m q_rule_law cont_africa cont_asia cont_europe cont_oceania ## 1 NA NA 0 0 0 0 ## 2 679.791 -1.687 0 1 0 0 ## 3 1106.763 -1.567 1 0 0 0 ## 4 NA NA 0 0 0 0 ## 5 1931.784 -0.820 0 0 1 0 ## 6 NA 1.515 0 0 1 0 ## cont_north_america cont_south_america legor_gbr legor_fra legor_soc legor_deu ## 1 1 0 0 1 0 0 ## 2 0 0 0 1 0 0 ## 3 0 0 0 1 0 0 ## 4 1 0 NA NA NA NA ## 5 0 0 0 0 1 0 ## 6 0 0 0 1 0 0 ## legor_sca colony_esp colony_gbr colony_fra colony_prt colony_oeu ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## 3 0 0 0 0 1 0 ## 4 NA 0 0 0 0 0 ## 5 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 ## africa_region_n africa_region_s africa_region_w africa_region_e ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## africa_region_c slave_exports dist_slavemkt_atlantic dist_slavemkt_indian ## 1 0 0 NA NA ## 2 0 0 NA NA ## 3 1 3610000 5.669 6.981 ## 4 0 0 NA NA ## 5 0 0 NA NA ## 6 0 0 NA NA ## dist_slavemkt_saharan dist_slavemkt_redsea pop_1400 european_descent ## 1 NA NA 614 NA ## 2 NA NA 1870829 0 ## 3 4.926 3.872 1223208 2 ## 4 NA NA NA NA ## 5 NA NA 200000 100 ## 6 NA NA NA NA ## 標準化(GDPは対数をとる) library(lemon) d %&gt;% dplyr::select(country,cont_africa,rugged,rgdppc_2000) %&gt;% na.omit() %&gt;% mutate(G = log(rgdppc_2000)/mean(log(rgdppc_2000)), C = as.factor(cont_africa)) %&gt;% mutate(C = fct_relevel(C, &quot;1&quot;,&quot;0&quot;)) %&gt;% mutate(C = dplyr::recode(C,&quot;1&quot;=&quot;African nations&quot;,&quot;0&quot;=&quot;Non-African nations&quot;)) %&gt;% stats::na.omit() %&gt;% mutate(R = rugged/max(rugged)) -&gt; d2 ## プロットする d2 %&gt;% ggplot(aes(x = R, y = G))+ geom_point(aes(color = C, shape = C), size = 2.5)+ geom_smooth(method=&quot;lm&quot;,color=&quot;black&quot;)+ scale_color_manual(values = c(&quot;navy&quot;,&quot;black&quot;))+ scale_shape_manual(values = c(16,1))+ geom_text_repel(data = d2 %&gt;% filter(country %in% c(&quot;Seychelles&quot;,&quot;Lesotho&quot;,&quot;Switzerland&quot;,&quot;Tajikistan&quot;)), aes(label = country))+ facet_rep_wrap(~ C, scales = &quot;free_y&quot;)+ labs(x = &quot;ruggedness(std)&quot;, y = &quot;log GDP (as proportion of mean)&quot;)+ theme(legend.position = &quot;none&quot;, strip.background = element_blank(), aspect.ratio=1, strip.text = element_text(size=12)) これはなぜだろうか？おそらく、RもGもともに未知の変数(ex. 他の地理的特徴、奴隷制による被害の大きさ)によって影響を受けているからだと思われる。 これについては最後のPracticeで考える。 8.1.1 Making a rugged model まずは交互作用のないシンプルなモデルを考える。 \\(\\alpha\\)は\\(r_{i}-\\bar{r}\\)が0、すなわち\\(r_{i}\\)が平均のときのGの値(平均1)なので、およそ1くらいになるはず。よって、\\(\\alpha\\)の事前分布は平均1、標準偏差1の正規分布とする。 \\[ \\begin{aligned} log(y_{i}) &amp;\\sim Normal(\\mu_{i},\\sigma)\\\\ \\mu_{i} &amp;= \\alpha + \\beta(r_{i} - \\bar{r})\\\\ \\alpha &amp;\\sim Normal(1,1)\\\\ \\beta &amp;\\sim Normal(0,1) \\end{aligned} \\] d2 %&gt;% mutate(R_c = R - mean(R)) -&gt; d2 b8.1_pre &lt;- brm( data = d2, family = gaussian, formula = G ~ 1 + R_c, prior = c(prior(normal(1, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), seed=8, sample_prior=TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.1_pre&quot; ) まずは事前分布を確認してみる。 (0.22, 1)の近くを通るはずなのに、ほとんどの線が通っていない。また、傾きは極端でも-0.6~0.6くらいの範囲(青い線)の間に入るはずなのに、それよりも極端な傾きがかなりある。 prior_pre &lt;- prior_samples(b8.1_pre) p1 &lt;- prior_pre %&gt;% slice_sample(n=50) %&gt;% rownames_to_column(var = &quot;iter&quot;) %&gt;% dplyr::select(-sigma) %&gt;% tidyr::expand(nesting(iter,Intercept,b), R_c = c(-2,2)) %&gt;% mutate(R = R_c + mean(d2$R)) %&gt;% ggplot(aes(x=R, y = Intercept + b*R_c, group=iter))+ geom_line(alpha = 1/2)+ geom_hline(yintercept = range(d2$G), linetype=2)+ geom_abline(intercept = c(1.3,0.7), slope=c(-0.6,0.6), color = &quot;blue&quot;, size=1)+ coord_cartesian(ylim = c(0.5,1.5), xlim = c(0,1))+ labs(y = &quot;log GDP (prop of mean)&quot;, x = &quot;ruggedness&quot;)+ labs(subtitle = &quot;Intercept ~ dnorm(1, 1)\\nb ~ dnorm(0, 1)&quot;) + theme(aspect.ratio=1) p1 よって、\\(\\alpha\\)と\\(\\beta\\)の事前分布の標準偏差をそれぞれ0.1と0.3にする。 b8.1 &lt;- brm( data = d2, family = gaussian, formula = G ~ 1 + R_c, prior = c(prior(normal(1, 0.1), class = Intercept), prior(normal(0, 0.3), class = b), prior(exponential(1), class = sigma)), seed=8, sample_prior=TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.1&quot; ) 今度は事前分布はもっともらしい範囲に収まっている。 prior &lt;- prior_samples(b8.1) p2 &lt;- prior %&gt;% slice_sample(n=50) %&gt;% rownames_to_column(var = &quot;iter&quot;) %&gt;% dplyr::select(-sigma) %&gt;% tidyr::expand(nesting(iter,Intercept,b), R_c = c(-2,2)) %&gt;% mutate(R = R_c + mean(d2$R)) %&gt;% ggplot(aes(x=R, y = Intercept + b*R_c, group=iter))+ geom_line(alpha = 1/2)+ geom_hline(yintercept = range(d2$G), linetype=2)+ coord_cartesian(ylim = c(0.5,1.5), xlim = c(0,1))+ labs(y = &quot;log GDP (prop of mean)&quot;, x = &quot;ruggedness&quot;)+ labs(subtitle = &quot;Intercept ~ dnorm(1, 0.1)\\nb ~ dnorm(0, 0.3)&quot;) theme(aspect.ratio=1) ## List of 1 ## $ aspect.ratio: num 1 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi FALSE ## - attr(*, &quot;validate&quot;)= logi TRUE p1+p2+ plot_annotation(title = &quot;Simulating in search of reasonable priors for the terrain ruggedness example.&quot;, theme = theme(plot.title = element_text(size = 12))) 結果を見ると、RとGの間には全く関係がないことになる。 posterior_summary(b8.1) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;b_&quot;)) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #rwutdqwjoq .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rwutdqwjoq .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rwutdqwjoq .gt_caption { padding-top: 4px; padding-bottom: 4px; } #rwutdqwjoq .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rwutdqwjoq .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #rwutdqwjoq .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rwutdqwjoq .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rwutdqwjoq .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rwutdqwjoq .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rwutdqwjoq .gt_column_spanner_outer:first-child { padding-left: 0; } #rwutdqwjoq .gt_column_spanner_outer:last-child { padding-right: 0; } #rwutdqwjoq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #rwutdqwjoq .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #rwutdqwjoq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rwutdqwjoq .gt_from_md > :first-child { margin-top: 0; } #rwutdqwjoq .gt_from_md > :last-child { margin-bottom: 0; } #rwutdqwjoq .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rwutdqwjoq .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #rwutdqwjoq .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #rwutdqwjoq .gt_row_group_first td { border-top-width: 2px; } #rwutdqwjoq .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rwutdqwjoq .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #rwutdqwjoq .gt_first_summary_row.thick { border-top-width: 2px; } #rwutdqwjoq .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rwutdqwjoq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rwutdqwjoq .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rwutdqwjoq .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rwutdqwjoq .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rwutdqwjoq .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rwutdqwjoq .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #rwutdqwjoq .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rwutdqwjoq .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #rwutdqwjoq .gt_left { text-align: left; } #rwutdqwjoq .gt_center { text-align: center; } #rwutdqwjoq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rwutdqwjoq .gt_font_normal { font-weight: normal; } #rwutdqwjoq .gt_font_bold { font-weight: bold; } #rwutdqwjoq .gt_font_italic { font-style: italic; } #rwutdqwjoq .gt_super { font-size: 65%; } #rwutdqwjoq .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #rwutdqwjoq .gt_asterisk { font-size: 100%; vertical-align: 0; } #rwutdqwjoq .gt_indent_1 { text-indent: 5px; } #rwutdqwjoq .gt_indent_2 { text-indent: 10px; } #rwutdqwjoq .gt_indent_3 { text-indent: 15px; } #rwutdqwjoq .gt_indent_4 { text-indent: 20px; } #rwutdqwjoq .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_Intercept 1.00 0.01 0.98 1.02 b_R_c 0.00 0.06 -0.11 0.11 sigma 0.14 0.01 0.12 0.15 8.1.2 Adding an indicator variable isn’t enough 続いて、Aをindicator variableとしてモデルに加える。 \\[ \\mu_{i} = \\alpha_{CID[i]} + \\beta(r_{i} - \\bar{r}) \\] d2 %&gt;% mutate(A_ind = ifelse(cont_africa ==1,&quot;1&quot;,&quot;2&quot;)) -&gt;d2 b8.2 &lt;- brm( data = d2, family = gaussian, formula = G ~ 0 + A_ind + R_c, prior = c(prior(normal(1, 0.1), class = b, coef=&quot;A_ind1&quot;), prior(normal(1, 0.1), class = b, coef=&quot;A_ind2&quot;), prior(normal(0, 0.3), class = b, coef = &quot;R_c&quot;), prior(exponential(1), class = sigma)), seed=8, sample_prior=TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.2&quot; ) WAICを比較すると、2番目のモデルの方がかなり小さい。 b8.1 &lt;- add_criterion(b8.1,c(&quot;waic&quot;,&quot;loo&quot;)) b8.2 &lt;- add_criterion(b8.2,c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b8.1,b8.2,criterion=&quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b8.2 0.0 0.0 126.1 7.4 4.1 0.8 -252.3 14.8 ## b8.1 -31.8 7.3 94.4 6.5 2.6 0.3 -188.7 13.0 結果を見ると、アフリカとそれ以外の国で違いがあることが分かったが、傾きはほとんど0。 posterior_summary(b8.2) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;^b_&quot;)) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #vkyltspiqf .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #vkyltspiqf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vkyltspiqf .gt_caption { padding-top: 4px; padding-bottom: 4px; } #vkyltspiqf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #vkyltspiqf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #vkyltspiqf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vkyltspiqf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vkyltspiqf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #vkyltspiqf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #vkyltspiqf .gt_column_spanner_outer:first-child { padding-left: 0; } #vkyltspiqf .gt_column_spanner_outer:last-child { padding-right: 0; } #vkyltspiqf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #vkyltspiqf .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #vkyltspiqf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #vkyltspiqf .gt_from_md > :first-child { margin-top: 0; } #vkyltspiqf .gt_from_md > :last-child { margin-bottom: 0; } #vkyltspiqf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #vkyltspiqf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #vkyltspiqf .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #vkyltspiqf .gt_row_group_first td { border-top-width: 2px; } #vkyltspiqf .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vkyltspiqf .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #vkyltspiqf .gt_first_summary_row.thick { border-top-width: 2px; } #vkyltspiqf .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vkyltspiqf .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vkyltspiqf .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #vkyltspiqf .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #vkyltspiqf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vkyltspiqf .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vkyltspiqf .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #vkyltspiqf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vkyltspiqf .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #vkyltspiqf .gt_left { text-align: left; } #vkyltspiqf .gt_center { text-align: center; } #vkyltspiqf .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #vkyltspiqf .gt_font_normal { font-weight: normal; } #vkyltspiqf .gt_font_bold { font-weight: bold; } #vkyltspiqf .gt_font_italic { font-style: italic; } #vkyltspiqf .gt_super { font-size: 65%; } #vkyltspiqf .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #vkyltspiqf .gt_asterisk { font-size: 100%; vertical-align: 0; } #vkyltspiqf .gt_indent_1 { text-indent: 5px; } #vkyltspiqf .gt_indent_2 { text-indent: 10px; } #vkyltspiqf .gt_indent_3 { text-indent: 15px; } #vkyltspiqf .gt_indent_4 { text-indent: 20px; } #vkyltspiqf .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_A_ind1 0.88 0.02 0.85 0.91 b_A_ind2 1.05 0.01 1.03 1.07 b_R_c -0.05 0.05 -0.14 0.04 sigma 0.11 0.01 0.10 0.13 posterior_samples(b8.2, summary = F) %&gt;% mutate(diff_A = b_A_ind2 - b_A_ind1) %&gt;% mean_qi(diff_A) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #oibmxuuqgd .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #oibmxuuqgd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oibmxuuqgd .gt_caption { padding-top: 4px; padding-bottom: 4px; } #oibmxuuqgd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #oibmxuuqgd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #oibmxuuqgd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oibmxuuqgd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oibmxuuqgd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #oibmxuuqgd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #oibmxuuqgd .gt_column_spanner_outer:first-child { padding-left: 0; } #oibmxuuqgd .gt_column_spanner_outer:last-child { padding-right: 0; } #oibmxuuqgd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #oibmxuuqgd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #oibmxuuqgd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #oibmxuuqgd .gt_from_md > :first-child { margin-top: 0; } #oibmxuuqgd .gt_from_md > :last-child { margin-bottom: 0; } #oibmxuuqgd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #oibmxuuqgd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #oibmxuuqgd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #oibmxuuqgd .gt_row_group_first td { border-top-width: 2px; } #oibmxuuqgd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oibmxuuqgd .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #oibmxuuqgd .gt_first_summary_row.thick { border-top-width: 2px; } #oibmxuuqgd .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oibmxuuqgd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oibmxuuqgd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #oibmxuuqgd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #oibmxuuqgd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oibmxuuqgd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oibmxuuqgd .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #oibmxuuqgd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oibmxuuqgd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #oibmxuuqgd .gt_left { text-align: left; } #oibmxuuqgd .gt_center { text-align: center; } #oibmxuuqgd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #oibmxuuqgd .gt_font_normal { font-weight: normal; } #oibmxuuqgd .gt_font_bold { font-weight: bold; } #oibmxuuqgd .gt_font_italic { font-style: italic; } #oibmxuuqgd .gt_super { font-size: 65%; } #oibmxuuqgd .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #oibmxuuqgd .gt_asterisk { font-size: 100%; vertical-align: 0; } #oibmxuuqgd .gt_indent_1 { text-indent: 5px; } #oibmxuuqgd .gt_indent_2 { text-indent: 10px; } #oibmxuuqgd .gt_indent_3 { text-indent: 15px; } #oibmxuuqgd .gt_indent_4 { text-indent: 20px; } #oibmxuuqgd .gt_indent_5 { text-indent: 25px; } diff_A .lower .upper .width .point .interval 0.1685071 0.1300103 0.2057698 0.95 mean qi プロットすると以下のようになる。 conditional_effects(b8.2,method=&quot;fitted&quot;, effects = &quot;R_c:A_ind&quot;) %&gt;% plot(points = T,theme = theme(aspect.ratio=1)) 8.1.3 Adding interaction does work 最後に、交互作用を入れてみる。 \\(\\mu_{i} = \\alpha_{CID[i]} + \\beta_{CID[i]}(r_{i} - \\bar{r})\\) brmで記述する際には、nl = TRUEにして非線形モデルを作る。 b8.3 &lt;- brm( data = d2, family = gaussian, formula = bf(G ~ 0 + a + b * R_c, a ~ 0 + A_ind, b ~ 0 + A_ind, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef=&quot;A_ind1&quot;, nlpar=a), prior(normal(1, 0.1), class = b, coef=&quot;A_ind2&quot;,nlpar=a), prior(normal(1, 0.3), class = b, coef=&quot;A_ind1&quot;, nlpar=b), prior(normal(1, 0.3), class = b, coef=&quot;A_ind2&quot;,nlpar=b), prior(exponential(1), class = sigma)), seed=8, sample_prior=TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.3&quot; ) WAICを比較するとb8.3がb8.2よりもよい。 重みづけをすると、b8.3がかなりのウェイトを占める。 b8.3 &lt;- add_criterion(b8.3, c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b8.1,b8.2,b8.3, criterion=&quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b8.3 0.0 0.0 128.9 7.3 5.3 1.0 -257.7 14.6 ## b8.2 -2.7 3.7 126.1 7.4 4.2 0.8 -252.3 14.9 ## b8.1 -34.5 7.5 94.4 6.5 2.6 0.3 -188.7 13.0 model_weights(b8.1, b8.2, b8.3, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b8.1 b8.2 b8.3 ## 0.00 0.06 0.94 pareto_kもすべて0.7以下である。 loo(b8.3) %&gt;% plot() 傾きはアフリカとそれ以外で逆向きになっている。 posterior_summary(b8.3) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;^b_&quot;)) %&gt;% as_tibble() %&gt;% print() ## # A tibble: 5 × 5 ## parameters Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_a_A_ind1 0.89 0.02 0.86 0.92 ## 2 b_a_A_ind2 1.05 0.01 1.03 1.07 ## 3 b_b_A_ind1 0.2 0.08 0.05 0.35 ## 4 b_b_A_ind2 -0.11 0.06 -0.22 0.01 ## 5 sigma 0.11 0.01 0.1 0.12 conditional_effects(b8.3,method=&quot;fitted&quot;, effects = &quot;R_c:A_ind&quot;) %&gt;% plot(points = T,theme = theme(aspect.ratio=1)) 8.1.4 Plotting the interaction 手で書くと… sample &lt;- crossing(R_c = seq(-0.3,0.8, length.out=50), A_ind = c(1,2)) fitted(b8.3, newdata = sample, probs = c(0.015,0.985)) %&gt;% data.frame() %&gt;% bind_cols(sample) %&gt;% mutate(R = R_c+mean(d2$R), A_ind = as.factor(A_ind)) %&gt;% mutate(A_ind = fct_relevel(A_ind, &quot;1&quot;,&quot;2&quot;)) -&gt; fitted labeli &lt;- as_labeller(c(&quot;1&quot; = &quot;African nations&quot;, &quot;2&quot; = &quot;Non african nations&quot;)) countries &lt;- c(&quot;Equatorial Guinea&quot;, &quot;South Africa&quot;, &quot;Seychelles&quot;, &quot;Swaziland&quot;, &quot;Lesotho&quot;, &quot;Rwanda&quot;, &quot;Burundi&quot;, &quot;Luxembourg&quot;, &quot;Greece&quot;, &quot;Switzerland&quot;, &quot;Lebanon&quot;, &quot;Yemen&quot;, &quot;Tajikistan&quot;, &quot;Nepal&quot;) d2 %&gt;% mutate(A_ind = as.factor(A_ind)) %&gt;% mutate(A_ind = fct_relevel(A_ind, &quot;1&quot;,&quot;2&quot;)) %&gt;% ggplot(aes(x = R,color = A_ind))+ geom_point(aes(y = G, shape = A_ind), size = 2.5)+ geom_smooth(data=fitted, aes(y = Estimate, ymax = Q98.5, ymin = Q1.5, fill = A_ind), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2)+ scale_shape_manual(values = c(16,1))+ scale_color_manual(values = c(&quot;navy&quot;,&quot;black&quot;))+ scale_fill_manual(values = c(&quot;navy&quot;,&quot;black&quot;))+ theme(aspect.ratio=1, legend.position = &quot;none&quot;, strip.background = element_blank(), strip.text = element_text(size=15))+ geom_text_repel(data =d2 %&gt;% filter(country %in% countries), aes(y=G, label=country), size=2.3)+ facet_wrap(~A_ind, labeller = labeli)+ labs(x = &quot;ruggedness (std)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) 8.2 Symmetry of interaction 交互作用は2通りに解釈できる。 1. アフリカの国かそれ以外かによってRがGに与える影響が変わる。 2. Rの値によって、アフリカとそれ以外の国のGDPの大小関係が変わる。 先ほどは1に着目していたが、今度は2に着目してみよう。 ruggednessが高くなるほど、GDPの差が徐々に小さくなっていき、Rが0.75あたりで大小関係が逆転していることが分かる。 fitted(b8.3, newdata = sample, probs = c(0.015,0.985), summary = F) %&gt;% data.frame() %&gt;% pivot_longer(everything(), values_to = &quot;Estimate&quot;) %&gt;% bind_cols(tidyr::expand(sample, iter = 1:4000, nesting(R_c, A_ind))) %&gt;% dplyr::select(-name) %&gt;% pivot_wider(names_from = A_ind, values_from = Estimate) %&gt;% mutate(delta = `1` - `2`) %&gt;% group_by(R_c) %&gt;% summarise(mean_qi(delta)) %&gt;% data.frame() %&gt;% mutate(R = R_c + mean(d2$R)) %&gt;% ggplot(aes(x=R, y = y, ymin = ymin, ymax = ymax))+ geom_smooth(color=&quot;black&quot;, alpha = 1/3, size=1/2, stat = &quot;identity&quot;)+ labs(x=&quot;ruggedness&quot;, y = &quot;expected difference log GDP&quot;)+ geom_hline(yintercept = 0, linetype=2)+ annotate(geom = &quot;text&quot;, x=0.1,y=0.03, label=&quot;Africa &gt; Non Africa&quot;, size=3)+ annotate(geom = &quot;text&quot;, x=0.1,y=-0.03, label=&quot;Non Africa &gt; Africa&quot;,size=3)+ theme(aspect.ratio=1) 8.3 Continuous interaction 連続変数の交互作用は解釈が一層難しい。 8.3.1 A winter flower 異なる日照条件と与える水の量によってチューリップの花のサイズを説明するモデルを考える。 ## データ読み込み data(tulips) d3 &lt;- tulips head(d3) ## bed water shade blooms ## 1 a 1 1 0.00 ## 2 a 1 2 0.00 ## 3 a 1 3 111.04 ## 4 a 2 1 183.47 ## 5 a 2 2 59.16 ## 6 a 2 3 76.75 d3 %&gt;% ggplot(aes(x=water,y=blooms,color=as.factor(shade)))+ geom_point(size =2, shape=1)+ theme(aspect.ratio=1) d3 %&gt;% ggplot(aes(x=shade,y=blooms,color=as.factor(water)))+ geom_point(size =2, shape=1)+ theme(aspect.ratio=1) 8.3.2 The models まずは交互作用なしのモデルを考える。 切片は01の範囲に、傾きは-0.50.5の範囲に収まると考えられるので、事前分布の標準偏差は0.25とする。 \\[ \\begin{aligned} B_{i} &amp;\\sim Normal(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp;= \\alpha + \\beta_{W}(W_{i} - \\bar{W}) + \\beta_{S}(S_{i} - \\bar{S})\\\\ \\alpha &amp;\\sim Normal(0.5,0.25)\\\\ \\beta_{j} &amp;\\sim Normal(0, 0.25) \\end{aligned} \\] d3 %&gt;% mutate(B = blooms/max(blooms), W = water - mean(water), S = shade - mean(shade)) -&gt; d3 b8.4 &lt;- brm( data = d3, family = gaussian, formula = B ~ 1 + W + S, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b), prior(exponential(1), class = sigma)), seed=8, sample_prior=TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.4&quot; ) posterior_summary(b8.4) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;^b_&quot;)) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xkomdyqblz .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xkomdyqblz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xkomdyqblz .gt_caption { padding-top: 4px; padding-bottom: 4px; } #xkomdyqblz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xkomdyqblz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #xkomdyqblz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xkomdyqblz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xkomdyqblz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xkomdyqblz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xkomdyqblz .gt_column_spanner_outer:first-child { padding-left: 0; } #xkomdyqblz .gt_column_spanner_outer:last-child { padding-right: 0; } #xkomdyqblz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #xkomdyqblz .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #xkomdyqblz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xkomdyqblz .gt_from_md > :first-child { margin-top: 0; } #xkomdyqblz .gt_from_md > :last-child { margin-bottom: 0; } #xkomdyqblz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xkomdyqblz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #xkomdyqblz .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #xkomdyqblz .gt_row_group_first td { border-top-width: 2px; } #xkomdyqblz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xkomdyqblz .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #xkomdyqblz .gt_first_summary_row.thick { border-top-width: 2px; } #xkomdyqblz .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xkomdyqblz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xkomdyqblz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xkomdyqblz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xkomdyqblz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xkomdyqblz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xkomdyqblz .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #xkomdyqblz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xkomdyqblz .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #xkomdyqblz .gt_left { text-align: left; } #xkomdyqblz .gt_center { text-align: center; } #xkomdyqblz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xkomdyqblz .gt_font_normal { font-weight: normal; } #xkomdyqblz .gt_font_bold { font-weight: bold; } #xkomdyqblz .gt_font_italic { font-style: italic; } #xkomdyqblz .gt_super { font-size: 65%; } #xkomdyqblz .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #xkomdyqblz .gt_asterisk { font-size: 100%; vertical-align: 0; } #xkomdyqblz .gt_indent_1 { text-indent: 5px; } #xkomdyqblz .gt_indent_2 { text-indent: 10px; } #xkomdyqblz .gt_indent_3 { text-indent: 15px; } #xkomdyqblz .gt_indent_4 { text-indent: 20px; } #xkomdyqblz .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.36 0.03 0.29 0.43 b_W 0.20 0.04 0.12 0.29 b_S -0.11 0.04 -0.19 -0.03 sigma 0.18 0.03 0.13 0.24 次に、交互作用ありのモデルを考える。 以下では\\(W_{i}\\)と\\(S_{i}\\)は既に中心化されているとする。 \\[ \\begin{aligned} B_{i} &amp;\\sim Normal(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp;= \\alpha + \\beta_{W}W_{i} + \\beta_{S}S_{i} + \\beta_{WS} W_{i}S_{i}\\\\ \\alpha &amp;\\sim Normal(0.5,0.25)\\\\ \\beta_{j} &amp;\\sim Normal(0, 0.25) \\end{aligned} \\] 交互作用も強く働いていることが分かった。 b8.5 &lt;- brm( data = d3, family = gaussian, formula = B ~ 1 + W+S+W:S, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b), prior(exponential(1), class = sigma)), seed=8, sample_prior=TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.5&quot; ) posterior_summary(b8.5) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;^b_&quot;)) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #tsswbockrq .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #tsswbockrq .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #tsswbockrq .gt_caption { padding-top: 4px; padding-bottom: 4px; } #tsswbockrq .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #tsswbockrq .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #tsswbockrq .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #tsswbockrq .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #tsswbockrq .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #tsswbockrq .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #tsswbockrq .gt_column_spanner_outer:first-child { padding-left: 0; } #tsswbockrq .gt_column_spanner_outer:last-child { padding-right: 0; } #tsswbockrq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #tsswbockrq .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #tsswbockrq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #tsswbockrq .gt_from_md > :first-child { margin-top: 0; } #tsswbockrq .gt_from_md > :last-child { margin-bottom: 0; } #tsswbockrq .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #tsswbockrq .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #tsswbockrq .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #tsswbockrq .gt_row_group_first td { border-top-width: 2px; } #tsswbockrq .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #tsswbockrq .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #tsswbockrq .gt_first_summary_row.thick { border-top-width: 2px; } #tsswbockrq .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #tsswbockrq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #tsswbockrq .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #tsswbockrq .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #tsswbockrq .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #tsswbockrq .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #tsswbockrq .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #tsswbockrq .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #tsswbockrq .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #tsswbockrq .gt_left { text-align: left; } #tsswbockrq .gt_center { text-align: center; } #tsswbockrq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #tsswbockrq .gt_font_normal { font-weight: normal; } #tsswbockrq .gt_font_bold { font-weight: bold; } #tsswbockrq .gt_font_italic { font-style: italic; } #tsswbockrq .gt_super { font-size: 65%; } #tsswbockrq .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #tsswbockrq .gt_asterisk { font-size: 100%; vertical-align: 0; } #tsswbockrq .gt_indent_1 { text-indent: 5px; } #tsswbockrq .gt_indent_2 { text-indent: 10px; } #tsswbockrq .gt_indent_3 { text-indent: 15px; } #tsswbockrq .gt_indent_4 { text-indent: 20px; } #tsswbockrq .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.36 0.03 0.30 0.41 b_W 0.21 0.03 0.14 0.27 b_S -0.11 0.03 -0.18 -0.05 b_W:S -0.14 0.04 -0.22 -0.06 sigma 0.14 0.02 0.11 0.20 8.3.3 Plotting posterior predictions まずは、事後分布についてプロットする。 交互作用項を入れたモデルでは、日照条件に応じて傾きが変わっていることが分かる。 seq &lt;- crossing(W = c(-1,1), S = c(-1,0,1)) ## fitする fitted(b8.4, newdata = seq, summary= F) %&gt;% data.frame() %&gt;% slice_sample(n=20) %&gt;% pivot_longer(everything()) %&gt;% bind_cols(tidyr::expand(seq, n=1:20, nesting(W,S))) -&gt; fitted8.4 fitted(b8.5, newdata = seq, summary= F) %&gt;% data.frame() %&gt;% slice_sample(n=20) %&gt;% pivot_longer(everything()) %&gt;% bind_cols(tidyr::expand(seq, n=1:20, nesting(W,S))) -&gt; fitted8.5 labeli8.4 &lt;- as_labeller(c(&quot;-1&quot; = &quot;m8.4 post: shade = -1&quot;, &quot;0&quot; = &quot;m8.4 post: shade = 0&quot;, &quot;1&quot; = &quot;m8.4 post: shade = 1&quot;)) labeli8.5 &lt;- as_labeller(c(&quot;-1&quot; = &quot;m8.5 post: shade = -1&quot;, &quot;0&quot; = &quot;m8.5 post: shade = 0&quot;, &quot;1&quot; = &quot;m8.5 post: shade = 1&quot;)) ## b8.4 p1 &lt;- d3 %&gt;% ggplot(aes(x=W))+ geom_point(aes(y=B), color = &quot;navy&quot;, size=2.5)+ geom_line(data = fitted8.4, aes(y = value, group = n), alpha = 1/2)+ labs(x=&quot;water (centered)&quot;, y = &quot;blooms (std)&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ scale_x_continuous(breaks = c(-1,0,1))+ facet_wrap(~S, labeller = labeli8.4) ## b8.5 p2 &lt;- d3 %&gt;% ggplot(aes(x=W))+ geom_point(aes(y=B), color = &quot;navy&quot;, size=2.5)+ geom_line(data = fitted8.5, aes(y = value, group = n), alpha = 1/2)+ labs(x=&quot;water (centered)&quot;, y = &quot;blooms (std)&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ scale_x_continuous(breaks = c(-1,0,1))+ facet_wrap(~S, labeller = labeli8.5) (p1)/(p2) 8.3.4 Plotting prior predictions 続いて、事前分布も確認する。 b8.4p &lt;- update(b8.4, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;output/Chapter8/b8.4p&quot;) b8.5p &lt;- update(b8.5, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;output/Chapter8/b8.5p&quot;) 事前分布はかなりばらついており、ほとんど情報を持っていないことが分かる。 prior8.4 &lt;- fitted(b8.4p, newdata = seq, summary=F,nsamples=20) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(tidyr::expand(seq, n = 1:20, nesting(W,S))) %&gt;% mutate(model = &quot;b8.4&quot;) prior8.5 &lt;- fitted(b8.5p, newdata = seq, summary=F,nsamples=20) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(tidyr::expand(seq, n = 1:20, nesting(W,S))) %&gt;% mutate(model = &quot;b8.5&quot;) labeli8.4p &lt;- as_labeller(c(&quot;-1&quot; = &quot;m8.4 prior: shade = -1&quot;, &quot;0&quot; = &quot;m8.4 prior: shade = 0&quot;, &quot;1&quot; = &quot;m8.4 prior: shade = 1&quot;)) labeli8.5p &lt;- as_labeller(c(&quot;-1&quot; = &quot;m8.5 prior: shade = -1&quot;, &quot;0&quot; = &quot;m8.5 prior: shade = 0&quot;, &quot;1&quot; = &quot;m8.5 prior: shade = 1&quot;)) ## b8.4 p3 &lt;- d3 %&gt;% ggplot(aes(x=W, y = B))+ geom_line(data = prior8.4, aes(y = value, group = n), alpha = 1/2)+ labs(x=&quot;water (centered)&quot;, y = &quot;blooms (std)&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ scale_x_continuous(breaks = c(-1,0,1))+ geom_hline(yintercept = c(0,1), linetype=2)+ facet_wrap(~S, labeller = labeli8.4p) ## b8.5 p4 &lt;- d3 %&gt;% ggplot(aes(x=W, y = B))+ geom_line(data = prior8.5, aes(y = value, group = n), alpha = 1/2)+ labs(x=&quot;water (centered)&quot;, y = &quot;blooms (std)&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ scale_x_continuous(breaks = c(-1,0,1))+ geom_hline(yintercept = c(0,1), linetype=2)+ facet_wrap(~S, labeller = labeli8.4p) p3/p4 8.4 Practice 8.4.1 8M4 Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior asumptions mean for the interaction prior, if anything? かなり傾きが制限されていることが分かる。 b8.5_2 &lt;- brm( data = d3, family = gaussian, formula = B ~ 1 + W+S+W:S, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0.25, 0.125), class = b,coef = W), prior(normal(-0.25, 0.125), class = b,coef = S), prior(normal(-0.25, 0.125), class = b,coef = W:S), prior(exponential(1), class = sigma)), seed=8, sample_prior=&quot;only&quot;, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.5_2&quot; ) prior8.5_2 &lt;- fitted(b8.5_2, newdata = seq, summary=F,nsamples=20) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(tidyr::expand(seq, n = 1:20, nesting(W,S))) d3 %&gt;% ggplot(aes(x=W, y = B))+ geom_line(data = prior8.5_2, aes(y = value, group = n), alpha = 1/2)+ labs(x=&quot;water (centered)&quot;, y = &quot;blooms (std)&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ scale_x_continuous(breaks = c(-1,0,1))+ geom_hline(yintercept = c(0,1), linetype=2)+ facet_wrap(~S, labeller = labeli8.4p) 結果も傾きが強くなるように少し変化。 b8.5_3 &lt;- update(b8.5_2, sample_prior = &quot;yes&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;output/Chapter8/b8.5_3&quot;) posterior_summary(b8.5) %&gt;% data.frame() %&gt;% slice(n=1:5) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xajbkdcqgu .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xajbkdcqgu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xajbkdcqgu .gt_caption { padding-top: 4px; padding-bottom: 4px; } #xajbkdcqgu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xajbkdcqgu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #xajbkdcqgu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xajbkdcqgu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xajbkdcqgu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xajbkdcqgu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xajbkdcqgu .gt_column_spanner_outer:first-child { padding-left: 0; } #xajbkdcqgu .gt_column_spanner_outer:last-child { padding-right: 0; } #xajbkdcqgu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #xajbkdcqgu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #xajbkdcqgu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xajbkdcqgu .gt_from_md > :first-child { margin-top: 0; } #xajbkdcqgu .gt_from_md > :last-child { margin-bottom: 0; } #xajbkdcqgu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xajbkdcqgu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #xajbkdcqgu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #xajbkdcqgu .gt_row_group_first td { border-top-width: 2px; } #xajbkdcqgu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xajbkdcqgu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #xajbkdcqgu .gt_first_summary_row.thick { border-top-width: 2px; } #xajbkdcqgu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xajbkdcqgu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xajbkdcqgu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xajbkdcqgu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xajbkdcqgu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xajbkdcqgu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xajbkdcqgu .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #xajbkdcqgu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xajbkdcqgu .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #xajbkdcqgu .gt_left { text-align: left; } #xajbkdcqgu .gt_center { text-align: center; } #xajbkdcqgu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xajbkdcqgu .gt_font_normal { font-weight: normal; } #xajbkdcqgu .gt_font_bold { font-weight: bold; } #xajbkdcqgu .gt_font_italic { font-style: italic; } #xajbkdcqgu .gt_super { font-size: 65%; } #xajbkdcqgu .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #xajbkdcqgu .gt_asterisk { font-size: 100%; vertical-align: 0; } #xajbkdcqgu .gt_indent_1 { text-indent: 5px; } #xajbkdcqgu .gt_indent_2 { text-indent: 10px; } #xajbkdcqgu .gt_indent_3 { text-indent: 15px; } #xajbkdcqgu .gt_indent_4 { text-indent: 20px; } #xajbkdcqgu .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 0.3580160 0.02742670 0.3043584 0.41166235 0.2052897 0.03436065 0.1368625 0.27334710 -0.1138346 0.03392792 -0.1785913 -0.04600386 -0.1436845 0.04113197 -0.2230427 -0.06313104 0.1431426 0.02297448 0.1064527 0.19516133 posterior_summary(b8.5_3) %&gt;% data.frame() %&gt;% slice(n=1:5) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #jdglcaagto .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #jdglcaagto .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jdglcaagto .gt_caption { padding-top: 4px; padding-bottom: 4px; } #jdglcaagto .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #jdglcaagto .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #jdglcaagto .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jdglcaagto .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jdglcaagto .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #jdglcaagto .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #jdglcaagto .gt_column_spanner_outer:first-child { padding-left: 0; } #jdglcaagto .gt_column_spanner_outer:last-child { padding-right: 0; } #jdglcaagto .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #jdglcaagto .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #jdglcaagto .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #jdglcaagto .gt_from_md > :first-child { margin-top: 0; } #jdglcaagto .gt_from_md > :last-child { margin-bottom: 0; } #jdglcaagto .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #jdglcaagto .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #jdglcaagto .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #jdglcaagto .gt_row_group_first td { border-top-width: 2px; } #jdglcaagto .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jdglcaagto .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #jdglcaagto .gt_first_summary_row.thick { border-top-width: 2px; } #jdglcaagto .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jdglcaagto .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jdglcaagto .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #jdglcaagto .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #jdglcaagto .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jdglcaagto .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jdglcaagto .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #jdglcaagto .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jdglcaagto .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #jdglcaagto .gt_left { text-align: left; } #jdglcaagto .gt_center { text-align: center; } #jdglcaagto .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #jdglcaagto .gt_font_normal { font-weight: normal; } #jdglcaagto .gt_font_bold { font-weight: bold; } #jdglcaagto .gt_font_italic { font-style: italic; } #jdglcaagto .gt_super { font-size: 65%; } #jdglcaagto .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #jdglcaagto .gt_asterisk { font-size: 100%; vertical-align: 0; } #jdglcaagto .gt_indent_1 { text-indent: 5px; } #jdglcaagto .gt_indent_2 { text-indent: 10px; } #jdglcaagto .gt_indent_3 { text-indent: 15px; } #jdglcaagto .gt_indent_4 { text-indent: 20px; } #jdglcaagto .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 0.3588534 0.02685320 0.3079950 0.41378615 0.2130110 0.03302569 0.1486325 0.27752850 -0.1233921 0.03278545 -0.1877740 -0.05758068 -0.1566358 0.04003497 -0.2367679 -0.07847675 0.1428600 0.02232079 0.1074478 0.19472025 8.4.2 8H1 Return to the data(tulips) example in the chapter. Now include the bed variable as a predictor in the interaction model. Don’t interact bed with the other predictors; just include it as a main effect. Note that bed is categorical. So to use it properly, you will need to either construct dummy variables, or rather an index variable, as explained in Chapter 5. d3 %&gt;% mutate(BD = ifelse(bed==&quot;a&quot;,1,ifelse(bed==&quot;b&quot;,2,3))) %&gt;% mutate(BD = as.factor(BD))-&gt; d4 b8.6 &lt;- brm( data = d4, family = gaussian, formula = B ~ 0 +BD + W + S + W:S, prior = c(prior(normal(0.5, 0.25), class = b, coef = BD1), prior(normal(0.5, 0.25), class = b, coef = BD2), prior(normal(0.5, 0.25), class = b, coef = BD3), prior(normal(0, 0.25), class = b, coef =W), prior(normal(0, 0.25), class = b, coef =S), prior(normal(0, 0.25), class = b, coef =W:S), prior(exponential(1), class = sigma)), seed=8, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.6&quot; ) posterior_summary(b8.6) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;^b_&quot;)) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #pgtfccecvh .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #pgtfccecvh .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pgtfccecvh .gt_caption { padding-top: 4px; padding-bottom: 4px; } #pgtfccecvh .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #pgtfccecvh .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #pgtfccecvh .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pgtfccecvh .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pgtfccecvh .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #pgtfccecvh .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #pgtfccecvh .gt_column_spanner_outer:first-child { padding-left: 0; } #pgtfccecvh .gt_column_spanner_outer:last-child { padding-right: 0; } #pgtfccecvh .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #pgtfccecvh .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #pgtfccecvh .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #pgtfccecvh .gt_from_md > :first-child { margin-top: 0; } #pgtfccecvh .gt_from_md > :last-child { margin-bottom: 0; } #pgtfccecvh .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #pgtfccecvh .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #pgtfccecvh .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #pgtfccecvh .gt_row_group_first td { border-top-width: 2px; } #pgtfccecvh .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pgtfccecvh .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #pgtfccecvh .gt_first_summary_row.thick { border-top-width: 2px; } #pgtfccecvh .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pgtfccecvh .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pgtfccecvh .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #pgtfccecvh .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #pgtfccecvh .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pgtfccecvh .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pgtfccecvh .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #pgtfccecvh .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pgtfccecvh .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #pgtfccecvh .gt_left { text-align: left; } #pgtfccecvh .gt_center { text-align: center; } #pgtfccecvh .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pgtfccecvh .gt_font_normal { font-weight: normal; } #pgtfccecvh .gt_font_bold { font-weight: bold; } #pgtfccecvh .gt_font_italic { font-style: italic; } #pgtfccecvh .gt_super { font-size: 65%; } #pgtfccecvh .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #pgtfccecvh .gt_asterisk { font-size: 100%; vertical-align: 0; } #pgtfccecvh .gt_indent_1 { text-indent: 5px; } #pgtfccecvh .gt_indent_2 { text-indent: 10px; } #pgtfccecvh .gt_indent_3 { text-indent: 15px; } #pgtfccecvh .gt_indent_4 { text-indent: 20px; } #pgtfccecvh .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_BD1 0.28 0.05 0.19 0.37 b_BD2 0.40 0.04 0.31 0.49 b_BD3 0.41 0.04 0.32 0.50 b_W 0.21 0.03 0.14 0.27 b_S -0.11 0.03 -0.17 -0.05 b_W:S -0.14 0.04 -0.21 -0.07 sigma 0.13 0.02 0.10 0.18 8.4.3 8H2 Use WAIC to compare the model from 8H1 to a model that omits bed. What do you infer from this comparison? Can you reconcile the WAIC results with the posterior distribution of the bed coefficients? b8.5と比較。 b8.6の方がいいが、あまり変わらない。 b8.5 &lt;- add_criterion(b8.5, c(&quot;waic&quot;,&quot;loo&quot;)) b8.6 &lt;- add_criterion(b8.6, c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b8.5,b8.6, criterion =&quot;waic&quot;) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b8.6 0.0 0.0 13.9 3.2 6.0 1.3 -27.7 6.5 ## b8.5 -1.5 2.7 12.4 3.8 4.4 1.2 -24.7 7.6 model_weights(b8.5, b8.6, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b8.5 b8.6 ## 0.18 0.82 8.4.4 8H3 Consider again the data(rugged) data on economic development and terrain ruggedness, examined in this chapter. One of the African countries in that example Seychelles, is far outside the cloud of other nations, being a rare country with both relatively high GDP and high ruggedness. Seychelles is also unusual, in that it is a group of islands far from the coast of mainland Africa, and its main economic activity is tourism. 8.4.4.1 a Focus on model m8.5 from the chapter. Use WAIC pointwise penalties and PSIS Pareto k values to measure relative influence of each country. By these criteria, is Seychelles influencing the results? Are there other nations that are relatively influential? If so, can you explain why? シチリアを含め、いくつかkやp_waicが高い国があるよう。 library(loo) b8.3 &lt;- add_criterion(b8.3, c(&quot;waic&quot;,&quot;loo&quot;)) k &lt;- pareto_k_values(loo(b8.3)) tibble(k = k, p_waic = b8.3$criteria$waic$pointwise[,2], Loc = pull(d2,country)) %&gt;% ggplot(aes(x = k, y = p_waic, color = Loc == &quot;Seychelles&quot;))+ geom_point(aes(shape = Loc == &quot;ID&quot;),size=2, stroke=1)+ geom_text_repel(data = . %&gt;% filter(k &gt; 0.3| p_waic &gt; 0.3), aes(label=Loc))+ scale_shape_manual(values=c(1,19))+ scale_color_manual(values=c(&quot;navy&quot;,&quot;red&quot;))+ theme(legend.position=&quot;none&quot;, aspect.ratio=1)+ labs(title = &quot;b8.3&quot;) -&gt;p5 p5 8.4.4.2 b Now use robust regression, as described in the previous chapter. Modify m8.5 to se a Student-t distribution with ν=2. Does this change the results in a substantial way? student_tを用いてrobust回帰を行う。 b8.3r &lt;- brm( data = d2, family = student, formula = bf(G ~ 0 + a + b * R_c, a ~ 0 + A_ind, b ~ 0 + A_ind, nl = TRUE, nu =2), prior = c(prior(normal(1, 0.1), class = b, coef=&quot;A_ind1&quot;, nlpar=a), prior(normal(1, 0.1), class = b, coef=&quot;A_ind2&quot;,nlpar=a), prior(normal(1, 0.3), class = b, coef=&quot;A_ind1&quot;, nlpar=b), prior(normal(1, 0.3), class = b, coef=&quot;A_ind2&quot;,nlpar=b), prior(exponential(1), class = sigma)), seed=8, sample_prior=TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter8/b8.3r&quot; ) posterior_summary(b8.3) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;^b_&quot;)) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kzvcrfoztt .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kzvcrfoztt .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kzvcrfoztt .gt_caption { padding-top: 4px; padding-bottom: 4px; } #kzvcrfoztt .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kzvcrfoztt .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #kzvcrfoztt .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kzvcrfoztt .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kzvcrfoztt .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kzvcrfoztt .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kzvcrfoztt .gt_column_spanner_outer:first-child { padding-left: 0; } #kzvcrfoztt .gt_column_spanner_outer:last-child { padding-right: 0; } #kzvcrfoztt .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #kzvcrfoztt .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #kzvcrfoztt .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kzvcrfoztt .gt_from_md > :first-child { margin-top: 0; } #kzvcrfoztt .gt_from_md > :last-child { margin-bottom: 0; } #kzvcrfoztt .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kzvcrfoztt .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #kzvcrfoztt .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #kzvcrfoztt .gt_row_group_first td { border-top-width: 2px; } #kzvcrfoztt .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kzvcrfoztt .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #kzvcrfoztt .gt_first_summary_row.thick { border-top-width: 2px; } #kzvcrfoztt .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kzvcrfoztt .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kzvcrfoztt .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kzvcrfoztt .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kzvcrfoztt .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kzvcrfoztt .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kzvcrfoztt .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #kzvcrfoztt .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kzvcrfoztt .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #kzvcrfoztt .gt_left { text-align: left; } #kzvcrfoztt .gt_center { text-align: center; } #kzvcrfoztt .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kzvcrfoztt .gt_font_normal { font-weight: normal; } #kzvcrfoztt .gt_font_bold { font-weight: bold; } #kzvcrfoztt .gt_font_italic { font-style: italic; } #kzvcrfoztt .gt_super { font-size: 65%; } #kzvcrfoztt .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #kzvcrfoztt .gt_asterisk { font-size: 100%; vertical-align: 0; } #kzvcrfoztt .gt_indent_1 { text-indent: 5px; } #kzvcrfoztt .gt_indent_2 { text-indent: 10px; } #kzvcrfoztt .gt_indent_3 { text-indent: 15px; } #kzvcrfoztt .gt_indent_4 { text-indent: 20px; } #kzvcrfoztt .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_a_A_ind1 0.89 0.02 0.86 0.92 b_a_A_ind2 1.05 0.01 1.03 1.07 b_b_A_ind1 0.20 0.08 0.05 0.35 b_b_A_ind2 -0.11 0.06 -0.22 0.01 sigma 0.11 0.01 0.10 0.12 posterior_summary(b8.3r) %&gt;% data.frame() %&gt;% round(2) %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters %in% c(&quot;sigma&quot;)| str_detect(parameters,&quot;^b_&quot;)) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #umjvxxgprs .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #umjvxxgprs .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #umjvxxgprs .gt_caption { padding-top: 4px; padding-bottom: 4px; } #umjvxxgprs .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #umjvxxgprs .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #umjvxxgprs .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #umjvxxgprs .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #umjvxxgprs .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #umjvxxgprs .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #umjvxxgprs .gt_column_spanner_outer:first-child { padding-left: 0; } #umjvxxgprs .gt_column_spanner_outer:last-child { padding-right: 0; } #umjvxxgprs .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #umjvxxgprs .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #umjvxxgprs .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #umjvxxgprs .gt_from_md > :first-child { margin-top: 0; } #umjvxxgprs .gt_from_md > :last-child { margin-bottom: 0; } #umjvxxgprs .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #umjvxxgprs .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #umjvxxgprs .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #umjvxxgprs .gt_row_group_first td { border-top-width: 2px; } #umjvxxgprs .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #umjvxxgprs .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #umjvxxgprs .gt_first_summary_row.thick { border-top-width: 2px; } #umjvxxgprs .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #umjvxxgprs .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #umjvxxgprs .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #umjvxxgprs .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #umjvxxgprs .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #umjvxxgprs .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #umjvxxgprs .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #umjvxxgprs .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #umjvxxgprs .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #umjvxxgprs .gt_left { text-align: left; } #umjvxxgprs .gt_center { text-align: center; } #umjvxxgprs .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #umjvxxgprs .gt_font_normal { font-weight: normal; } #umjvxxgprs .gt_font_bold { font-weight: bold; } #umjvxxgprs .gt_font_italic { font-style: italic; } #umjvxxgprs .gt_super { font-size: 65%; } #umjvxxgprs .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #umjvxxgprs .gt_asterisk { font-size: 100%; vertical-align: 0; } #umjvxxgprs .gt_indent_1 { text-indent: 5px; } #umjvxxgprs .gt_indent_2 { text-indent: 10px; } #umjvxxgprs .gt_indent_3 { text-indent: 15px; } #umjvxxgprs .gt_indent_4 { text-indent: 20px; } #umjvxxgprs .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_a_A_ind1 0.88 0.02 0.84 0.91 b_a_A_ind2 1.05 0.01 1.02 1.07 b_b_A_ind1 0.24 0.10 0.05 0.44 b_b_A_ind2 -0.16 0.07 -0.29 -0.01 sigma 0.09 0.01 0.07 0.10 結果はほとんど変わらない。 pareto-kとp_waicの値は改善した？。 b8.3r &lt;- add_criterion(b8.3r, c(&quot;waic&quot;,&quot;loo&quot;)) k &lt;- pareto_k_values(loo(b8.3r)) tibble(k = k, p_waic = b8.3r$criteria$waic$pointwise[,2], Loc = pull(d2,country)) %&gt;% ggplot(aes(x = k, y = p_waic, color = Loc == &quot;Seychelles&quot;))+ geom_point(aes(shape = Loc == &quot;ID&quot;),size=2, stroke=1)+ geom_text_repel(data = . %&gt;% filter(k &gt; 0.3| p_waic &gt; 0.3), aes(label=Loc))+ scale_shape_manual(values=c(1,19))+ scale_color_manual(values=c(&quot;navy&quot;,&quot;red&quot;))+ theme(legend.position=&quot;none&quot;, aspect.ratio=1)+ labs(title = &quot;b8.3r&quot;) -&gt; p6 p5+p6 8.4.5 8H4 The values in data(nettle) are data on language diversity in 74 nations.1 The meaning of each column is given below. country: Name of the country num.lang: Number of recognized languages spoken area: Area in square kilometers k.pop: Population, in thousands num.stations: Number of weather stations that provided data for the next two columns mean.growing.season: Average length of growing season, in months sd.growing.season: Standard deviation of length of growing season, in months Use these data to evaluate the hypothesis that language diversity is partly a product of food security. The notion is that, in productive ecologies, people don’t need large social networks to buffer them against risk of food shortfalls. The means cultural groups can be smaller and more self-sufficient, leading to more languages per capita. Use the number of languages per capita as the outcome: data(nettle) d5 &lt;- nettle d5$lang.per.cap &lt;- d5$num.lang / d5$k.pop Use the logarithm of this new variable as your regression outcome. (A count model would be better here, but you’ll learn those later, in Chapter 11.) This problem is open ended, allowing you to decide how you address the hypotheses and the uncertain advice the modeling provides. If you think you need to use WAIC anyplace, please do. If you think you need certain priors, argue for them. If you think you need to plot predictions in a certain way, please do. Just try to honestly evaluate the main effects of both mean.growing.season and sd.growing.season, as well as their two-way interaction. Here are three parts to help. d5 %&gt;% mutate(L = log(num.lang/k.pop), meanG = standardize(mean.growing.season), sdG = standardize(sd.growing.season), A = standardize(log(area))) -&gt; d5 d5 %&gt;% dplyr::select(L, meanG, sdG, A) %&gt;% pairs() d5 %&gt;% dplyr::select(L, meanG, sdG, A) %&gt;% cor() ## L meanG sdG A ## L 1.0000000 0.35963657 -0.25338920 -0.2785991 ## meanG 0.3596366 1.00000000 0.02136416 -0.3692084 ## sdG -0.2533892 0.02136416 1.00000000 0.5320083 ## A -0.2785991 -0.36920845 0.53200827 1.0000000 プロットしてみる。 ## L ~ meanG d5 %&gt;% ggplot(aes(x=meanG, y=L))+ geom_point()+ theme(aspect.ratio=1)+ geom_smooth(method=&quot;lm&quot;)-&gt; p6 ## L ~ sdG d5 %&gt;% ggplot(aes(x=sdG, y=L))+ geom_point()+ theme(aspect.ratio=1)+ geom_smooth(method=&quot;lm&quot;) -&gt;p7 ## L ~ area d5 %&gt;% ggplot(aes(x=A, y=L))+ geom_point()+ theme(aspect.ratio=1)+ geom_smooth(method=&quot;lm&quot;) -&gt;p8 p6 + p7 + p8 ## meanG ~ area d5 %&gt;% ggplot(aes(x=A, y=meanG))+ geom_point()+ theme(aspect.ratio=1)+ geom_smooth(method=&quot;lm&quot;) -&gt;p9 ## sdG ~ area d5 %&gt;% ggplot(aes(x=A, y=sdG))+ geom_point()+ theme(aspect.ratio=1)+ geom_smooth(method=&quot;lm&quot;) -&gt;p10 p9 + p10 8.4.5.1 a Evaluate the hypothesis that language diversity, as measured by log(lang.per.cap), is positively associated with average length of the growing season, mean.growing.season. Consider log(area) in your regression(s) as a covariate (not an interaction). Interpret your results. まずは、L ~ meanG + Aを考える。 b8H4.a &lt;- brm(data = d5, family = gaussian, formula = L ~ 1 + meanG + A, prior =c(prior(normal(-5.5, 1.5), class = Intercept), prior(normal(0,2),class=b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =8, file = &quot;output/Chapter8/b8H4.a&quot;) posterior_summary(b8H4.a)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gxcxjwxoqd .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gxcxjwxoqd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gxcxjwxoqd .gt_caption { padding-top: 4px; padding-bottom: 4px; } #gxcxjwxoqd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gxcxjwxoqd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #gxcxjwxoqd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gxcxjwxoqd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gxcxjwxoqd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gxcxjwxoqd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gxcxjwxoqd .gt_column_spanner_outer:first-child { padding-left: 0; } #gxcxjwxoqd .gt_column_spanner_outer:last-child { padding-right: 0; } #gxcxjwxoqd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #gxcxjwxoqd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #gxcxjwxoqd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gxcxjwxoqd .gt_from_md > :first-child { margin-top: 0; } #gxcxjwxoqd .gt_from_md > :last-child { margin-bottom: 0; } #gxcxjwxoqd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gxcxjwxoqd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #gxcxjwxoqd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #gxcxjwxoqd .gt_row_group_first td { border-top-width: 2px; } #gxcxjwxoqd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gxcxjwxoqd .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #gxcxjwxoqd .gt_first_summary_row.thick { border-top-width: 2px; } #gxcxjwxoqd .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gxcxjwxoqd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gxcxjwxoqd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gxcxjwxoqd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gxcxjwxoqd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gxcxjwxoqd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gxcxjwxoqd .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #gxcxjwxoqd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gxcxjwxoqd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #gxcxjwxoqd .gt_left { text-align: left; } #gxcxjwxoqd .gt_center { text-align: center; } #gxcxjwxoqd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gxcxjwxoqd .gt_font_normal { font-weight: normal; } #gxcxjwxoqd .gt_font_bold { font-weight: bold; } #gxcxjwxoqd .gt_font_italic { font-style: italic; } #gxcxjwxoqd .gt_super { font-size: 65%; } #gxcxjwxoqd .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #gxcxjwxoqd .gt_asterisk { font-size: 100%; vertical-align: 0; } #gxcxjwxoqd .gt_indent_1 { text-indent: 5px; } #gxcxjwxoqd .gt_indent_2 { text-indent: 10px; } #gxcxjwxoqd .gt_indent_3 { text-indent: 15px; } #gxcxjwxoqd .gt_indent_4 { text-indent: 20px; } #gxcxjwxoqd .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.46 0.17 -5.79 -5.12 0.45 0.18 0.11 0.80 -0.26 0.18 -0.61 0.11 1.43 0.12 1.21 1.69 -6.03 0.13 -6.30 -5.80 -137.04 1.47 -140.73 -135.22 事前分布の妥当性を確認。 若干ばらつきはあるものの、大丈夫そう？ b8H4.a_p &lt;- update(b8H4.a, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;output/Chapter8/b8H5.a_p&quot;) seq &lt;- tibble(meanG = c(-2.5, 2), A = mean(d5$A)) prior8H4 &lt;- fitted(b8H4.a_p, newdata = seq, summary=F,nsamples=50) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(tidyr::expand(seq, n = 1:50, nesting(meanG,A))) d5 %&gt;% ggplot(aes(x=meanG, y = L))+ geom_line(data = prior8H4, aes(y = value, group = n), alpha = 1/2)+ labs(x=&quot;meanG&quot;, y = &quot;L&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ scale_x_continuous(breaks = seq(-2.5,2,by=0.5))+ coord_cartesian(ylim = c(-15,5))+ geom_hline(yintercept = c(min(d5$L),max(d5$L)), linetype=2) ちなみに、meanGだけで回帰すると…。 結果を見ると、meanGもAもそれぞれLに少なからず影響しているよう。 b8H4.a_2 &lt;- brm(data = d5, family = gaussian, formula = L ~ 1 + meanG, prior =c(prior(normal(-5.5, 1.5), class = Intercept), prior(normal(0,2),class=b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =8, file = &quot;output/Chapter8/b8H4.a_2&quot;) posterior_summary(b8H4.a)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xapqlayjzw .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xapqlayjzw .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xapqlayjzw .gt_caption { padding-top: 4px; padding-bottom: 4px; } #xapqlayjzw .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xapqlayjzw .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #xapqlayjzw .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xapqlayjzw .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xapqlayjzw .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xapqlayjzw .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xapqlayjzw .gt_column_spanner_outer:first-child { padding-left: 0; } #xapqlayjzw .gt_column_spanner_outer:last-child { padding-right: 0; } #xapqlayjzw .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #xapqlayjzw .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #xapqlayjzw .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xapqlayjzw .gt_from_md > :first-child { margin-top: 0; } #xapqlayjzw .gt_from_md > :last-child { margin-bottom: 0; } #xapqlayjzw .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xapqlayjzw .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #xapqlayjzw .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #xapqlayjzw .gt_row_group_first td { border-top-width: 2px; } #xapqlayjzw .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xapqlayjzw .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #xapqlayjzw .gt_first_summary_row.thick { border-top-width: 2px; } #xapqlayjzw .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xapqlayjzw .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xapqlayjzw .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xapqlayjzw .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xapqlayjzw .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xapqlayjzw .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xapqlayjzw .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #xapqlayjzw .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xapqlayjzw .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #xapqlayjzw .gt_left { text-align: left; } #xapqlayjzw .gt_center { text-align: center; } #xapqlayjzw .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xapqlayjzw .gt_font_normal { font-weight: normal; } #xapqlayjzw .gt_font_bold { font-weight: bold; } #xapqlayjzw .gt_font_italic { font-style: italic; } #xapqlayjzw .gt_super { font-size: 65%; } #xapqlayjzw .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #xapqlayjzw .gt_asterisk { font-size: 100%; vertical-align: 0; } #xapqlayjzw .gt_indent_1 { text-indent: 5px; } #xapqlayjzw .gt_indent_2 { text-indent: 10px; } #xapqlayjzw .gt_indent_3 { text-indent: 15px; } #xapqlayjzw .gt_indent_4 { text-indent: 20px; } #xapqlayjzw .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.46 0.17 -5.79 -5.12 0.45 0.18 0.11 0.80 -0.26 0.18 -0.61 0.11 1.43 0.12 1.21 1.69 -6.03 0.13 -6.30 -5.80 -137.04 1.47 -140.73 -135.22 posterior_summary(b8H4.a_2)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qmoffbhhzy .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qmoffbhhzy .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qmoffbhhzy .gt_caption { padding-top: 4px; padding-bottom: 4px; } #qmoffbhhzy .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qmoffbhhzy .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #qmoffbhhzy .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qmoffbhhzy .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qmoffbhhzy .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qmoffbhhzy .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qmoffbhhzy .gt_column_spanner_outer:first-child { padding-left: 0; } #qmoffbhhzy .gt_column_spanner_outer:last-child { padding-right: 0; } #qmoffbhhzy .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #qmoffbhhzy .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #qmoffbhhzy .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qmoffbhhzy .gt_from_md > :first-child { margin-top: 0; } #qmoffbhhzy .gt_from_md > :last-child { margin-bottom: 0; } #qmoffbhhzy .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qmoffbhhzy .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #qmoffbhhzy .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #qmoffbhhzy .gt_row_group_first td { border-top-width: 2px; } #qmoffbhhzy .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qmoffbhhzy .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #qmoffbhhzy .gt_first_summary_row.thick { border-top-width: 2px; } #qmoffbhhzy .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qmoffbhhzy .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qmoffbhhzy .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qmoffbhhzy .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qmoffbhhzy .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qmoffbhhzy .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qmoffbhhzy .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #qmoffbhhzy .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qmoffbhhzy .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qmoffbhhzy .gt_left { text-align: left; } #qmoffbhhzy .gt_center { text-align: center; } #qmoffbhhzy .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qmoffbhhzy .gt_font_normal { font-weight: normal; } #qmoffbhhzy .gt_font_bold { font-weight: bold; } #qmoffbhhzy .gt_font_italic { font-style: italic; } #qmoffbhhzy .gt_super { font-size: 65%; } #qmoffbhhzy .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #qmoffbhhzy .gt_asterisk { font-size: 100%; vertical-align: 0; } #qmoffbhhzy .gt_indent_1 { text-indent: 5px; } #qmoffbhhzy .gt_indent_2 { text-indent: 10px; } #qmoffbhhzy .gt_indent_3 { text-indent: 15px; } #qmoffbhhzy .gt_indent_4 { text-indent: 20px; } #qmoffbhhzy .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.45 0.17 -5.78 -5.12 0.54 0.17 0.21 0.87 1.43 0.12 1.23 1.69 -4.42 0.12 -4.68 -4.20 -135.95 1.22 -139.09 -134.54 一方でPSISをみると、Aはあまり予測には寄与していない。 b8H4.a &lt;- add_criterion(b8H4.a, c(&quot;waic&quot;,&quot;loo&quot;)) b8H4.a_2 &lt;- add_criterion(b8H4.a_2, c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b8H4.a, b8H4.a_2) %&gt;% print(simplify=F) %&gt;% data.frame()-&gt; loo_a ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b8H4.a_2 0.0 0.0 -133.7 7.3 3.4 1.0 267.4 14.6 ## b8H4.a -0.2 1.7 -133.9 7.5 4.6 1.4 267.8 15.1 model_weights(b8H4.a, b8H4.a_2) ## b8H4.a b8H4.a_2 ## 0.4116151 0.5883849 8.4.5.2 b Now evaluate the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, sd.growing.season. This hypothesis follows from uncertainty in harvest favoring social insurance through larger social networks and therefore fewer languages. Again, consider log(area) as a covariate (not an interaction). Interpret your results. 次に、L ~ sdGを考える。 b8H4.b &lt;- brm(data = d5, family = gaussian, formula = L ~ 1 + sdG, prior =c(prior(normal(-5.5, 1.5), class = Intercept), prior(normal(0,2),class=b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =8, file = &quot;output/Chapter8/b8H4.b&quot;) L ~ sdG + Aも考えてみる。 b8H4.b_2 &lt;- brm(data = d5, family = gaussian, formula = L ~ 1 + A + sdG, prior =c(prior(normal(-5.5, 1.5), class = Intercept), prior(normal(0,2),class=b), prior(exponential(1),class=sigma)), seed =8, file = &quot;output/Chapter8/b8H4.b_2&quot;) 結果を見てみると…。 posterior_summary(b8H4.b)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #arnbxnudwo .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #arnbxnudwo .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #arnbxnudwo .gt_caption { padding-top: 4px; padding-bottom: 4px; } #arnbxnudwo .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #arnbxnudwo .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #arnbxnudwo .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #arnbxnudwo .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #arnbxnudwo .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #arnbxnudwo .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #arnbxnudwo .gt_column_spanner_outer:first-child { padding-left: 0; } #arnbxnudwo .gt_column_spanner_outer:last-child { padding-right: 0; } #arnbxnudwo .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #arnbxnudwo .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #arnbxnudwo .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #arnbxnudwo .gt_from_md > :first-child { margin-top: 0; } #arnbxnudwo .gt_from_md > :last-child { margin-bottom: 0; } #arnbxnudwo .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #arnbxnudwo .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #arnbxnudwo .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #arnbxnudwo .gt_row_group_first td { border-top-width: 2px; } #arnbxnudwo .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #arnbxnudwo .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #arnbxnudwo .gt_first_summary_row.thick { border-top-width: 2px; } #arnbxnudwo .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #arnbxnudwo .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #arnbxnudwo .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #arnbxnudwo .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #arnbxnudwo .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #arnbxnudwo .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #arnbxnudwo .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #arnbxnudwo .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #arnbxnudwo .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #arnbxnudwo .gt_left { text-align: left; } #arnbxnudwo .gt_center { text-align: center; } #arnbxnudwo .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #arnbxnudwo .gt_font_normal { font-weight: normal; } #arnbxnudwo .gt_font_bold { font-weight: bold; } #arnbxnudwo .gt_font_italic { font-style: italic; } #arnbxnudwo .gt_super { font-size: 65%; } #arnbxnudwo .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #arnbxnudwo .gt_asterisk { font-size: 100%; vertical-align: 0; } #arnbxnudwo .gt_indent_1 { text-indent: 5px; } #arnbxnudwo .gt_indent_2 { text-indent: 10px; } #arnbxnudwo .gt_indent_3 { text-indent: 15px; } #arnbxnudwo .gt_indent_4 { text-indent: 20px; } #arnbxnudwo .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.46 0.17 -5.78 -5.12 -0.38 0.18 -0.73 -0.02 1.49 0.13 1.27 1.76 -4.46 0.13 -4.73 -4.23 -138.65 1.23 -141.81 -137.23 posterior_summary(b8H4.b_2)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #dknqhblerk .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #dknqhblerk .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #dknqhblerk .gt_caption { padding-top: 4px; padding-bottom: 4px; } #dknqhblerk .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #dknqhblerk .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #dknqhblerk .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #dknqhblerk .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #dknqhblerk .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #dknqhblerk .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #dknqhblerk .gt_column_spanner_outer:first-child { padding-left: 0; } #dknqhblerk .gt_column_spanner_outer:last-child { padding-right: 0; } #dknqhblerk .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #dknqhblerk .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #dknqhblerk .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #dknqhblerk .gt_from_md > :first-child { margin-top: 0; } #dknqhblerk .gt_from_md > :last-child { margin-bottom: 0; } #dknqhblerk .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #dknqhblerk .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #dknqhblerk .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #dknqhblerk .gt_row_group_first td { border-top-width: 2px; } #dknqhblerk .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #dknqhblerk .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #dknqhblerk .gt_first_summary_row.thick { border-top-width: 2px; } #dknqhblerk .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #dknqhblerk .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #dknqhblerk .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #dknqhblerk .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #dknqhblerk .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #dknqhblerk .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #dknqhblerk .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #dknqhblerk .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #dknqhblerk .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #dknqhblerk .gt_left { text-align: left; } #dknqhblerk .gt_center { text-align: center; } #dknqhblerk .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #dknqhblerk .gt_font_normal { font-weight: normal; } #dknqhblerk .gt_font_bold { font-weight: bold; } #dknqhblerk .gt_font_italic { font-style: italic; } #dknqhblerk .gt_super { font-size: 65%; } #dknqhblerk .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #dknqhblerk .gt_asterisk { font-size: 100%; vertical-align: 0; } #dknqhblerk .gt_indent_1 { text-indent: 5px; } #dknqhblerk .gt_indent_2 { text-indent: 10px; } #dknqhblerk .gt_indent_3 { text-indent: 15px; } #dknqhblerk .gt_indent_4 { text-indent: 20px; } #dknqhblerk .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.45 0.17 -5.79 -5.12 -0.30 0.20 -0.70 0.10 -0.22 0.20 -0.63 0.18 1.48 0.12 1.27 1.75 -6.06 0.13 -6.34 -5.84 -139.58 1.44 -143.34 -137.79 PSISも比較。 Aは加えても加えなくても変わらない。 b8H4.b &lt;- add_criterion(b8H4.b, c(&quot;waic&quot;,&quot;loo&quot;)) b8H4.b_2 &lt;- add_criterion(b8H4.b_2, c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b8H4.b, b8H4.b_2) %&gt;% print(simplify=F) %&gt;% data.frame() -&gt; loo_b ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b8H4.b_2 0.0 0.0 -136.5 8.0 4.8 1.3 273.0 15.9 ## b8H4.b -0.1 1.9 -136.6 8.2 3.7 1.1 273.1 16.5 model_weights(b8H4.b, b8H4.b_2) ## b8H4.b b8H4.b_2 ## 0.4869283 0.5130717 8.4.5.3 c Finally, evaluate the hypothesis that mean.growing.season and sd.growing.season interact to synergistically reduce language diversity. The idea is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect windfalls to be used during the droughts. まずは、交互作用なしのモデルを考える。 b8H4.c &lt;- brm(data = d5, family = gaussian, formula = L ~ 1 + A + meanG + sdG, prior =c(prior(normal(-5.5, 1.5), class = Intercept), prior(normal(0,2),class=b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =8, file = &quot;output/Chapter8/b8H4.c&quot;) b8H4.c &lt;- add_criterion(b8H4.c, c(&quot;waic&quot;,&quot;loo&quot;)) posterior_summary(b8H4.c)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #hxzjyxcrfh .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #hxzjyxcrfh .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hxzjyxcrfh .gt_caption { padding-top: 4px; padding-bottom: 4px; } #hxzjyxcrfh .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #hxzjyxcrfh .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #hxzjyxcrfh .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hxzjyxcrfh .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hxzjyxcrfh .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #hxzjyxcrfh .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #hxzjyxcrfh .gt_column_spanner_outer:first-child { padding-left: 0; } #hxzjyxcrfh .gt_column_spanner_outer:last-child { padding-right: 0; } #hxzjyxcrfh .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #hxzjyxcrfh .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #hxzjyxcrfh .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #hxzjyxcrfh .gt_from_md > :first-child { margin-top: 0; } #hxzjyxcrfh .gt_from_md > :last-child { margin-bottom: 0; } #hxzjyxcrfh .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #hxzjyxcrfh .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #hxzjyxcrfh .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #hxzjyxcrfh .gt_row_group_first td { border-top-width: 2px; } #hxzjyxcrfh .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hxzjyxcrfh .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #hxzjyxcrfh .gt_first_summary_row.thick { border-top-width: 2px; } #hxzjyxcrfh .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hxzjyxcrfh .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hxzjyxcrfh .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #hxzjyxcrfh .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #hxzjyxcrfh .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hxzjyxcrfh .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hxzjyxcrfh .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #hxzjyxcrfh .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hxzjyxcrfh .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hxzjyxcrfh .gt_left { text-align: left; } #hxzjyxcrfh .gt_center { text-align: center; } #hxzjyxcrfh .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #hxzjyxcrfh .gt_font_normal { font-weight: normal; } #hxzjyxcrfh .gt_font_bold { font-weight: bold; } #hxzjyxcrfh .gt_font_italic { font-style: italic; } #hxzjyxcrfh .gt_super { font-size: 65%; } #hxzjyxcrfh .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #hxzjyxcrfh .gt_asterisk { font-size: 100%; vertical-align: 0; } #hxzjyxcrfh .gt_indent_1 { text-indent: 5px; } #hxzjyxcrfh .gt_indent_2 { text-indent: 10px; } #hxzjyxcrfh .gt_indent_3 { text-indent: 15px; } #hxzjyxcrfh .gt_indent_4 { text-indent: 20px; } #hxzjyxcrfh .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.46 0.16 -5.79 -5.14 -0.02 0.22 -0.45 0.40 0.54 0.18 0.18 0.90 -0.38 0.21 -0.80 0.02 1.40 0.12 1.19 1.66 -7.64 0.13 -7.92 -7.41 -137.26 1.66 -141.34 -135.08 続いて、交互作用ありのモデル。 交互作用が強く働いていそう。 b8H4.c_2 &lt;- brm(data = d5, family = gaussian, formula = L ~ 1 + A + meanG + sdG + meanG:sdG, prior =c(prior(normal(-5.5, 1.5), class = Intercept), prior(normal(0,2),class=b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =8, file = &quot;output/Chapter8/b8H4.c_2&quot;) b8H4.c_2 &lt;- add_criterion(b8H4.c_2, c(&quot;waic&quot;,&quot;loo&quot;)) posterior_summary(b8H4.c_2)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #oetvelaufc .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #oetvelaufc .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oetvelaufc .gt_caption { padding-top: 4px; padding-bottom: 4px; } #oetvelaufc .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #oetvelaufc .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #oetvelaufc .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oetvelaufc .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oetvelaufc .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #oetvelaufc .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #oetvelaufc .gt_column_spanner_outer:first-child { padding-left: 0; } #oetvelaufc .gt_column_spanner_outer:last-child { padding-right: 0; } #oetvelaufc .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #oetvelaufc .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #oetvelaufc .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #oetvelaufc .gt_from_md > :first-child { margin-top: 0; } #oetvelaufc .gt_from_md > :last-child { margin-bottom: 0; } #oetvelaufc .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #oetvelaufc .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #oetvelaufc .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #oetvelaufc .gt_row_group_first td { border-top-width: 2px; } #oetvelaufc .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oetvelaufc .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #oetvelaufc .gt_first_summary_row.thick { border-top-width: 2px; } #oetvelaufc .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oetvelaufc .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oetvelaufc .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #oetvelaufc .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #oetvelaufc .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oetvelaufc .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oetvelaufc .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #oetvelaufc .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oetvelaufc .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #oetvelaufc .gt_left { text-align: left; } #oetvelaufc .gt_center { text-align: center; } #oetvelaufc .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #oetvelaufc .gt_font_normal { font-weight: normal; } #oetvelaufc .gt_font_bold { font-weight: bold; } #oetvelaufc .gt_font_italic { font-style: italic; } #oetvelaufc .gt_super { font-size: 65%; } #oetvelaufc .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #oetvelaufc .gt_asterisk { font-size: 100%; vertical-align: 0; } #oetvelaufc .gt_indent_1 { text-indent: 5px; } #oetvelaufc .gt_indent_2 { text-indent: 10px; } #oetvelaufc .gt_indent_3 { text-indent: 15px; } #oetvelaufc .gt_indent_4 { text-indent: 20px; } #oetvelaufc .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.45 0.16 -5.76 -5.14 -0.02 0.21 -0.42 0.40 0.35 0.20 -0.04 0.75 -0.35 0.19 -0.73 0.04 -0.37 0.16 -0.68 -0.06 1.36 0.12 1.15 1.62 -9.21 0.13 -9.48 -8.98 -136.81 1.81 -141.27 -134.28 Aはほとんど効果がないので、おそらくmeanGとsdGでLへのルートをすべてブロックされている。Aを外したモデルも回してみる。 b8H4.c_3 &lt;- brm(data = d5, family = gaussian, formula = L ~ 1 + meanG + sdG + meanG:sdG, prior =c(prior(normal(-5.5, 1.5), class = Intercept), prior(normal(0,2),class=b), prior(exponential(1),class=sigma)), backend = &quot;cmdstanr&quot;, seed =8, file = &quot;output/Chapter8/b8H4.c_3&quot;) b8H4.c_3 &lt;- add_criterion(b8H4.c_3, c(&quot;waic&quot;, &quot;loo&quot;)) posterior_summary(b8H4.c_3)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #umuyhdvnha .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #umuyhdvnha .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #umuyhdvnha .gt_caption { padding-top: 4px; padding-bottom: 4px; } #umuyhdvnha .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #umuyhdvnha .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #umuyhdvnha .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #umuyhdvnha .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #umuyhdvnha .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #umuyhdvnha .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #umuyhdvnha .gt_column_spanner_outer:first-child { padding-left: 0; } #umuyhdvnha .gt_column_spanner_outer:last-child { padding-right: 0; } #umuyhdvnha .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #umuyhdvnha .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #umuyhdvnha .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #umuyhdvnha .gt_from_md > :first-child { margin-top: 0; } #umuyhdvnha .gt_from_md > :last-child { margin-bottom: 0; } #umuyhdvnha .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #umuyhdvnha .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #umuyhdvnha .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #umuyhdvnha .gt_row_group_first td { border-top-width: 2px; } #umuyhdvnha .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #umuyhdvnha .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #umuyhdvnha .gt_first_summary_row.thick { border-top-width: 2px; } #umuyhdvnha .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #umuyhdvnha .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #umuyhdvnha .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #umuyhdvnha .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #umuyhdvnha .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #umuyhdvnha .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #umuyhdvnha .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #umuyhdvnha .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #umuyhdvnha .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #umuyhdvnha .gt_left { text-align: left; } #umuyhdvnha .gt_center { text-align: center; } #umuyhdvnha .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #umuyhdvnha .gt_font_normal { font-weight: normal; } #umuyhdvnha .gt_font_bold { font-weight: bold; } #umuyhdvnha .gt_font_italic { font-style: italic; } #umuyhdvnha .gt_super { font-size: 65%; } #umuyhdvnha .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #umuyhdvnha .gt_asterisk { font-size: 100%; vertical-align: 0; } #umuyhdvnha .gt_indent_1 { text-indent: 5px; } #umuyhdvnha .gt_indent_2 { text-indent: 10px; } #umuyhdvnha .gt_indent_3 { text-indent: 15px; } #umuyhdvnha .gt_indent_4 { text-indent: 20px; } #umuyhdvnha .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -5.45 0.15 -5.75 -5.15 0.37 0.18 0.01 0.72 -0.37 0.16 -0.68 -0.05 -0.36 0.16 -0.68 -0.05 1.35 0.12 1.15 1.60 -7.58 0.12 -7.84 -7.37 -134.57 1.62 -138.58 -132.45 全てのモデルのPSISを比較してみる。 最後のmeanGとsdGおよびその交互作用だけのモデルが最もPSISが低い。 loo_compare(b8H4.c, b8H4.c_2, b8H4.c_3, criterion=&quot;loo&quot;) %&gt;% print(simplify=F) %&gt;% data.frame() -&gt; loo_c ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b8H4.c_3 0.0 0.0 -130.2 7.4 5.2 1.2 260.4 14.9 ## b8H4.c_2 -1.4 0.2 -131.6 7.5 6.4 1.5 263.1 15.1 ## b8H4.c -2.8 2.3 -133.0 7.4 5.5 1.4 266.0 14.8 bind_rows(loo_a, loo_b, loo_c) %&gt;% arrange(looic) %&gt;% dplyr::select(looic, se_looic) ## looic se_looic ## b8H4.c_3 260.3982 14.89968 ## b8H4.c_2 263.1379 15.07885 ## b8H4.c 266.0279 14.84678 ## b8H4.a_2 267.3554 14.64422 ## b8H4.a 267.8227 15.05103 ## b8H4.b_2 273.0191 15.92537 ## b8H4.b 273.1233 16.45595 プロットしてみる。 seq &lt;- crossing(meanG = seq(-2.5, 2, length.out = 50), sdG = c(quantile(d5$sdG,0.1)[[1]], quantile(d5$sdG,0.5)[[1]], quantile(d5$sdG,0.9)[[1]])) fitted(b8H4.c_3, newdata = seq) %&gt;% data.frame() %&gt;% bind_cols(seq) %&gt;% mutate(cat_sdG = ifelse(sdG &lt; -1, &quot;sdG 10% quantile&quot;, ifelse(sdG &gt; 1, &quot;sdG 90% quantile&quot;, &quot;sdG 50% quantile&quot;))) %&gt;% mutate(cat_sdG = as.factor(cat_sdG)) -&gt; fitted8H5.c ggplot(fitted8H5.c, aes(x=meanG))+ geom_ribbon(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), alpha = 1/4, fill = &quot;black&quot;)+ geom_line(aes(y = Estimate), alpha = 1/2, color = &quot;black&quot;)+ geom_point(data = d5, aes(y=L, color = sdG),size=2)+ labs(x=&quot;mean growing season(std)&quot;, y = &quot;log Language per capita&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ coord_cartesian(ylim =c(-10,0))+ facet_wrap(~cat_sdG)-&gt; p10 seq &lt;- crossing(sdG = seq(-2.5, 4, length.out = 50), meanG = c(quantile(d5$meanG,0.1)[[1]], quantile(d5$meanG,0.5)[[1]], quantile(d5$meanG,0.9)[[1]])) fitted(b8H4.c_3, newdata = seq) %&gt;% data.frame() %&gt;% bind_cols(seq) %&gt;% mutate(cat_meanG = ifelse(meanG &lt; -1,&quot;meanG 10% quantile&quot;, ifelse(meanG &gt; 1, &quot;meanG 90% quantile&quot;, &quot;meanG 50% quantile&quot;))) %&gt;% mutate(cat_meanG = as.factor(cat_meanG)) -&gt; fitted8H5.c2 ggplot(fitted8H5.c2, aes(x=sdG))+ geom_ribbon(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), alpha = 1/4, fill = &quot;black&quot;)+ geom_line(aes(y = Estimate), alpha = 1/2, color = &quot;black&quot;)+ geom_point(data = d5, aes(y=L, color = meanG),size=2)+ labs(x=&quot;sd growing season(std)&quot;, y = &quot;log Language per capita&quot;)+ theme(aspect.ratio=1, strip.background = element_blank(), strip.text = element_text(size=10))+ facet_wrap(~cat_meanG) -&gt; p11 meanGはsdGが小さいほど、sdGはmeanGが大きいほどLに与える影響が大きくなる。これは、教科書の予想通りである。 p10/p11 8.4.6 8H5 Consider the data(Wines2012) data table. These data are expert ratings of 20 different French and American wines by 9 different French and American judges. Your goal is to model score, the subjective rating assigned by each judge to each wine. I recommend standardizing it. In this problem, consider only variation among judges and wines. Construct index variables of judge and wine and then use these index variables to construct a linear regression model. Justify your priors. You should end up with 9 judge parameters and 20 wine parameters. How do you interpret the variation among individual judges and individual wines? Do you notice any patterns, just by plotting the differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/best on average? 9人の審査員(アメリカ人とフランス人)が20本のワインについて得点を付けている。 data(Wines2012) d6 &lt;- Wines2012 slice(d6, 1:20) ## judge flight wine score wine.amer judge.amer ## 1 Jean-M Cardebat white A1 10 1 0 ## 2 Jean-M Cardebat white B1 13 1 0 ## 3 Jean-M Cardebat white C1 14 0 0 ## 4 Jean-M Cardebat white D1 15 0 0 ## 5 Jean-M Cardebat white E1 8 1 0 ## 6 Jean-M Cardebat white F1 13 1 0 ## 7 Jean-M Cardebat white G1 15 1 0 ## 8 Jean-M Cardebat white H1 11 0 0 ## 9 Jean-M Cardebat white I1 9 1 0 ## 10 Jean-M Cardebat white J1 12 0 0 ## 11 Tyler Colman white A1 16 1 1 ## 12 Tyler Colman white B1 14 1 1 ## 13 Tyler Colman white C1 14 0 1 ## 14 Tyler Colman white D1 16 0 1 ## 15 Tyler Colman white E1 12 1 1 ## 16 Tyler Colman white F1 11 1 1 ## 17 Tyler Colman white G1 11 1 1 ## 18 Tyler Colman white H1 14 0 1 ## 19 Tyler Colman white I1 11 1 1 ## 20 Tyler Colman white J1 14 0 1 まずは、審査員のID(judgeID)とワインのID(wine)が得点(S)に与える影響を考える。 d6 %&gt;% mutate(judgeID = str_sub(judge, 1,3)) %&gt;% mutate(S = standardize(score)) -&gt; d6 ## judgeのID d6 %&gt;% ggplot(aes(x=judgeID, y = S))+ geom_boxplot()+ theme(aspect.ratio = .5) ## wineのID d6 %&gt;% ggplot(aes(x=wine, y = S))+ geom_boxplot()+ theme(aspect.ratio = .5) モデリングする。Sは標準化しているので、事前分布は標準正規分布とする。 b8H5 &lt;- brm( data = d6, family = gaussian, formula = bf(S ~ 0 +a + b, a ~ 0 + judgeID, b ~ 0 + wine, nl = TRUE), prior = c(prior(normal(0,1), nlpar = &quot;a&quot;), prior(normal(0,1), nlpar = &quot;b&quot;), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed = 8, file = &quot;output/Chapter8/b8H5&quot; ) posterior_summary(b8H5)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ptmkgovred .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ptmkgovred .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ptmkgovred .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ptmkgovred .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ptmkgovred .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ptmkgovred .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ptmkgovred .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ptmkgovred .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ptmkgovred .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ptmkgovred .gt_column_spanner_outer:first-child { padding-left: 0; } #ptmkgovred .gt_column_spanner_outer:last-child { padding-right: 0; } #ptmkgovred .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ptmkgovred .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ptmkgovred .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ptmkgovred .gt_from_md > :first-child { margin-top: 0; } #ptmkgovred .gt_from_md > :last-child { margin-bottom: 0; } #ptmkgovred .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ptmkgovred .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ptmkgovred .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ptmkgovred .gt_row_group_first td { border-top-width: 2px; } #ptmkgovred .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ptmkgovred .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ptmkgovred .gt_first_summary_row.thick { border-top-width: 2px; } #ptmkgovred .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ptmkgovred .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ptmkgovred .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ptmkgovred .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ptmkgovred .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ptmkgovred .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ptmkgovred .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #ptmkgovred .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ptmkgovred .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ptmkgovred .gt_left { text-align: left; } #ptmkgovred .gt_center { text-align: center; } #ptmkgovred .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ptmkgovred .gt_font_normal { font-weight: normal; } #ptmkgovred .gt_font_bold { font-weight: bold; } #ptmkgovred .gt_font_italic { font-style: italic; } #ptmkgovred .gt_super { font-size: 65%; } #ptmkgovred .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #ptmkgovred .gt_asterisk { font-size: 100%; vertical-align: 0; } #ptmkgovred .gt_indent_1 { text-indent: 5px; } #ptmkgovred .gt_indent_2 { text-indent: 10px; } #ptmkgovred .gt_indent_3 { text-indent: 15px; } #ptmkgovred .gt_indent_4 { text-indent: 20px; } #ptmkgovred .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -0.30 0.25 -0.79 0.20 0.24 0.26 -0.27 0.76 0.23 0.26 -0.27 0.73 -0.59 0.26 -1.09 -0.08 0.88 0.26 0.39 1.40 0.53 0.26 0.03 1.04 0.15 0.26 -0.35 0.66 -0.72 0.26 -1.23 -0.21 -0.37 0.25 -0.86 0.12 0.14 0.33 -0.50 0.78 0.09 0.32 -0.54 0.74 0.27 0.32 -0.37 0.87 0.56 0.32 -0.08 1.18 -0.14 0.32 -0.78 0.49 -0.38 0.32 -1.02 0.25 0.29 0.32 -0.35 0.90 0.27 0.33 -0.36 0.92 0.08 0.33 -0.56 0.72 0.12 0.33 -0.52 0.74 -0.02 0.32 -0.65 0.61 -0.04 0.32 -0.68 0.58 -0.11 0.33 -0.75 0.51 -0.01 0.33 -0.64 0.64 -0.23 0.33 -0.87 0.40 -0.21 0.32 -0.86 0.41 -0.15 0.32 -0.79 0.48 -0.88 0.33 -1.54 -0.25 -0.18 0.32 -0.80 0.44 0.39 0.33 -0.25 1.05 0.85 0.05 0.77 0.95 -30.92 0.89 -33.15 -29.66 -257.20 4.17 -266.32 -250.18 結果を図示してみる。 conditional_effects(b8H5, method = &quot;fitted&quot;) %&gt;% plot() 8.4.7 8H6 Now consider three features of the wines and judges: flight: Whether the wine is red or white. wine.amer: Indicator variable for American wines. judge.amer: Indicator variable for American judges. Use indicator or index variables to model the influence of these features on the scores. Omit the individual judge and wine index variables from Problem 1. Do not include interaction effects yet. Again justify your priors What do you conclude about the differences among the wines and judges? Try to relate the results to the inferences in the previous problem. 続いて、ワインの種類(flight)、ワインがアメリカ産か否か(wine.amer)、審査員がアメリカ人か否か(judge.amer)が得点(S)に与える影響を考える。 グラフを書いても、今一つ関係がはっきりしない。 d6 %&gt;% ggplot(aes(x=flight, y = score,color = as.factor(wine.amer)))+ geom_boxplot() d6 %&gt;% ggplot(aes(x=flight, y = score,color = as.factor(judge.amer)))+ geom_boxplot() d6 %&gt;% ggplot(aes(x=as.factor(judge.amer), y = score,color = as.factor(wine.amer)))+ geom_boxplot() d6 %&gt;% ggplot(aes(x=as.factor(wine.amer), y = score,color = as.factor(judge.amer)))+ geom_boxplot() モデリングしてみる。 まずは交互作用を考えない。 d6 %&gt;% mutate(WA = as.factor(wine.amer+1), JA = as.factor(judge.amer+1)) -&gt; d6 b8H6 &lt;- brm( data = d6, family = gaussian, formula = bf(S ~ 0 + a + b + c, a ~ 0 + JA, b ~ 0 + WA, c ~ 0 + flight, nl = TRUE), prior = c(prior(normal(0,1), nlpar = &quot;a&quot;), prior(normal(0,1), nlpar = &quot;b&quot;), prior(normal(0,1), nlpar = &quot;c&quot;), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed = 8, file = &quot;output/Chapter8/b8H6&quot; ) b8H6 &lt;- add_criterion(b8H6, c(&quot;waic&quot;,&quot;loo&quot;)) いずれの要因もほとんど影響がない。 posterior_summary(b8H6)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #akyjyzmyuu .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #akyjyzmyuu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #akyjyzmyuu .gt_caption { padding-top: 4px; padding-bottom: 4px; } #akyjyzmyuu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #akyjyzmyuu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #akyjyzmyuu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #akyjyzmyuu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #akyjyzmyuu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #akyjyzmyuu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #akyjyzmyuu .gt_column_spanner_outer:first-child { padding-left: 0; } #akyjyzmyuu .gt_column_spanner_outer:last-child { padding-right: 0; } #akyjyzmyuu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #akyjyzmyuu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #akyjyzmyuu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #akyjyzmyuu .gt_from_md > :first-child { margin-top: 0; } #akyjyzmyuu .gt_from_md > :last-child { margin-bottom: 0; } #akyjyzmyuu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #akyjyzmyuu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #akyjyzmyuu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #akyjyzmyuu .gt_row_group_first td { border-top-width: 2px; } #akyjyzmyuu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #akyjyzmyuu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #akyjyzmyuu .gt_first_summary_row.thick { border-top-width: 2px; } #akyjyzmyuu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #akyjyzmyuu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #akyjyzmyuu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #akyjyzmyuu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #akyjyzmyuu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #akyjyzmyuu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #akyjyzmyuu .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #akyjyzmyuu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #akyjyzmyuu .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #akyjyzmyuu .gt_left { text-align: left; } #akyjyzmyuu .gt_center { text-align: center; } #akyjyzmyuu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #akyjyzmyuu .gt_font_normal { font-weight: normal; } #akyjyzmyuu .gt_font_bold { font-weight: bold; } #akyjyzmyuu .gt_font_italic { font-style: italic; } #akyjyzmyuu .gt_super { font-size: 65%; } #akyjyzmyuu .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #akyjyzmyuu .gt_asterisk { font-size: 100%; vertical-align: 0; } #akyjyzmyuu .gt_indent_1 { text-indent: 5px; } #akyjyzmyuu .gt_indent_2 { text-indent: 10px; } #akyjyzmyuu .gt_indent_3 { text-indent: 15px; } #akyjyzmyuu .gt_indent_4 { text-indent: 20px; } #akyjyzmyuu .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -0.12 0.59 -1.24 1.02 0.12 0.59 -1.00 1.28 0.11 0.56 -1.01 1.17 -0.08 0.56 -1.17 1.01 -0.01 0.58 -1.15 1.10 -0.01 0.58 -1.13 1.10 1.00 0.05 0.90 1.11 -7.54 0.96 -10.07 -6.55 -262.79 1.89 -267.29 -260.11 8.4.8 8H7 Now consider two-way interactions among the three features. You should end up with three different interaction terms in your model. These will be easier to build, if you use indicator variables. Again justify your priors. Explain what each interaction means. Be sure to interpret the model’s predictions on the outcome scale (mu, the expected score), not on the scale of individual parameters. You can use link to help with this, or just use your knowledge of the linear model instead. What do you conclude about the features and the scores? Can you relate the results of your model(s) to the individual judge and wine inferences from 8H5? b8H7 &lt;- brm( data = d6, family = gaussian, formula = bf(S ~ 0 + a+b+c+d+e+f , a ~ 0 + JA, b ~ 0 + WA, c ~ 0 + flight, d ~ 0 + JA:WA, e ~ 0 + WA:flight, f ~ 0 + flight:JA, nl = TRUE), prior = c(prior(normal(0,1), nlpar = &quot;a&quot;), prior(normal(0,1), nlpar = &quot;b&quot;), prior(normal(0,1), nlpar = &quot;c&quot;), prior(normal(0,1), nlpar = &quot;d&quot;), prior(normal(0,1), nlpar = &quot;e&quot;), prior(normal(0,1), nlpar = &quot;f&quot;), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed = 8, file = &quot;output/Chapter8/b8H7&quot; ) b8H7 &lt;- add_criterion(b8H7, c(&quot;waic&quot;,&quot;loo&quot;)) あまり大きな違いはない。 posterior_summary(b8H7)%&gt;% data.frame() %&gt;% round(2) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ahkxgjvbsv .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ahkxgjvbsv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ahkxgjvbsv .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ahkxgjvbsv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ahkxgjvbsv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ahkxgjvbsv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ahkxgjvbsv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ahkxgjvbsv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ahkxgjvbsv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ahkxgjvbsv .gt_column_spanner_outer:first-child { padding-left: 0; } #ahkxgjvbsv .gt_column_spanner_outer:last-child { padding-right: 0; } #ahkxgjvbsv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ahkxgjvbsv .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ahkxgjvbsv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ahkxgjvbsv .gt_from_md > :first-child { margin-top: 0; } #ahkxgjvbsv .gt_from_md > :last-child { margin-bottom: 0; } #ahkxgjvbsv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ahkxgjvbsv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ahkxgjvbsv .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ahkxgjvbsv .gt_row_group_first td { border-top-width: 2px; } #ahkxgjvbsv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ahkxgjvbsv .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ahkxgjvbsv .gt_first_summary_row.thick { border-top-width: 2px; } #ahkxgjvbsv .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ahkxgjvbsv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ahkxgjvbsv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ahkxgjvbsv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ahkxgjvbsv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ahkxgjvbsv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ahkxgjvbsv .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #ahkxgjvbsv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ahkxgjvbsv .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ahkxgjvbsv .gt_left { text-align: left; } #ahkxgjvbsv .gt_center { text-align: center; } #ahkxgjvbsv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ahkxgjvbsv .gt_font_normal { font-weight: normal; } #ahkxgjvbsv .gt_font_bold { font-weight: bold; } #ahkxgjvbsv .gt_font_italic { font-style: italic; } #ahkxgjvbsv .gt_super { font-size: 65%; } #ahkxgjvbsv .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #ahkxgjvbsv .gt_asterisk { font-size: 100%; vertical-align: 0; } #ahkxgjvbsv .gt_indent_1 { text-indent: 5px; } #ahkxgjvbsv .gt_indent_2 { text-indent: 10px; } #ahkxgjvbsv .gt_indent_3 { text-indent: 15px; } #ahkxgjvbsv .gt_indent_4 { text-indent: 20px; } #ahkxgjvbsv .gt_indent_5 { text-indent: 25px; } Estimate Est.Error Q2.5 Q97.5 -0.05 0.81 -1.62 1.55 0.04 0.79 -1.53 1.58 0.04 0.80 -1.51 1.59 -0.02 0.81 -1.61 1.59 0.01 0.83 -1.62 1.61 0.01 0.79 -1.54 1.58 -0.03 0.77 -1.56 1.48 0.11 0.77 -1.39 1.61 -0.06 0.77 -1.59 1.41 -0.03 0.79 -1.57 1.54 0.17 0.78 -1.35 1.75 -0.15 0.78 -1.64 1.39 -0.13 0.78 -1.65 1.40 0.13 0.79 -1.40 1.72 -0.04 0.79 -1.52 1.48 -0.06 0.76 -1.56 1.50 0.06 0.76 -1.45 1.53 0.00 0.78 -1.55 1.48 1.00 0.05 0.90 1.11 -23.17 2.32 -28.65 -19.52 -277.90 3.12 -284.92 -272.82 conditional_effects(b8H7) %&gt;% plot() 交互作用があるモデルとないモデルに違いはあまりない。 むしろ、交互作用がないモデルの方がPSISは低く、model weightも低い。 loo_compare(b8H7, b8H6) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b8H6 0.0 0.0 -257.7 8.6 4.8 0.5 515.4 17.2 ## b8H7 -1.0 2.0 -258.7 8.6 7.5 0.7 517.3 17.3 model_weights(b8H6, b8H7) %&gt;% round(2) ## b8H6 b8H7 ## 0.73 0.27 "],["marcov-chain-monte-carlo.html", "9 Marcov Chain Monte Carlo 9.1 Good King Marcov and his island kingdom 9.2 Metropolis algorythm 9.3 Hamiltonian Monte Carlo 9.4 Easy HMC: ulam brm() 9.5 Care and feeding of your Marcov chain 9.6 Practice", " 9 Marcov Chain Monte Carlo 9.1 Good King Marcov and his island kingdom 10の島からなる諸島がある。それぞれの島は2つの島に隣接しており、全体で円になっている。各島は面積が異なり、それに比例して人口も異なる。面積と人口は1つめの島から順に2倍、3倍、…10倍になっている(つまり、1つめの島の大きさと人口が1だとすれば、10個目の島はそれぞれ10である)。 さて、この諸島の王様は1週間ごとに島々を訪れるが、その際には隣接している島にしか移動できない。王様は各島を人口比率に応じて訪れたいが、訪問計画を長期的に策定するのは面倒である。そこで、彼の側近は以下の方法で島を訪れることを提案した。この方法に従えば、各島に訪れる頻度が人口比率に一致する。ここではこの方法をMetropolis algorithmと呼ぶ。 毎週王様はその島にとどまるか、隣接するいずれかの島に移動するかをコインを投げて決める。 もしコインが表なら、王様は時計回りに隣の島に移動することを考える。一方コインが裏なら、反時計回りに移動することを考える。ここで、提案された島をproposal islandとする。 王様はporposal islandの大きさだけ(7つめの島にいるなら7個)貝殻を集める。また、現在いる島の大きさだけ同様に石を集める。 もし貝殻の数が石よりも多ければ、王様はproposal islandへ移動する。一方で石の数の方が多い場合、王様は集めた石から貝殻と同数の石を捨てる(例えば石が6つ、貝殻が4つなら、手元には\\(6-4=2\\)個の石が残る)。その後、残された石と貝殻をカバンに入れ、王様はランダムにそのうちの一つを引く。もしそれが貝殻ならばproposal islandに移動し、石ならば今いる島に留まる。 この方法は一見奇妙だが、長期間繰り返していくと非常にうまくいく。以下でシミュレーションしてみよう。 set.seed(9) num_weeks &lt;- 1e6 positions &lt;- rep(0, num_weeks) current &lt;- 10 ## アルゴリズムの記述 for(i in 1:num_weeks){ positions[i] &lt;- current proposal &lt;- current + sample(c(-1,1), size=1) if(proposal &lt;1) proposal &lt;- 10 if(proposal &gt;10) proposal &lt;- 1 prob_move &lt;- proposal/current current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } 国王の動きを可視化してみる。 tibble(week = 1:1e6, island = positions) %&gt;% ggplot(aes(x=week, y = island))+ geom_line(size=1/3)+ coord_cartesian(xlim = c(0,500))+ labs(title = &quot;Behold the Metropolis algorithm in action!&quot;, subtitle = &quot;The dots show the king&#39;s path over the first 500 weeks.&quot;) 各島を訪れた回数を見てみると以下のようになり、人口に応じて訪れていることが分かる。 tibble(week = 1:1e6, island = positions) %&gt;% mutate(island = factor(island)) %&gt;% ggplot(aes(x=island))+ geom_bar()+ labs(title = &quot;Old Metropolis shines in the long run.&quot;, subtitle = &quot;Sure enough, the time the king spent on each island\\nwas proportional to its population size.&quot;) 島を訪れている比率はおよそ人口通りになる。このアルゴリズムは、隣の島だけでなく全ての島への移動が可能であっても同様に機能する。 tibble(week = 1:1e6, island = positions) %&gt;% count(island) %&gt;% mutate(prop = n/n[1]) %&gt;% gt() %&gt;% fmt_number(&quot;prop&quot;,decimals=2) %&gt;% tab_options(table.align=&#39;left&#39;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #wqlbootwvl .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wqlbootwvl .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wqlbootwvl .gt_caption { padding-top: 4px; padding-bottom: 4px; } #wqlbootwvl .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wqlbootwvl .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #wqlbootwvl .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wqlbootwvl .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wqlbootwvl .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wqlbootwvl .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wqlbootwvl .gt_column_spanner_outer:first-child { padding-left: 0; } #wqlbootwvl .gt_column_spanner_outer:last-child { padding-right: 0; } #wqlbootwvl .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #wqlbootwvl .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #wqlbootwvl .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wqlbootwvl .gt_from_md > :first-child { margin-top: 0; } #wqlbootwvl .gt_from_md > :last-child { margin-bottom: 0; } #wqlbootwvl .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wqlbootwvl .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #wqlbootwvl .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #wqlbootwvl .gt_row_group_first td { border-top-width: 2px; } #wqlbootwvl .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wqlbootwvl .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #wqlbootwvl .gt_first_summary_row.thick { border-top-width: 2px; } #wqlbootwvl .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wqlbootwvl .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wqlbootwvl .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wqlbootwvl .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wqlbootwvl .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wqlbootwvl .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wqlbootwvl .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #wqlbootwvl .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wqlbootwvl .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wqlbootwvl .gt_left { text-align: left; } #wqlbootwvl .gt_center { text-align: center; } #wqlbootwvl .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wqlbootwvl .gt_font_normal { font-weight: normal; } #wqlbootwvl .gt_font_bold { font-weight: bold; } #wqlbootwvl .gt_font_italic { font-style: italic; } #wqlbootwvl .gt_super { font-size: 65%; } #wqlbootwvl .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #wqlbootwvl .gt_asterisk { font-size: 100%; vertical-align: 0; } #wqlbootwvl .gt_indent_1 { text-indent: 5px; } #wqlbootwvl .gt_indent_2 { text-indent: 10px; } #wqlbootwvl .gt_indent_3 { text-indent: 15px; } #wqlbootwvl .gt_indent_4 { text-indent: 20px; } #wqlbootwvl .gt_indent_5 { text-indent: 25px; } island n prop 1 18142 1.00 2 36232 2.00 3 54787 3.02 4 72686 4.01 5 90272 4.98 6 108747 5.99 7 127527 7.03 8 145756 8.03 9 163703 9.02 10 182148 10.04 9.2 Metropolis algorythm 国王が用いたアルゴリズムはMetropolis algorythmの特殊例であり、このアルゴリズムはマルコフ連鎖モンテカルロ法の一つである。この方法は未知の分布からサンプルを収集するために用いられる。本章では、Metropolis algorythmの改良版であるGibbs samplingについて概説する。 9.2.1 Gibbs sampling 上記のアルゴリズムでは次の島への移動の提案はランダムに(コインによって)行われていたが、より効率よくサンプリングをするためには事後確率に応じて提案が行われる必要がある。そのようなアルゴリズムの一つがGibbs samplingである。 Gibbs samplingはconjugate pairsと呼ばれる特定の事前分布と尤度の組み合わせを用いる方法で、JAGSやBUGSで実装されている。 9.2.2 High-dimentional problem Gibbs samplingはconjugate pairsが複雑なモデルになると使えなかったり、非効率になったりするという問題点がある。例えば、パラメータ数が多いモデルではパラメータ間に相関がみられることが多く、そのような場合に上手くサンプリングが行えない。 また、concentration of measureという問題によって、パラメータ数が多くなるほどサンプリングは困難になる。まず、あるパラメータの組み合わせの最頻値からの距離を考える。このとき、パラメータ数が多くなるほど、最頻値から離れた距離にあるパラメータの組み合わせが最も多くなってしまう。そのため、パラメータ数が多いほど、最頻値からサンプリングされる確率が低くなってしまう。 以下がシミュレーションである。パラメータ数が増えるにつれて、最頻値(この場合は全てのパラメータが0のとき)から離れたところにあるものが一番多くなることが分かる。 ## パラメータ数に応じて各点の距離を計算する関数 concentration_sim &lt;- function(d = 1, t = 1e3, seed = 9) { set.seed(seed) y &lt;- rethinking::rmvnorm(t, rep(0, d), diag(d)) rad_dist &lt;- function(y) sqrt(sum(y^2)) rd &lt;- sapply(1:t, function(i) rad_dist( y[i, ])) } ## パラメータ数が1, 10, 100, 1000のとき d &lt;- tibble(d = c(1, 10, 100, 1000)) %&gt;% mutate(con = purrr::map(d, concentration_sim)) %&gt;% unnest(con) %&gt;% mutate(`# dimensions` = factor(d)) d %&gt;% ggplot(aes(x = con, fill = `# dimensions`)) + geom_density(size = 0, alpha = 3/4)+ xlab(&quot;Radial distance from mode&quot;) + theme(legend.position = c(.7, .625))+ theme_classic() 9.3 Hamiltonian Monte Carlo Hamiltonian Monte Carlo(HMC)法はGibbs samplingなどよりも効率的にサンプリングができるアルゴリズムで、パラメータ数が多くなっても他のアルゴリズムに比べてうまく推定ができる。 9.3.1 Another probable ここで、またある国を考える。この国の領土は南北に伸びる狭い谷に広がっており、国民は標高と反比例して分布している(高い場所ほど人口が少ない)。さて、この国の国王も国民の人口比率に応じて国中を訪れなければいけないとする。彼の側近のHamiltonは以下のアルゴリズムに従えばうまくいくことを提案した。 国王の車はまずランダムな方向にランダムな運動量で動き出す。車は坂道を上がると速度を落とし、やがて運動量の減少とともに坂道を下ってまた加速していく。 一定時間後に車を止めて国民にあいさつをする。 (1)と(2)を繰り返す。 この規則に従うとき、長期的に見れば車がある場所を訪れる頻度は標高に反比例する。この方法はサンプル内の自己相関が低くなるので、分布の幅広い範囲を探索することができる。この方法は諸島では採用できない。なぜならこの方法は国土が連続的であること(=パラメータが連続変数であること)を前提としているからである。 2つのパラメータで簡単なシミュレーションを行う。ここでは以下の統計モデルを用いる。 \\[ \\begin{aligned} x_{i} &amp;\\sim Normal(\\mu_{x},1) \\\\ y_{i} &amp;\\sim Normal(\\mu_{y},1) \\\\ \\mu_{x} &amp;\\sim Normal(0,0.5) \\\\ \\mu_{y} &amp;\\sim Normal(0,0.5) \\end{aligned} \\] HMCでは、2つの関数と2つの設定によって動く。まず一つ目は対数事後確率であり、以下のようになる(ベイズの定理の分子)。これは、上記の例でいうと標高を表す。 \\[ U = \\sum_{i} logp(y_{i}|\\mu_{y},1) + \\sum_{i} logp(x_{i}|\\mu_{x},1) + logp(\\mu_{y}|0.0.5) + logp(\\mu_{x}|0.0.5) \\] 続いて、もう一つ必要な関数は傾きであり、上記の式を\\(\\mu_{x}\\)と\\(\\mu_{y}\\)について偏微分したものに相当する。\\(\\frac{\\partial logN(y|a,b)}{\\partial a} = \\frac{y-a}{b^2}\\)なので、 \\[ \\begin{aligned} \\frac{\\partial U}{\\partial \\mu_{x}} &amp;= \\frac{\\partial logN(x|\\mu_{x},1)}{\\partial \\mu_{x}} + \\frac{\\partial logN(\\mu_{x}|0,0.5)}{\\partial \\mu_{x}} = \\sum_{i} \\frac {x_{i} - \\mu_{x}}{1^2} + \\frac {0-\\mu_{x}}{0.5^2} \\\\ \\frac{\\partial U}{\\partial \\mu_{y}} &amp;= \\frac{\\partial logN(y|\\mu_{y},1)}{\\partial \\mu_{y}} + \\frac{\\partial logN(\\mu_{y}|0,0.5)}{\\partial \\mu_{y}} = \\sum_{i} \\frac {y_{i} - \\mu_{y}}{1^2} + \\frac {0-\\mu_{y}}{0.5^2} \\end{aligned} \\] 必要な2つの設定は、step sizeとleapfrog stepsの数(何ステップに1回サンプリングするか)である。効率的にサンプリングするためには、これらを適切に設定することが必要であるが、通常はソフトウェアが設定してくれる。stanでは、warmup期間中にこのチューニングが行われる。また、NUTSというアルゴリズムによって、運動ベクトルが大きく変わった時にサンプリングを行うことで、同じ場所からばかりサンプリングが行われることを防いでいる。 9.3.2 Limitations 上述のようにHMCは離散的なパラメータを扱えない。ただし、うまくコードを変えればそれらも扱うことができる。また、事後分布がうまくサンプルできないこともしばしばある(devergent transitionなど)。 9.4 Easy HMC: ulam brm() Chapter8の地形とGDPに関するデータについて再び検討する。 ## データ読み込み data(rugged) d &lt;- rugged ## 標準化(GDPは対数をとる) d %&gt;% dplyr::select(country,cont_africa,rugged,rgdppc_2000) %&gt;% filter(complete.cases(rgdppc_2000)) %&gt;% mutate(G = log(rgdppc_2000)/mean(log(rgdppc_2000), na.rm=T), R = rugged/max(rugged), A = ifelse(cont_africa==&quot;1&quot;,&quot;1&quot;,&quot;2&quot;)) %&gt;% mutate(R_c = R - mean(R))-&gt; d2 9.4.1 Preparation brmsでモデリングをするのであれば、教科書のようにデータをlist化する必要はない。 9.4.2 Sampling from posterior それでは、brmsでモデリングを行う。 b9.1 &lt;- brm(data = d2, family = gaussian, formula = bf(G ~ 0 + a + b*R_c, a ~ 0 + A, b ~ 0 + A, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, nlpar = a), prior(normal(0, 0.3), class = b, nlpar = b), prior(exponential(1), class=sigma)), seed = 9, chain =1, cores=1, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9.1&quot;) #posterior_summary(b9.1) %&gt;% #data.frame() %&gt;% #rownames_to_column(var =&quot;parameters&quot;) %&gt;% #filter(parameters != &quot;lp__&quot;) %&gt;% #gt() posterior_samples(b9.1) %&gt;% pivot_longer(-lp__, names_to = &quot;parameters&quot;) %&gt;% group_by(parameters) %&gt;% mean_hdi(value, .width = .89) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lbootwvlbx .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lbootwvlbx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lbootwvlbx .gt_caption { padding-top: 4px; padding-bottom: 4px; } #lbootwvlbx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lbootwvlbx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #lbootwvlbx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lbootwvlbx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lbootwvlbx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lbootwvlbx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lbootwvlbx .gt_column_spanner_outer:first-child { padding-left: 0; } #lbootwvlbx .gt_column_spanner_outer:last-child { padding-right: 0; } #lbootwvlbx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #lbootwvlbx .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #lbootwvlbx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lbootwvlbx .gt_from_md > :first-child { margin-top: 0; } #lbootwvlbx .gt_from_md > :last-child { margin-bottom: 0; } #lbootwvlbx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lbootwvlbx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #lbootwvlbx .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #lbootwvlbx .gt_row_group_first td { border-top-width: 2px; } #lbootwvlbx .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lbootwvlbx .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #lbootwvlbx .gt_first_summary_row.thick { border-top-width: 2px; } #lbootwvlbx .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lbootwvlbx .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lbootwvlbx .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lbootwvlbx .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lbootwvlbx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lbootwvlbx .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lbootwvlbx .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #lbootwvlbx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lbootwvlbx .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lbootwvlbx .gt_left { text-align: left; } #lbootwvlbx .gt_center { text-align: center; } #lbootwvlbx .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lbootwvlbx .gt_font_normal { font-weight: normal; } #lbootwvlbx .gt_font_bold { font-weight: bold; } #lbootwvlbx .gt_font_italic { font-style: italic; } #lbootwvlbx .gt_super { font-size: 65%; } #lbootwvlbx .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #lbootwvlbx .gt_asterisk { font-size: 100%; vertical-align: 0; } #lbootwvlbx .gt_indent_1 { text-indent: 5px; } #lbootwvlbx .gt_indent_2 { text-indent: 10px; } #lbootwvlbx .gt_indent_3 { text-indent: 15px; } #lbootwvlbx .gt_indent_4 { text-indent: 20px; } #lbootwvlbx .gt_indent_5 { text-indent: 25px; } parameters value .lower .upper .width .point .interval b_a_A1 0.8864752 0.860290836 0.90806460 0.89 mean hdi b_a_A2 1.0504113 1.033940994 1.06580555 0.89 mean hdi b_b_A1 0.1260194 0.005795189 0.25055026 0.89 mean hdi b_b_A2 -0.1446658 -0.231614404 -0.04967538 0.89 mean hdi lprior 2.1821531 1.804216122 2.54075927 0.89 mean hdi sigma 0.1116524 0.101538272 0.12156657 0.89 mean hdi #rhat(b9.1) 9.4.3 Sampling again in parallel 次は、4つのchainで回してみる。 b9.1b &lt;- update(b9.1, chains = 4, cores=4, seed=9, file = &quot;output/Chapter9/b9.1b&quot;) posterior_samples(b9.1b) %&gt;% pivot_longer(-lp__, names_to = &quot;parameters&quot;) %&gt;% group_by(parameters) %&gt;% mean_hdi(value, .width = .89) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kcxtluoelb .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kcxtluoelb .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kcxtluoelb .gt_caption { padding-top: 4px; padding-bottom: 4px; } #kcxtluoelb .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kcxtluoelb .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #kcxtluoelb .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kcxtluoelb .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kcxtluoelb .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kcxtluoelb .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kcxtluoelb .gt_column_spanner_outer:first-child { padding-left: 0; } #kcxtluoelb .gt_column_spanner_outer:last-child { padding-right: 0; } #kcxtluoelb .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #kcxtluoelb .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #kcxtluoelb .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kcxtluoelb .gt_from_md > :first-child { margin-top: 0; } #kcxtluoelb .gt_from_md > :last-child { margin-bottom: 0; } #kcxtluoelb .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kcxtluoelb .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #kcxtluoelb .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #kcxtluoelb .gt_row_group_first td { border-top-width: 2px; } #kcxtluoelb .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kcxtluoelb .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #kcxtluoelb .gt_first_summary_row.thick { border-top-width: 2px; } #kcxtluoelb .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kcxtluoelb .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kcxtluoelb .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kcxtluoelb .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kcxtluoelb .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kcxtluoelb .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kcxtluoelb .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #kcxtluoelb .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kcxtluoelb .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #kcxtluoelb .gt_left { text-align: left; } #kcxtluoelb .gt_center { text-align: center; } #kcxtluoelb .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kcxtluoelb .gt_font_normal { font-weight: normal; } #kcxtluoelb .gt_font_bold { font-weight: bold; } #kcxtluoelb .gt_font_italic { font-style: italic; } #kcxtluoelb .gt_super { font-size: 65%; } #kcxtluoelb .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #kcxtluoelb .gt_asterisk { font-size: 100%; vertical-align: 0; } #kcxtluoelb .gt_indent_1 { text-indent: 5px; } #kcxtluoelb .gt_indent_2 { text-indent: 10px; } #kcxtluoelb .gt_indent_3 { text-indent: 15px; } #kcxtluoelb .gt_indent_4 { text-indent: 20px; } #kcxtluoelb .gt_indent_5 { text-indent: 25px; } parameters value .lower .upper .width .point .interval b_a_A1 0.8866523 0.86013454 0.91021227 0.89 mean hdi b_a_A2 1.0507625 1.03399223 1.06591912 0.89 mean hdi b_b_A1 0.1321000 0.01681887 0.25173934 0.89 mean hdi b_b_A2 -0.1425208 -0.23471392 -0.05644302 0.89 mean hdi lprior 2.1793178 1.80629736 2.54256135 0.89 mean hdi sigma 0.1116139 0.10205780 0.12163415 0.89 mean hdi 全ての結果 print(b9.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 0.89 0.02 0.86 0.92 1.00 4788 3163 ## a_A2 1.05 0.01 1.03 1.07 1.00 4423 3086 ## b_A1 0.13 0.07 -0.02 0.28 1.00 4279 2983 ## b_A2 -0.14 0.06 -0.25 -0.03 1.00 4892 2828 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 4850 2996 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). #以下でも同じ出力 #summary(b9.1b) #show(b9.1) モデル式 b9.1b$formula ## G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A 事前分布 b9.1b$prior %&gt;% print() ## prior class coef group resp dpar nlpar lb ub source ## normal(1, 0.1) b a user ## normal(1, 0.1) b A1 a (vectorized) ## normal(1, 0.1) b A2 a (vectorized) ## normal(0, 0.3) b b user ## normal(0, 0.3) b A1 b (vectorized) ## normal(0, 0.3) b A2 b (vectorized) ## exponential(1) sigma 0 user #以下でも同じ #prior_summary(b9.1b) 9.4.4 Visualization 各パラメータの事後分布と変数間の相関はpairs()とvcoc()で確認できる。ただし、\\(\\sigma\\)は入らない。 pairs(b9.1b, off_diag_args = list(size = 1/5, alpha = 1/5)) #vcov(b9.1b, correlation=T) %&gt;% # round(2) GGallyパッケージの`ggpairs()’を使えば、\\(\\sigma\\)も入れて可視化可能。 library(GGally) posterior_samples(b9.1b) %&gt;% dplyr::select(-lp__) %&gt;% ggpairs() 9.4.5 Checking the chain 収束を診断する際には、まずトレースプロットを確認する。 bayesplotパッケージが便利である。 ## brms plot(b9.1b) ## bayesplot library(bayesplot) post &lt;- posterior_samples(b9.1b, add_chain =TRUE) #chainの情報を残す mcmc_trace(post[,c(1:5,7)], facet_args = list(ncol=3), size=.15) 自己相関を確認する。 post %&gt;% mcmc_acf(pars = vars(1:5), lags=5) warmup期間のサンプルを含む場合には、ggmcmcパッケージを用いるとよい。 library(ggmcmc) ggs(b9.1b) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x=Iteration, y=value))+ annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = &quot;grey&quot;, alpha = 3/6, size = 0) + geom_line(aes(color = chain), size = .15)+ theme_classic()+ theme(strip.background = element_blank(), legend.position = c(.75, .18))+ facet_wrap(~Parameter, scales = &quot;free_y&quot;) 最初の50サンプル ggs(b9.1b) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x=Iteration, y=value))+ annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = &quot;grey&quot;, alpha = 3/6, size = 0) + geom_line(aes(color = chain), size = .15)+ theme_classic()+ theme(strip.background = element_blank(), legend.position = c(.75, .18))+ coord_cartesian(xlim=c(0,50))+ facet_wrap(~Parameter, scales = &quot;free_y&quot;) トレースプロットの診断では、以下の3つの基準を満たしているかを確認する。 stationarity chainがある一定の範囲から安定して得られているか。別の言い方をすると、サンプルの平均値が最初から最後まで安定しているか。 good mixing chainが全ての範囲から素早く探索できているか。 convergence それぞれのchainが同じ範囲に収まっているか。 もうひとつ収束を確認する方法としてtrace rank plot、もしくはtrank plotをみるというものがある。これは、サンプルを小さい方から順に1, 2, 3,…,Nというように順位づけ、chainごとにヒストグラムを描くというもの。もしうまく収束できていれば、各ヒストグラムは似たような形になり、重複するはず。 post %&gt;% mcmc_rank_overlay(pars = vars(1:5))+ coord_cartesian(ylim=c(25,NA))+ theme(legend.position = c(.75,.2)) 9.5 Care and feeding of your Marcov chain HMCのすぐれたところは、その効率性だけでなく、もしサンプリングがうまくいっていないときにそれが簡単にわかるということである。以下で、サンプリングがうまくいっていない場合を見ていこう。 9.5.1 How many samples do you need 重要なのはどのくらい有効サンプル数(自己相関していないマルコフ連鎖の長さ)である。有効サンプルサイズはサンプルに負の自己相関がある場合には、chainの長さよりも大きくなることがありうる。なお、bulk-ESSは事後分布の中心でどの程度有効サンプルが得られているかを評価している一方で、tail-ESSはより広い区間についてのものであるので、区間推定を行う場合にはtail-ESSに着目する。 何を知りたいかによって必要なサンプルサイズは異なる。もし事後分布の平均を知りたいだけであればそこまで多くのサンプルを必要としないが、より広い範囲(例えば、99%信用区間など)について調べたければ、より多くのサンプルを必要とする。一般的に、平均の推定値を知りたいだけならば、有効サンプルサイズは200程度でよい。また、事後分布が正規分布に近似できれば、分布の分散を推定すればうまく区間推定ができるが、偏りのある分布では区間推定のために多くのサンプルを必要とする。 9.5.2 How many chains do we need デバッグをする場合には、まず1つを回してうまくいくか確認する。 その後、終息の診断をするときには1つ以上のchainが必要。その場合、3~4個で十分。 最後に(上手く収束することが分かった後に)推定をする際には1つでも構わない。2つ以上でもよいが、全てのchainでwarmupすることになるので、効率は悪くなる。 収束の判断について詳しくは以下のリンクを参照。 Visual MCMC diagnostics using the bayesplot package 9.5.3 Taming and wild chain 極端に小さい(大きい)値からサンプリングをしてしまっている例を見ていく。これは事前分布が広すぎる場合にしばしば起きる。 b9.2 &lt;- brm(data = list(y=c(-1,1)), family=gaussian, y~1, prior = c(prior(normal(0, 1000), class = Intercept), prior(exponential(0.0001), class = sigma)), iter = 2000, warmup = 1000, chains = 3, backend = &quot;cmdstanr&quot;, seed = 9, file = &quot;output/Chapter9/b9.2&quot;) エラーがかなり出ている。また、結果もあり得ない値になっている。有効サンプルサイズもかなり小さい。 print(b9.2) ## Warning: There were 433 divergent transitions after ## warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 4.46 332.87 -782.85 767.68 1.01 604 388 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 554.99 1407.42 26.10 3613.65 1.02 108 86 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). nuts_param()関数をつかえば診断情報を調べることができる。 3000サンプルの内282個でdivergent transitionが起きていることが分かる。 ## 項目 nuts_params(b9.2) %&gt;% distinct() %&gt;% datatable() ## divergentについて nuts_params(b9.2) %&gt;% filter(Parameter == &quot;divergent__&quot;) %&gt;% count(Value) ## Value n ## 1 0 2567 ## 2 1 433 pairs()を用いて診断結果を図示できる。赤い点がdivergent transitionが起きているサンプル。 ※ 現在うまく出力されない。 pairs(b9.2, off_diag_args = list(size = 1/4)) chainを確認しても、うまく収束していないことが分かる。 post &lt;- posterior_samples(b9.2, add_chain = TRUE) p1 &lt;- post %&gt;% mcmc_trace(pars = vars(1:2), size=.25) p2 &lt;- post %&gt;% mcmc_rank_overlay(pars = vars(1:2)) ((p1/p2)&amp; theme(legend.position=&quot;none&quot;)&amp; plot_annotation(subtitle = &quot;These chains are not healthy&quot;)) 弱情報事前分布を用いることで、この問題を解決できる。 b9.3 &lt;- brm(data = list(y=c(-1,1)), family=gaussian, y~1, prior = c(prior(normal(1, 10), class = Intercept), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9.3&quot;) print(b9.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.03 1.16 -2.28 2.46 1.00 1256 1050 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.54 0.81 0.61 3.63 1.00 989 979 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). divergent transitionも起きていない。 pairs(b9.3, np = nuts_params(b9.3), off_diag_args = list(size = 1/4)) traceplotも概ね問題なさそう。 post_weekprior &lt;- posterior_samples(b9.3, add_chain = TRUE) p3 &lt;- post_weekprior %&gt;% mcmc_trace(pars = vars(1:2), size=.25) p4 &lt;- post_weekprior %&gt;% mcmc_rank_overlay(pars = vars(1:2)) ((p3/p4)&amp; theme(legend.position=&quot;none&quot;)&amp; plot_annotation(subtitle = &quot;Weakly informative priors cleared up the condition right away&quot;)) 事前分布と事後分布を図示してみる。 事前分布に少し情報を与えただけで、推定がうまくいくことが分かる。 post_weekprior %&gt;% dplyr::select(b_Intercept) %&gt;% ggplot(aes(x=b_Intercept))+ geom_density(trim = T, color=&quot;navy&quot;)+ geom_line(data=tibble(x=seq(-15,15,length.out=50)), aes(x=x, y=dnorm(x,mean=1,sd=10)), linetype=2, color=&quot;grey26&quot;)+ theme_classic()+ theme(aspect.ratio=1)+ xlab(expression(alpha)) -&gt;p5 post_weekprior %&gt;% dplyr::select(sigma) %&gt;% ggplot(aes(x=sigma))+ geom_density(trim = T, color=&quot;navy&quot;)+ geom_line(data=tibble(x=seq(0,10,length.out=50)), aes(x=x, y=dexp(x,1)), linetype=2, color = &quot;grey26&quot;)+ theme_classic()+ theme(aspect.ratio=1)+ xlab(expression(sigma)) -&gt;p6 p5+p6+ plot_annotation(title = &quot;Prior (dashed) vs posterior (solod) distributions. &quot;) 9.5.4 Non-identifiable parameters 識別不能な(一意に定まらない)パラメータがあるモデルを考え、そのときHMCがどのように働いているかを見てみる。 \\[ \\begin{aligned} y_{i} &amp;\\sim Normal(\\mu,\\sigma)\\\\ \\mu &amp;= \\alpha_{1} + \\alpha_{2}\\\\ \\alpha_{1} &amp;\\sim Normal(0,1000)\\\\ \\alpha_{2} &amp;\\sim Normal(0,1000)\\\\ \\sigma &amp;\\sim Exponential(1) \\end{aligned} \\] set.seed(384) y &lt;- rnorm(100,0,1) b9.4 &lt;- brm( data = list(y = y, a1 = 1, a2 = 1), family=gaussian, y ~ 0 + a1 + a2, prior = c(prior(normal(0, 1000), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9.4&quot; ) 上手く推定ができない。ほとんど有効サンプルがない。 また、上手く収束できていない。 print(b9.4) ## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be ## careful when analysing the results! We recommend running more iterations and/or ## setting stronger priors. ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + a1 + a2 ## Data: list(y = y, a1 = 1, a2 = 1) (Number of observations: 100) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a1 310.57 590.61 -676.22 1466.19 2.38 4 12 ## a2 -310.73 590.61 -1466.48 676.00 2.38 4 12 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.08 0.08 0.94 1.26 1.15 25 45 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). chainをみても収束がうまくいっていないのは明らか。 post_UI &lt;- posterior_samples(b9.4, add_chain = TRUE) post_UI %&gt;% mcmc_trace(pars = vars(1:3), size=.25) post_UI %&gt;% mcmc_rank_overlay(pars = vars(1:3)) 再び、弱情報事前分布を設定するとうまく推定ができる。 b9.5 &lt;- brm( data = list(y = y, a1 = 1, a2 = 1), family=gaussian, y ~ 0 + a1 + a2, prior = c(prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9.5&quot; ) 上手く推定ができている。a1とa2の係数は一意に定まらないが、その合計はうまく推定できる。 print(b9.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + a1 + a2 ## Data: list(y = y, a1 = 1, a2 = 1) (Number of observations: 100) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a1 -0.41 6.99 -14.16 12.85 1.00 725 600 ## a2 0.24 6.99 -13.11 14.14 1.00 724 597 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.07 0.08 0.93 1.23 1.00 1164 1072 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 二つのモデルのtraceplotを比べてみる。 # b9.4 post_b9.4 &lt;- posterior_samples(b9.4, add_chain=TRUE) p7 &lt;- trace_rank(data = post_b9.4, var = &quot;b_a1&quot;, subtitle = &quot;b9.4 (bad priors)&quot;) p8 &lt;- trace_rank(data = post_b9.4, var = &quot;b_a2&quot;) p9 &lt;- trace_rank(data = post_b9.4, var = &quot;sigma&quot;) # b9.4 post_b9.5 &lt;- posterior_samples(b9.5, add_chain=TRUE) p10 &lt;- trace_rank(data = post_b9.5, var = &quot;b_a1&quot;, subtitle = &quot;b9.5 (good priors)&quot;) p11 &lt;- trace_rank(data = post_b9.5, var = &quot;b_a2&quot;) p12 &lt;- trace_rank(data = post_b9.5, var = &quot;sigma&quot;) (p7/p8/p9/p10/p11/p12)&amp; theme(legend.position = &quot;none&quot;) 9.6 Practice 9.6.1 9M1 Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior and an exponential prior for the standard deviation, sigma. The uniform prior should be dunif(0,10) and the exponential should be dexp(1). Do the different priors have any detectable influence on the posterior distribution? 再びGDPと地形の関連についてモデリングするが、今回は標準偏差の事前分布に一様分布を用いる。 b9.1_unif &lt;- brm(data = d2, family = gaussian, formula = bf(G ~ 0 + a + b*R_c, a ~ 0 + A, b ~ 0 + A, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, nlpar = a), prior(normal(0, 0.3), class = b, nlpar = b), prior(uniform(0,1), class=sigma)), seed = 9, chain =4, cores=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9.1_unif&quot;) 事後分布にほとんど影響はない。 ## Exponential(1) posterior_summary(b9.1b) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters != &quot;lp__&quot;) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qbvdixtxao .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qbvdixtxao .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qbvdixtxao .gt_caption { padding-top: 4px; padding-bottom: 4px; } #qbvdixtxao .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qbvdixtxao .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #qbvdixtxao .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qbvdixtxao .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qbvdixtxao .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qbvdixtxao .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qbvdixtxao .gt_column_spanner_outer:first-child { padding-left: 0; } #qbvdixtxao .gt_column_spanner_outer:last-child { padding-right: 0; } #qbvdixtxao .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #qbvdixtxao .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #qbvdixtxao .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qbvdixtxao .gt_from_md > :first-child { margin-top: 0; } #qbvdixtxao .gt_from_md > :last-child { margin-bottom: 0; } #qbvdixtxao .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qbvdixtxao .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #qbvdixtxao .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #qbvdixtxao .gt_row_group_first td { border-top-width: 2px; } #qbvdixtxao .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qbvdixtxao .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #qbvdixtxao .gt_first_summary_row.thick { border-top-width: 2px; } #qbvdixtxao .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qbvdixtxao .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qbvdixtxao .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qbvdixtxao .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qbvdixtxao .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qbvdixtxao .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qbvdixtxao .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #qbvdixtxao .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qbvdixtxao .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qbvdixtxao .gt_left { text-align: left; } #qbvdixtxao .gt_center { text-align: center; } #qbvdixtxao .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qbvdixtxao .gt_font_normal { font-weight: normal; } #qbvdixtxao .gt_font_bold { font-weight: bold; } #qbvdixtxao .gt_font_italic { font-style: italic; } #qbvdixtxao .gt_super { font-size: 65%; } #qbvdixtxao .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #qbvdixtxao .gt_asterisk { font-size: 100%; vertical-align: 0; } #qbvdixtxao .gt_indent_1 { text-indent: 5px; } #qbvdixtxao .gt_indent_2 { text-indent: 10px; } #qbvdixtxao .gt_indent_3 { text-indent: 15px; } #qbvdixtxao .gt_indent_4 { text-indent: 20px; } #qbvdixtxao .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_a_A1 0.8866523 0.015798028 0.85643840 0.91717842 b_a_A2 1.0507625 0.010079614 1.03083530 1.07050915 b_b_A1 0.1321000 0.073764966 -0.01533556 0.27864973 b_b_A2 -0.1425208 0.056347768 -0.25179496 -0.03043489 sigma 0.1116139 0.006185409 0.10034831 0.12474091 lprior 2.1793178 0.231572347 1.69404549 2.59457936 plot(b9.1b, pars = &quot;sigma&quot;) ## Uniform(0,1) posterior_summary(b9.1_unif) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters != &quot;lp__&quot;) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #llipdrixob .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #llipdrixob .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #llipdrixob .gt_caption { padding-top: 4px; padding-bottom: 4px; } #llipdrixob .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #llipdrixob .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #llipdrixob .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #llipdrixob .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #llipdrixob .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #llipdrixob .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #llipdrixob .gt_column_spanner_outer:first-child { padding-left: 0; } #llipdrixob .gt_column_spanner_outer:last-child { padding-right: 0; } #llipdrixob .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #llipdrixob .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #llipdrixob .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #llipdrixob .gt_from_md > :first-child { margin-top: 0; } #llipdrixob .gt_from_md > :last-child { margin-bottom: 0; } #llipdrixob .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #llipdrixob .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #llipdrixob .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #llipdrixob .gt_row_group_first td { border-top-width: 2px; } #llipdrixob .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #llipdrixob .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #llipdrixob .gt_first_summary_row.thick { border-top-width: 2px; } #llipdrixob .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #llipdrixob .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #llipdrixob .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #llipdrixob .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #llipdrixob .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #llipdrixob .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #llipdrixob .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #llipdrixob .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #llipdrixob .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #llipdrixob .gt_left { text-align: left; } #llipdrixob .gt_center { text-align: center; } #llipdrixob .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #llipdrixob .gt_font_normal { font-weight: normal; } #llipdrixob .gt_font_bold { font-weight: bold; } #llipdrixob .gt_font_italic { font-style: italic; } #llipdrixob .gt_super { font-size: 65%; } #llipdrixob .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #llipdrixob .gt_asterisk { font-size: 100%; vertical-align: 0; } #llipdrixob .gt_indent_1 { text-indent: 5px; } #llipdrixob .gt_indent_2 { text-indent: 10px; } #llipdrixob .gt_indent_3 { text-indent: 15px; } #llipdrixob .gt_indent_4 { text-indent: 20px; } #llipdrixob .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_a_A1 0.8866944 0.01631168 0.85495623 0.91723303 b_a_A2 1.0505099 0.01015696 1.03073625 1.07104025 b_b_A1 0.1327129 0.07751632 -0.01952676 0.28281228 b_b_A2 -0.1430805 0.05478988 -0.24940050 -0.03657879 sigma 0.1113705 0.00613813 0.10000777 0.12382435 lprior 2.2878055 0.23574217 1.79088850 2.70819550 plot(b9.1_unif, pars = &quot;sigma&quot;) 9.6.2 9M2 Modify the terrain ruggedness model again. This time, change the prior for b[cid] to dexp(0.3). What does this do to the posterior distribution? Can you explain it? 今度は、R_cにかかる係数の事前分布をExponential(0.3)にする。これは、事前分布を正に限定していることになる。 b9.1_exp &lt;- brm(data = d2, family = gaussian, formula = bf(G ~ 0 + a + b*R_c, a ~ 0 + A, b ~ 0 + A, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, nlpar = a), prior(exponential(0.3),class=b,nlpar = b), prior(uniform(0,1), class=sigma)), seed = 9, chain =4, cores=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9.1_exp&quot;) かなり警告が出ていて、b_A2の係数が正になってしまっている。他のパラメータの推定はほとんど変わらないが、有効サンプルサイズはかなり少なくなっている。 print(b9.1_exp) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 0.89 0.02 0.86 0.92 1.01 383 618 ## a_A2 1.05 0.01 1.03 1.07 1.01 548 891 ## b_A1 0.15 0.07 0.02 0.30 1.00 489 972 ## b_A2 0.02 0.02 0.00 0.06 1.01 320 701 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.13 1.00 499 851 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). posterior_summary(b9.1_exp) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters != &quot;lp__&quot;) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #fgbtrpmpdq .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #fgbtrpmpdq .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fgbtrpmpdq .gt_caption { padding-top: 4px; padding-bottom: 4px; } #fgbtrpmpdq .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #fgbtrpmpdq .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #fgbtrpmpdq .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fgbtrpmpdq .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fgbtrpmpdq .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #fgbtrpmpdq .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #fgbtrpmpdq .gt_column_spanner_outer:first-child { padding-left: 0; } #fgbtrpmpdq .gt_column_spanner_outer:last-child { padding-right: 0; } #fgbtrpmpdq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #fgbtrpmpdq .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #fgbtrpmpdq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #fgbtrpmpdq .gt_from_md > :first-child { margin-top: 0; } #fgbtrpmpdq .gt_from_md > :last-child { margin-bottom: 0; } #fgbtrpmpdq .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #fgbtrpmpdq .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #fgbtrpmpdq .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #fgbtrpmpdq .gt_row_group_first td { border-top-width: 2px; } #fgbtrpmpdq .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fgbtrpmpdq .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #fgbtrpmpdq .gt_first_summary_row.thick { border-top-width: 2px; } #fgbtrpmpdq .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fgbtrpmpdq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fgbtrpmpdq .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #fgbtrpmpdq .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #fgbtrpmpdq .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fgbtrpmpdq .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fgbtrpmpdq .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #fgbtrpmpdq .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fgbtrpmpdq .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fgbtrpmpdq .gt_left { text-align: left; } #fgbtrpmpdq .gt_center { text-align: center; } #fgbtrpmpdq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #fgbtrpmpdq .gt_font_normal { font-weight: normal; } #fgbtrpmpdq .gt_font_bold { font-weight: bold; } #fgbtrpmpdq .gt_font_italic { font-style: italic; } #fgbtrpmpdq .gt_super { font-size: 65%; } #fgbtrpmpdq .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #fgbtrpmpdq .gt_asterisk { font-size: 100%; vertical-align: 0; } #fgbtrpmpdq .gt_indent_1 { text-indent: 5px; } #fgbtrpmpdq .gt_indent_2 { text-indent: 10px; } #fgbtrpmpdq .gt_indent_3 { text-indent: 15px; } #fgbtrpmpdq .gt_indent_4 { text-indent: 20px; } #fgbtrpmpdq .gt_indent_5 { text-indent: 25px; } parameters Estimate Est.Error Q2.5 Q97.5 b_a_A1 0.88738587 0.016264158 0.8550791750 0.91906462 b_a_A2 1.04853402 0.010236978 1.0286857500 1.06822025 b_b_A1 0.14937156 0.070870054 0.0225516525 0.29556600 b_b_A2 0.01693269 0.015985722 0.0005696201 0.05762359 sigma 0.11395191 0.006049614 0.1030100000 0.12689643 lprior -0.46087953 0.189949524 -0.8744978750 -0.12167350 b_A2のchainだけおかしなことになっている。 ##全部 posterior_samples(b9.1_exp, add_chain = TRUE) %&gt;% dplyr::select(-iter, -lp__) %&gt;% mcmc_trace(size = .25) posterior_samples(b9.1_exp, add_chain =TRUE) %&gt;% dplyr::select(-iter, -lp__) %&gt;% mcmc_rank_overlay()+ coord_cartesian(ylim = c(30,NA)) ## b_A2のみ posterior_samples(b9.1_exp, add_chain = TRUE) %&gt;% mcmc_trace(size = .25, pars = &quot;b_b_A2&quot;) posterior_samples(b9.1_exp, add_chain =TRUE) %&gt;% mcmc_rank_overlay(pars = &quot;b_b_A2&quot;)+ coord_cartesian(ylim = c(30,NA)) posterior_samples(b9.1_exp, add_chain =TRUE) %&gt;% mcmc_dens(pars = &quot;b_b_A2&quot;) 9.6.3 9M3 Re-estimate one of the Stan models from the chapter, but at different numbers of warmup iterations. Be sure to use the same number of sampling iterations in each case. Compare the n_eff values. b9.1bについてwarmupの回数を変えてみる(5,10,100,500,1000,1500)。 そもそもwarmupが5と10ではdivergence transitionが起きてしまう。それ以後は特に大きな差はない。 list &lt;- list(5,10,100,500,1000,1500) no_eff &lt;- function(w){ r &lt;- update(b9.1b, chains = 4, cores=4, iter=1000+w, warmup=w, seed=9) } purrr::map(list, no_eff) ## [[1]] ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 1005; warmup = 5; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 1.00 0.10 0.91 1.16 17.53 4 NA ## a_A2 1.22 0.20 0.96 1.51 9.45 4 NA ## b_A1 0.72 0.55 0.14 1.61 22.81 4 4 ## b_A2 0.72 1.30 -1.51 1.63 26.63 4 4 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.35 0.11 0.23 0.52 10.12 4 4 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). ## ## [[2]] ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 1010; warmup = 10; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 0.89 0.02 0.86 0.92 1.08 118 112 ## a_A2 1.05 0.01 1.03 1.07 1.33 18 6 ## b_A1 0.13 0.13 -0.07 0.38 1.62 7 12 ## b_A2 -0.12 0.12 -0.26 0.08 1.31 10 52 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.12 0.01 0.10 0.15 1.40 9 29 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). ## ## [[3]] ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 1100; warmup = 100; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 0.89 0.02 0.86 0.92 1.00 4260 3024 ## a_A2 1.05 0.01 1.03 1.07 1.00 4568 2992 ## b_A1 0.13 0.07 -0.01 0.28 1.00 1667 1697 ## b_A2 -0.14 0.06 -0.25 -0.03 1.00 2618 1956 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 2316 2070 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). ## ## [[4]] ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 0.89 0.02 0.86 0.92 1.00 4816 3212 ## a_A2 1.05 0.01 1.03 1.07 1.00 4834 3104 ## b_A1 0.13 0.07 -0.01 0.28 1.00 4546 3021 ## b_A2 -0.14 0.05 -0.25 -0.04 1.00 4508 2896 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 4789 3463 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). ## ## [[5]] ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 0.89 0.02 0.86 0.92 1.00 4788 3163 ## a_A2 1.05 0.01 1.03 1.07 1.00 4423 3086 ## b_A1 0.13 0.07 -0.02 0.28 1.00 4279 2983 ## b_A2 -0.14 0.06 -0.25 -0.03 1.00 4892 2828 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 4850 2996 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). ## ## [[6]] ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: G ~ 0 + a + b * R_c ## a ~ 0 + A ## b ~ 0 + A ## Data: d2 (Number of observations: 170) ## Draws: 4 chains, each with iter = 2500; warmup = 1500; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_A1 0.89 0.02 0.86 0.92 1.00 5188 2933 ## a_A2 1.05 0.01 1.03 1.07 1.00 4582 3237 ## b_A1 0.13 0.08 -0.02 0.28 1.00 4577 3089 ## b_A2 -0.14 0.06 -0.25 -0.03 1.00 4871 3113 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 4782 3087 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 9.6.4 9H1 Run the model below and then inspect the posterior distribution and explain what it is accomplishing. 単に事前分布からサンプリングしていることになる。 bのtraceplotがところどころ極端な値に飛んでいるのは、コーシー分布が非常に幅広い裾を持つからである。この場合、bのtraceplotは問題ない。 mp &lt;- brm(bf(1 ~ a + b, a ~ 1, b ~ 1, nl = TRUE), data = 1, prior = c(prior(normal(0, 1), class = b, nlpar = a), prior(cauchy(0, 1), class = b, nlpar = b)), iter = 2000, chains = 1, sample_prior = &quot;only&quot;, file = &quot;output/Chapter9/mp&quot;) as_draws_df(mp, variable = &quot;^b_&quot;, regex = TRUE) %&gt;% as_tibble() %&gt;% pivot_longer(starts_with(&quot;b_&quot;), names_to = &quot;param&quot;) %&gt;% mutate(param = str_replace_all(param, &quot;b_([ab])_Intercept&quot;, &quot;\\\\1&quot;)) %&gt;% ggplot(aes(x = .draw, y = value)) + facet_wrap(~param, nrow = 1, scales = &quot;free_y&quot;) + geom_line(color = &quot;#009FB7&quot;) 9.6.5 9H2 Recall the divorce rate example from Chapter 5. Repeat that analysis, using ulam() this time, fitting models m5.1, m5.2, and m5.3. Use compare to compare the models on the basis of WAIC or PSIS. Explain the results. b5.1 &lt;- readRDS(&quot;output/Chapter5/b5.1.rds&quot;) b5.2 &lt;- readRDS(&quot;output/Chapter5/b5.2.rds&quot;) b5.3 &lt;- readRDS(&quot;output/Chapter5/b5.3.rds&quot;) loo_compare(b5.1,b5.2,b5.3) %&gt;% print(simplify = FALSE) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b5.1 0.0 0.0 -62.9 6.4 3.7 1.8 125.9 12.8 ## b5.3 -0.9 0.4 -63.8 6.4 4.8 1.9 127.7 12.9 ## b5.2 -6.7 4.6 -69.6 4.9 2.9 0.9 139.2 9.8 9.6.6 9H3 Sometimes changing a prior for one parameter has unanticipated effects on other parameters. This is because when a parameter is highly correlated with another parameter in the posterior, the prior influences both parameters. Here’s an example to work and think through. Go back to the leg length example in Chapter 5. Here is the code again, which simulates height and leg lengths for 100 imagined individuals: set.seed(909) n &lt;- 100 d &lt;- tibble(height = rnorm(n, 10, 2), leg_prop = runif(n, 0.4, 0.5)) %&gt;% mutate(leg_left = leg_prop*height + rnorm(n,0,0.02), leg_right = leg_prop*height + rnorm(n, 0, 0.02)) b9H3 &lt;- brm(data = d, family = gaussian, formula = height ~ 1 + leg_left + leg_right, prior = c(prior(normal(10,100),class=Intercept), prior(normal(2,10),class=b), prior(exponential(1),class = sigma)), iter=2000,warmup=1000,chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9H3&quot;) b9H3_2 &lt;- brm(data = d, family = gaussian, formula = bf(height ~ a + b + c, a ~ 0 + leg_left, b ~ 0 + leg_right, c ~ 1, nl = TRUE), prior = c(prior(normal(10,100),nlpar = c), prior(normal(2,10),nlpar = a), prior(normal(2,10),nlpar =b,lb =0), prior(exponential(1),class = sigma)), iter=2000,warmup=1000,chains=4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter9/b9H3_2&quot;) もともとの制約なしのモデル。 pairs(b9H3, off_diag_args = list(size = 1/5, alpha = 1/5)) 右脚の係数の事前分布を正に制約したモデル。 右脚と左脚は強く相関していたので、右脚の係数が左に傾いたことで左脚の係数が右に傾いた。 pairs(b9H3_2, off_diag_args = list(size = 1/5, alpha = 1/5)) 9.6.7 9H4 For the two models fit in the previous problem, use WAIC or PSIS to compare the effective numbers of parameters for each model. You will need to use log_lik=TRUE to instruct ulam() to compute the terms that both WAIC and PSIS need. Which model has more effective parameters? Why? 2つめのモデルの方がPSISが小さい。これは、事前分布が制約されたことによって、事後分布の分散が小さくなったからだと考えられる。 b9H3 &lt;- add_criterion(b9H3, c(&quot;loo&quot;,&quot;waic&quot;)) b9H3_2 &lt;- add_criterion(b9H3_2, c(&quot;loo&quot;,&quot;waic&quot;)) loo_compare(b9H3, b9H3_2) %&gt;% print(simplify=FALSE) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b9H3_2 0.0 0.0 -97.1 5.7 2.7 0.4 194.1 11.3 ## b9H3 -0.5 0.2 -97.6 5.6 3.2 0.5 195.1 11.2 9.6.8 9H5 Modify the Metropolis algorithm code from the chapter to handle the case that the island populations have a different distribution than the island labels. This means the island’s number will not be the same as its population. set.seed(9) num_weeks &lt;- 1e6 positions &lt;- rep(0, num_weeks) current &lt;- 10 ## 各島の人口をランダムに決定する population &lt;- sample(1:10) ## アルゴリズムの記述 for(i in 1:num_weeks){ positions[i] &lt;- current proposal &lt;- current + sample(c(-1,1), size=1) if(proposal &lt;1) proposal &lt;- 10 if(proposal &gt;10) proposal &lt;- 1 prob_move &lt;- population[proposal]/population[current] current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } 人口をランダムに振り分けてもうまく働くことが分かった。 tibble(week = 1:1e6, island = positions) %&gt;% count(island) %&gt;% mutate(population = population) %&gt;% mutate(prop = n/n[population==&quot;1&quot;]) %&gt;% gt() %&gt;% fmt_number(&quot;prop&quot;,decimals=2) %&gt;% tab_options(table.align=&#39;left&#39;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #bxkcxtluoe .gt_table { display: table; border-collapse: collapse; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #bxkcxtluoe .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bxkcxtluoe .gt_caption { padding-top: 4px; padding-bottom: 4px; } #bxkcxtluoe .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #bxkcxtluoe .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #bxkcxtluoe .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bxkcxtluoe .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bxkcxtluoe .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #bxkcxtluoe .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #bxkcxtluoe .gt_column_spanner_outer:first-child { padding-left: 0; } #bxkcxtluoe .gt_column_spanner_outer:last-child { padding-right: 0; } #bxkcxtluoe .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #bxkcxtluoe .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #bxkcxtluoe .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #bxkcxtluoe .gt_from_md > :first-child { margin-top: 0; } #bxkcxtluoe .gt_from_md > :last-child { margin-bottom: 0; } #bxkcxtluoe .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #bxkcxtluoe .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #bxkcxtluoe .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #bxkcxtluoe .gt_row_group_first td { border-top-width: 2px; } #bxkcxtluoe .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bxkcxtluoe .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #bxkcxtluoe .gt_first_summary_row.thick { border-top-width: 2px; } #bxkcxtluoe .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bxkcxtluoe .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bxkcxtluoe .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #bxkcxtluoe .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #bxkcxtluoe .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bxkcxtluoe .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bxkcxtluoe .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #bxkcxtluoe .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bxkcxtluoe .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #bxkcxtluoe .gt_left { text-align: left; } #bxkcxtluoe .gt_center { text-align: center; } #bxkcxtluoe .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #bxkcxtluoe .gt_font_normal { font-weight: normal; } #bxkcxtluoe .gt_font_bold { font-weight: bold; } #bxkcxtluoe .gt_font_italic { font-style: italic; } #bxkcxtluoe .gt_super { font-size: 65%; } #bxkcxtluoe .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #bxkcxtluoe .gt_asterisk { font-size: 100%; vertical-align: 0; } #bxkcxtluoe .gt_indent_1 { text-indent: 5px; } #bxkcxtluoe .gt_indent_2 { text-indent: 10px; } #bxkcxtluoe .gt_indent_3 { text-indent: 15px; } #bxkcxtluoe .gt_indent_4 { text-indent: 20px; } #bxkcxtluoe .gt_indent_5 { text-indent: 25px; } island n population prop 1 91653 5 5.06 2 109872 6 6.06 3 54369 3 3.00 4 144904 8 7.99 5 126778 7 6.99 6 72408 4 3.99 7 181812 10 10.03 8 163674 9 9.03 9 36400 2 2.01 10 18130 1 1.00 9.6.9 9H6 Modify the Metropolis algorithm code from the chapter to write your own simple MCMC estimator for globe tossing data and model from Chapter 2. コインは{W,L,W,W,W,L,W,L,W}の順番で落ちた。 ## 以下アルゴリズム w &lt;- 6 n &lt;- 9 iter=1e4 p_prior &lt;- function(p) dunif(p, min=0,max=1) p_sample &lt;- rep(0, iter) p_current &lt;- 0.5 for(i in 1:iter){ p_sample[i] &lt;- p_current p_proposal &lt;- runif(1,min=0,max=1) prob_current &lt;- dbinom(w,n,p_current)*p_prior(p_current) prob_proposal &lt;- dbinom(w,n,p_proposal)*p_prior(p_proposal) prob_move &lt;- prob_proposal/prob_current p_current &lt;-ifelse(runif(1) &lt; prob_move, p_proposal, p_current) } 上手く推定できていそう。 tibble(iter = 1:iter, prob = p_sample) %&gt;% ggplot(aes(x=iter, y = prob))+ geom_line()+ theme_bw() tibble(iter = 1:iter, prob = p_sample) %&gt;% ggplot(aes(x=prob))+ geom_histogram(binwidth = 0.1)+ stat_function(fun=function(x) iter*dbinom(w,n,x), color = &quot;red&quot;)+ labs(y = &quot;Density&quot;)+ theme_bw() mean_qi(p_sample) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gvehgwzrox .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gvehgwzrox .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gvehgwzrox .gt_caption { padding-top: 4px; padding-bottom: 4px; } #gvehgwzrox .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gvehgwzrox .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #gvehgwzrox .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gvehgwzrox .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gvehgwzrox .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gvehgwzrox .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gvehgwzrox .gt_column_spanner_outer:first-child { padding-left: 0; } #gvehgwzrox .gt_column_spanner_outer:last-child { padding-right: 0; } #gvehgwzrox .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #gvehgwzrox .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #gvehgwzrox .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gvehgwzrox .gt_from_md > :first-child { margin-top: 0; } #gvehgwzrox .gt_from_md > :last-child { margin-bottom: 0; } #gvehgwzrox .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gvehgwzrox .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #gvehgwzrox .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #gvehgwzrox .gt_row_group_first td { border-top-width: 2px; } #gvehgwzrox .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gvehgwzrox .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #gvehgwzrox .gt_first_summary_row.thick { border-top-width: 2px; } #gvehgwzrox .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gvehgwzrox .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gvehgwzrox .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gvehgwzrox .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gvehgwzrox .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gvehgwzrox .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gvehgwzrox .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #gvehgwzrox .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gvehgwzrox .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #gvehgwzrox .gt_left { text-align: left; } #gvehgwzrox .gt_center { text-align: center; } #gvehgwzrox .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gvehgwzrox .gt_font_normal { font-weight: normal; } #gvehgwzrox .gt_font_bold { font-weight: bold; } #gvehgwzrox .gt_font_italic { font-style: italic; } #gvehgwzrox .gt_super { font-size: 65%; } #gvehgwzrox .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #gvehgwzrox .gt_asterisk { font-size: 100%; vertical-align: 0; } #gvehgwzrox .gt_indent_1 { text-indent: 5px; } #gvehgwzrox .gt_indent_2 { text-indent: 10px; } #gvehgwzrox .gt_indent_3 { text-indent: 15px; } #gvehgwzrox .gt_indent_4 { text-indent: 20px; } #gvehgwzrox .gt_indent_5 { text-indent: 25px; } y ymin ymax .width .point .interval 0.6354329 0.3450299 0.8791578 0.95 mean qi "],["big-entropy-and-the-generalized-linear-model.html", "10 Big Entropy and the Generalized Linear Model 10.1 Maximum entropy 10.2 Generalized linear models", " 10 Big Entropy and the Generalized Linear Model 統計モデリングでは、確率分布としてエントロピーが最大になるものを選ぶべきである。 事前分布の選択で言えば、そのパラメータについて私たちが持っている科学的な知識に合うように、最も情報が少ない分布を選ぶ。尤度については、応答変数のいかなる値であってもそこから得られる可能性があるような分布を選ぶ。そうすることで、最小の情報を持ちつつも現実のデータに即した分布を得ることができる。 自然現象はエントロピーが最大になる分布に従うことが多い。 上手くいくことが多い。 10.1 Maximum entropy 第7章で見たように、infromation entropyは以下の式で表せる。 \\[ H(p) = -\\sum_{i} p_{i} log(p_{i}) \\] 最大エントロピー原理は、この指標を確率分布を選ぶ際に適用する。例を見ていこう。10の小石を5つのバケツにランダムに入れるとする。このときバケツ3にすべての小石が入るのは1通りだが、バケツ2、バケツ3、バケツ4にそれぞれ1,8,1個の石が入るのは\\(_{10}C_{8}×_{2}C_{1} = 90\\)通りである。それぞれに同じ数ずつ石が入るようにすればするほど、考えうる場合の数は大きくなっていく。バケツ1~5に入る小石の数が{0,0,10,0,0}、{0,1,8,1,0}、{0,2,6,2,0}、{1,2,4,2,1}、{2,2,2,2,2}の場合を考える。 d &lt;- tibble(a = c(0,0,10,0,0), b = c(0,1,8,1,0), c = c(0,2,6,2,0), d = c(1,2,4,2,1), e = c(2,2,2,2,2)) この時、それぞれの場合のinformation entropyは、 d %&gt;% mutate_all(~. /sum(.)) %&gt;% pivot_longer(everything(), names_to = &quot;bucket&quot;) %&gt;% group_by(bucket) %&gt;% summarise(entropy = -sum(ifelse(value==0,0, value*log(value)))) -&gt; ent print(ent) ## # A tibble: 5 × 2 ## bucket entropy ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 0 ## 2 b 0.639 ## 3 c 0.950 ## 4 d 1.47 ## 5 e 1.61 また、それぞれの場合に取りうる場合の数とエントロピーの関係を図示すると正比例することが分かる。 ent %&gt;% mutate(n_ways = c(1, 90, 1260, 37800, 113400)) %&gt;% group_by(bucket) %&gt;% mutate(log_ways = log(n_ways)/10, text_y = ifelse(bucket &lt; &quot;c&quot;, entropy + .15, entropy - .15)) %&gt;% ggplot(aes(x=log_ways, y=entropy))+ geom_point(size=2.5, color = &quot;black&quot;)+ geom_abline(intercept = 0, slope=1.37, linetype=2)+ theme_bw()+ labs(x=&quot;log(ways) per pebble&quot;)+ theme(aspect.ratio=1) このことから、information entropy の大きさはある分布に対して何通りの配置が可能であるかを表していることが分かる。このとき、最大のものを最大エントロピー分布(maximum entropy distribution)と言う。最も多くの通りが発生する分布が最もplausibleな分布なので、最大エントロピー分布が最もplausibleな分布であることが多い。もちろん、これは私たちが何を仮定するかによる。以下では、正規分布と二項分布の2つの例を見ていく。 10.1.1 Gaussian 同じ分散を持った様々な連続分布を考える。generalized normal distributuionは以下の確率密度で表せる。\\(\\alpha\\)はスケールを、\\(\\beta\\)は形を表すパラメータ。これらの分布を正規分布と比較することで、正規分布がもっとも均等な分布であることを見ていく。 \\[ Pr(y|\\mu,\\alpha,\\beta) = \\frac{\\beta}{2\\alpha\\Gamma(1/\\beta)} e^-(\\frac{|y-\\mu|}{\\alpha}) \\] なお、このとき\\(\\alpha\\)は、 \\[ \\alpha = \\sqrt{\\frac{\\sigma^2\\Gamma(1/\\beta)}{\\Gamma(3/\\beta)}} \\] \\(\\beta\\)がおよそ2のとき、つまり正規分布のときにエントロピーが最大になっていることが分かる。このように分散が一定の値のとき、正規分布が最大エントロピー分布となる。 ## 分布 alpha_per_beta &lt;- function(beta, variance =1){ sqrt(variance*gamma(1/beta)/gamma(3/beta)) } crossing(value = seq(-5,5,by=.1), beta = c(1,1.5,2,4)) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta)) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) -&gt; GND GND %&gt;% ggplot(aes(x=value, y = density, group=beta))+ geom_line(aes(color = beta==2, size = beta==2))+ scale_color_manual(values = c(&quot;black&quot;,&quot;blue&quot;)) + scale_size_manual(values = c(1/4, 1.25))+ theme(legend.position=&quot;none&quot;, aspect.ratio=1) -&gt; p1 ## エントロピー crossing(value = seq(-8,8,by=1), beta = seq(1,4,length.out=1000)) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta, 1)) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% group_by(beta) %&gt;% summarise(entropy = -sum(ifelse(density==0,0, density*log(density))))%&gt;% ggplot(aes(x = beta, y = entropy))+ geom_line(color=&quot;navy&quot;)+ geom_vline(xintercept=2, linetype=2)+ theme(aspect.ratio=1) -&gt;p2 p1 + p2 ただし、分散以外にも様々な仮定を設けるとき、正規分布以外が適切な場合もある。 10.1.2 Binomial もし2つの事象しか発生しないとき、一方の事象が起こる確率をpとすれば、n回試行を繰り返したときに、事象1と2がそれぞれy回とn-y回起こるとき、その確率は、 \\[ Pr(y_{1},y_{2}, ..., y_{n}|n,p) = p^y(1-p)^{n-y} \\] 10.1.2.1 Example 1 青と白の石が入ったカバン(それぞれが何個入っているかは不明)から2つの石を取り出す。このとき、考えうる組み合わせは{W,W}、{W,B}、{B,W}、{B,B}の4通り。それぞれの結果が起こりうる確率を割り当てていくことを考える。このとき、引いた青い石の数の期待値が1であると仮定する。 4つの場合を考える。すべて期待値は1だが、Aのみが二項分布である(p = 0.5, n =2)。 prob &lt;- tibble(Distribution = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), ww = c(1/4, 2/6, 1/6, 1/8), bw = c(1/4, 1/6, 2/6, 4/8), wb = c(1/4, 1/6, 2/6, 2/8), bb = c(1/4, 2/6, 1/6, 1/8)) prob %&gt;% mutate_if(is.numeric, ~MASS::fractions(.) %&gt;% as.character()) %&gt;% kable() Distribution ww bw wb bb A 1/4 1/4 1/4 1/4 B 1/3 1/6 1/6 1/3 C 1/6 1/3 1/3 1/6 D 1/8 1/2 1/4 1/8 prob %&gt;% pivot_longer(cols = 2:5) %&gt;% mutate(sequence=factor(name,levels=c(&quot;ww&quot;,&quot;bw&quot;,&quot;wb&quot;,&quot;bb&quot;)))-&gt; prob prob %&gt;% ggplot(aes(x=sequence, y=value, group=1))+ geom_point(size=1.5, color = &quot;navy&quot;)+ geom_line(color=&quot;navy&quot;)+ facet_wrap(~Distribution) それぞれのエントロピーを求めてみると、Aが最も高いことが分かる。ただし、この例は期待値が1という特殊な状況しか試していない。 prob %&gt;% group_by(Distribution) %&gt;% summarise(entropy = -sum(value*log(value))) %&gt;% flextable::flextable() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td, .tabwid th { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-93b0cc12{}.cl-93a0edf6{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-93abc4f6{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-93abc500{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-93abed3c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93abed3d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93abed3e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93abed46{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93abed47{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93abed50{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}DistributionentropyA1.386294B1.329661C1.329661D1.213008 10.1.2.2 Example 2 今度は、青い石の数の期待値が1.4の時を考える。これは、\\(p=0.7\\)の時に相当する。このとき、二項分布では… p &lt;- 0.7 (A &lt;- c((1-p)^2, p*(1-p),(1-p)*p,p^2)) ## [1] 0.09 0.21 0.21 0.49 ent_binom &lt;- -sum(A*log(A)) print(ent_binom) ## [1] 1.221729 期待値が1.4になるランダムな分布をたくさん作って、二項分布と比べてみる。 sim_p &lt;- function(seed, g = 1.4) { set.seed(seed) x_123 &lt;- runif(3) x_4 &lt;- ((g) * sum(x_123) - x_123[2] - x_123[3]) / (2 - g) z &lt;- sum(c(x_123, x_4)) p &lt;- c(x_123, x_4) / z tibble(h = -sum(p * log(p)), p = p, key = factor(c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;), levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) } sim_p(9.9, g=1.4) %&gt;% flextable::flextable() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td, .tabwid th { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-93eb9996{}.cl-93e59fe6{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-93e7f750{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-93e7f764{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-93e80498{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93e804a2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93e804a3{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93e804ac{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93e804ad{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-93e804ae{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}hpkey1.0223540.19710875ww1.0223540.02155544bw1.0223540.18422705wb1.0223540.59710875bb 100000回シミュレーションしてみる。 n_rep &lt;- 1e5 d &lt;- tibble(seed = 1:n_rep) %&gt;% mutate(sim = map2(seed, 1.4, sim_p)) %&gt;% unnest(sim) head(d) ## # A tibble: 6 × 4 ## seed h p key ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 1.21 0.108 ww ## 2 1 1.21 0.151 bw ## 3 1 1.21 0.233 wb ## 4 1 1.21 0.508 bb ## 5 2 1.21 0.0674 ww ## 6 2 1.21 0.256 bw 上から順番に見てみる。一番エントロピーが高いものは、ほとんど二項分布と同じであることが分かる。 d %&gt;% group_by(seed) %&gt;% arrange(desc(h)) %&gt;% ungroup() %&gt;% mutate(rank = rep(1:n_rep, each=4)) -&gt; rank_d head(rank_d) ## # A tibble: 6 × 5 ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 ## 5 71132 1.22 0.0902 ww 2 ## 6 71132 1.22 0.210 bw 2 subset_d &lt;- rank_d %&gt;% filter(rank %in% c(1, 87373, n_rep-1500, n_rep-10)) %&gt;% mutate(height = rep(c(8, 2.25, .75, .5), each = 4), distribution = rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), each = 4)) p1 &lt;- d %&gt;% ggplot(aes(x=h))+ geom_density(size=0, adjust = 1/4, color = &quot;navy&quot;)+ geom_linerange(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(ymin = 0, ymax = height)) + geom_text(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(y = height + .5, label = distribution))+ labs(x=&quot;Entropy&quot;, y = &quot;Density&quot;)+ theme(aspect.ratio=1) p2 &lt;- subset_d %&gt;% ggplot(aes(x=key, y = p, group=1))+ geom_point(color=&quot;navy&quot;)+ geom_line(color=&quot;navy&quot;)+ facet_wrap(~distribution)+ labs(x = NULL, y = NULL)+ theme(strip.background = element_blank(), aspect.ratio=0.8) p1+p2 10.2 Generalized linear models これまでの章では応答変数の分布として正規分布のみを用いていたが、それが適当でない場合もある(例えば、負の値をとりえない場合など)。そのため、応答変数に関する知識を用いてエントロピーを最大にする分布を用いる必要がある。これが一般化線形モデルのエッセンスである。 一般化線形モデルでは、応答変数の分布として正規分布以外も想定し、パラメータをもっともらしい範囲に収めるためにリンク関数を用いる。 10.2.1 Meet the family 統計モデリングで用いられることが多いのは、指数関数族と呼ばれる分布たちである。 指数分布 単位時間あたりに平均\\(\\lambda\\)回起こる事象の発生間隔が\\(x\\)である確率(ex. 1時間当たり5人が訪れるレストランで、次の客が来るまでの時間間隔。)。 \\[ f(x) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; \\quad (x \\ge 0) \\\\ 0 &amp; \\quad (x &lt;0) \\end{cases} \\] ガンマ分布 期間\\(\\frac{1}{\\lambda}\\)あたりに1回起こると期待されるランダムな事象が\\(\\alpha\\)回起こるまでの時間の分布(ex. 製品部品の寿命、ガンになるまでの時間)。 \\[ f(x) = \\begin{cases} \\frac {\\lambda ^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1}e^{-\\lambda x} &amp; \\quad (x \\ge 0)\\\\ 0 &amp; \\quad (x &lt;0) \\end{cases} \\] ポワソン分布 単位時間あたりに平均\\(\\lambda\\)回起こる現象が、単位時間に\\(x\\)回起きる確率。二項分布で\\(n\\)がとても大きく、\\(p\\)が小さいとき、二項分布は\\(\\lambda = np\\)のポワソン分布に収束する。 \\[ f(x) = \\frac {\\lambda ^x e^{-\\lambda}}{k!} \\] 10.2.2 Linking linear models to distributions リンク関数としては、よく使われるものが2つある。 logit link パラメータを0から1までの範囲に変換する。 \\[ \\begin{aligned} y_{i} &amp;\\sim Binomial(n, p_{i})\\\\ logit(p_{i}) &amp;= \\alpha + \\beta x_{i} \\end{aligned} \\] なお、 \\[ \\begin{aligned} logit(p_{i}) &amp;= log \\frac{p_{i}}{1-p_{i}}\\\\ \\therefore p_{i} &amp;= \\frac{exp(\\alpha + \\beta x_{i})}{1 + exp(\\alpha + \\beta x_{i})} \\end{aligned} \\] リンク関数を使う場合には、説明変数が1単位増えるごとに応答変数の平均に与える変化が一定でないことに注意する必要がある。ロジット関数であれば、説明変数が大きく(小さく)なるほど変化も小さくなる。 log link パラメータを0以上に変換する。 \\[ \\begin{aligned} y_{i} &amp;\\sim Normal(\\mu, \\sigma_{i})\\\\ log(\\sigma_{i}) &amp;= \\alpha + \\beta x_{i}\\\\ \\therefore \\sigma_{i} &amp;= exp(\\alpha + \\beta x_{i}) \\end{aligned} \\] ログリンク関数では、説明変数が1単位増えると説明変数の平均が幾何級数的に増える。 10.2.3 Overthinking リンク関数を導入したとき、説明変数が1単位増えるごとに応答変数の平均に与える変化が一定でないことを数学的に示す。 ロジット関数を使うとき、オッズ比\\(p/(1-p)\\)の変化量は、 \\[ \\begin{aligned} p &amp;= \\frac{exp(\\alpha + \\beta x)}{1 + exp(\\alpha +\\beta x)}\\\\ \\frac{\\partial p/(1-p)}{\\partial x} &amp;= \\beta exp(\\alpha +\\beta x) \\end{aligned} \\] すなわち、オッズ比\\(p/(1-p)\\)の変化量は\\(x\\)の値に依存することが分かる。 10.2.4 Omitted variable bias again リンク関数を使うとパラメータに上限や下限ができるため、因果推論に必要な説明変数がないとうまく推定結果が得られないことが多くなる。 10.2.5 Absolute and relative differences それぞれのパラメータは応答変数に絶対的な違いではなく、相対的な違いをもたらす。そのため、係数が大きいからと言って必ずして大きな影響を与えるとは言えないことがある。 10.2.6 GLMs and information criteria 異なる分布に当てはめたモデルを比較することはできない(Watts 2015; Städele et al. 2019)。 References "],["god-spiked-the-integers.html", "11 God Spiked the Integers 11.1 Binomial regression 11.2 Aggregated binomial: Graduate school admittions. 11.3 Poisson regression 11.4 Multinomial and categorical models 11.5 Practice", " 11 God Spiked the Integers 11.1 Binomial regression 二項分布のGLMはデータの表現によって2通りある(モデル自体は全く同じ)。 Logistic regression 応答変数を0と1で表現する場合。 Aggregated binomial regression 同じ共変量についての試行をまとめる場合。この場合0からnまでの値をとる。 11.1.1 Logistic regression Silk et al. (2005) のチンパンジーを対象とした向社会的選択課題を例に見ていく。チンパンジーは相手がいるときにのみ向社会的な選択をするのかを調べるため、condition(相手の有無)とoption(左右どちらが高社会的選択か)の交互作用を見ていく。 # データ読み込み data(&quot;chimpanzees&quot;) d &lt;- chimpanzees head(d) ## actor recipient condition block trial prosoc_left chose_prosoc pulled_left ## 1 1 NA 0 1 2 0 1 0 ## 2 1 NA 0 1 4 0 0 1 ## 3 1 NA 0 1 6 1 0 0 ## 4 1 NA 0 1 8 0 1 0 ## 5 1 NA 0 1 10 1 1 1 ## 6 1 NA 0 1 12 1 1 1 d %&gt;% distinct(prosoc_left, condition) %&gt;% mutate(description = c(&quot;Two food items on right and no partner&quot;, &quot;Two food items on left and no partner&quot;, &quot;Two food items on right and partner present&quot;, &quot;Two food items on left and partner present&quot;))%&gt;% kable(caption = &quot;4つの実験条件&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.1: 4つの実験条件 condition prosoc_left description 0 0 Two food items on right and no partner 0 1 Two food items on left and no partner 1 0 Two food items on right and partner present 1 1 Two food items on left and partner present パッと見た限りでは、相手がいるときといないときで向社会的な選択をする割合に大きな違いはなさそう。 d %&gt;% group_by(condition, prosoc_left) %&gt;% summarise(prop = mean(pulled_left), se = sd(pulled_left)/n()) %&gt;% mutate(condition = ifelse(condition ==&quot;0&quot;,&quot;no partner&quot;,&quot;partner&quot;), prosoc_left = ifelse(prosoc_left==&quot;0&quot;,&quot;right&quot;,&quot;left&quot;)) %&gt;% ggplot(aes(x=prosoc_left, y = prop))+ geom_col(aes(fill = condition), position=position_dodge(0.95))+ geom_errorbar(aes(ymin = prop-se,ymax=prop+se, fill = condition), position=position_dodge(0.95), width = 0.2)+ theme(aspect.ratio=1)+ labs(fill=&quot;condition&quot;,x = &quot;which was prosocial&quot;, y = &quot;proportion of choosing left&quot;) 図11.1: 実験データの図示 これまで同様、ダミー変数は使わずindicator variableを用いる。 d &lt;- d %&gt;% mutate(treatment = factor(1+prosoc_left+2*condition)) %&gt;% mutate(labels = factor(treatment, levels = 1:4, labels = c(&quot;r/n&quot;, &quot;l/n&quot;, &quot;r/p&quot;, &quot;l/p&quot;))) d %&gt;% count(condition,prosoc_left, treatment) %&gt;% kable(caption = &quot;各実験条件のサンプル数&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.2: 各実験条件のサンプル数 condition prosoc_left treatment n 0 0 1 126 0 1 2 126 1 0 3 126 1 1 4 126 今回のモデル式は以下の通り。 \\[ \\begin{aligned} L_{i} &amp;\\sim Binomial(1,p_{i})\\\\ logit(p_{i}) &amp;= \\alpha_{ACTOR[i]} + \\beta_{TREATMENT_{i}}\\\\ \\alpha_{j} &amp;\\sim to \\ be \\ determined\\\\ \\beta_{k} &amp;\\sim to \\ be \\ determined \\end{aligned} \\] 事前分布はどのようにするのが適切だろうか。まずは、切片だけしかないモデルを考えて試してみる。 \\[ \\begin{aligned} L_{i} &amp;\\sim Binomial(1,p_{i})\\\\ logit(p_{i}) &amp;= \\alpha\\\\ \\alpha &amp;\\sim Normal(0,10) \\end{aligned} \\] b11.1 &lt;- brm(data = d, ## family = &quot;bernoulli&quot;でも可 family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 10), class = Intercept), seed = 11, sample_prior = TRUE, file = &quot;output/Chapter11/b11.1&quot;) 標準偏差が10のとき、事前分布は0と1の近くに集中していることが分かる。これは、奇妙なことである。 今度は、標準偏差が1.5の場合を考える。今度は、うまく全体に広がっていることが分かる。 b11.1b &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 1.5), class = Intercept), seed = 11, sample_prior = TRUE, file = &quot;output/Chapter11/b11.1b&quot;) bind_rows(prior_samples(b11.1), prior_samples(b11.1b)) %&gt;% mutate(p = inv_logit_scaled(Intercept), w = factor(rep(c(10, 1.5), each = n() / 2), levels = c(10, 1.5))) %&gt;% ggplot(aes(x=p, fill =w))+ geom_density(alpha=3/4, adjust =0.1, size=0, color = NA)+ scale_fill_manual(expression(italic(w)), values = c(&quot;grey&quot;, &quot;lightblue&quot;))+ labs(title = expression(alpha%~%Normal(0*&quot;, &quot;*italic(w)))) 図11.2: 標準偏差による切片の事前分布からのサンプリングの違い それでは、標準偏差1.5でモデルを回してみる。まず、treatmentのみを説明変数に入れるモデルを考える。説明変数の係数の事前分布としては、ひとまず平均0、標準偏差10と0.5の正規分布を考える。なお、brmsの事前分布の指定は単に事後分布を得るだけであればよりシンプルにかけるが、すべての条件について事前分布からのサンプルを得たければ、下記のように書く必要がある。 If all you want to do is fit the models, you wouldn’t have to add a separate prior() statement for each level of treatment. You could have just included a single line, prior(normal(0, 0.5), nlpar = b), that did not include a coef argument. The problem with this approach is we’d only get one column for treatment when using the prior_samples() function to retrieve the prior samples. To get separate columns for the prior samples of each of the levels of treatment, you need to take the verbose approach, above. # w = 10 b11.2 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1, b ~ 0+ treatment, nl = TRUE), prior=c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 10), nlpar = b, coef = &quot;treatment1&quot;), prior(normal(0, 10), nlpar = b, coef = &quot;treatment2&quot;), prior(normal(0, 10), nlpar = b, coef = &quot;treatment3&quot;), prior(normal(0, 10), nlpar = b, coef = &quot;treatment4&quot;)), seed = 11, sample_prior = TRUE, file = &quot;output/Chapter11/b11.2&quot;) # w = 0.5 b11.3 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1, b ~ 0+treatment, nl = TRUE), prior=c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b, coef = &quot;treatment1&quot;), prior(normal(0, 0.5), nlpar = b, coef = &quot;treatment2&quot;), prior(normal(0, 0.5), nlpar = b, coef = &quot;treatment3&quot;), prior(normal(0, 0.5), nlpar = b, coef = &quot;treatment4&quot;)), seed = 11, sample_prior = TRUE, file = &quot;output/Chapter11/b11.3&quot;) treatmentが1と2のときの違いを可視化してみる。標準偏差が10のときは、やはり0と1のところに集中していて考えられにくい。一方、0.5の場合はうまく低い値に集まっている。 prior &lt;- bind_rows(prior_samples(b11.2), prior_samples(b11.3)) %&gt;% mutate(w= factor(rep(c(10, 0.5), each = n() / 2), levels = c(10, 0.5)), p1 = inv_logit_scaled(b_a + b_b_treatment1), p2 = inv_logit_scaled(b_a + b_b_treatment2)) %&gt;% mutate(diff = abs(p1 - p2)) prior %&gt;% ggplot(aes(x=diff, fill = w))+ geom_density(size=0, alpha=3/4, adjust = 0.1, color=NA)+ scale_fill_manual(expression(italic(w)), values = c(&quot;lightsalmon4&quot;,&quot;orange&quot;))+ labs(title = expression(alpha%~%Normal(0*&quot;, &quot;*italic(w))), x = &quot;prior diff between treatments&quot;) 図11.3: 標準偏差による事前分布からのサンプリングの違い #平均 prior %&gt;% group_by(w) %&gt;% summarise(mean = mean(diff)) %&gt;% mutate(mean = round(mean,2)) %&gt;% kable(caption = &quot;標準偏差が10と0.5のときのtreatment1と2の違いの平均&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.3: 標準偏差が10と0.5のときのtreatment1と2の違いの平均 w mean 10 0.48 0.5 0.10 ちなみに、モデルによるtreatment1~4の平均は以下の通り。 posterior_samples(b11.3) %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(treatment = str_remove(name, &quot;b_b_treatment&quot;), mean= inv_logit_scaled(b_a_Intercept + value)) %&gt;% group_by(treatment) %&gt;% mean_qi(mean) %&gt;% kable(digits =2, caption = &quot;モデルb11.3の結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.4: モデルb11.3の結果 treatment mean .lower .upper .width .point .interval 1 0.55 0.47 0.63 0.95 mean qi 2 0.65 0.57 0.73 0.95 mean qi 3 0.49 0.41 0.57 0.95 mean qi 4 0.63 0.55 0.71 0.95 mean qi 個体名も含めたモデルを回す。 d %&gt;% mutate(actor = factor(actor))-&gt;d b11.4 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b , a ~ 0+ actor, b ~ 0+ treatment, nl = TRUE), prior=c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b)), seed = 11, sample_prior = TRUE, file = &quot;output/Chapter11/b11.4&quot;) posterior_summary(b11.4) %&gt;% data.frame() %&gt;% rownames_to_column(var=&quot;parameters&quot;) %&gt;% filter(parameters !=&quot;lp__&quot;, parameters !=&quot;prior_b_a&quot;, parameters !=&quot;prior_b_b&quot;) %&gt;% kable(digits=2, caption = &quot;モデルb11.4の結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.5: モデルb11.4の結果 parameters Estimate Est.Error Q2.5 Q97.5 b_a_actor1 -0.46 0.32 -1.09 0.16 b_a_actor2 3.89 0.74 2.60 5.46 b_a_actor3 -0.76 0.33 -1.42 -0.11 b_a_actor4 -0.76 0.33 -1.39 -0.12 b_a_actor5 -0.47 0.33 -1.10 0.19 b_a_actor6 0.47 0.33 -0.17 1.09 b_a_actor7 1.94 0.43 1.15 2.80 b_b_treatment1 -0.03 0.28 -0.56 0.52 b_b_treatment2 0.49 0.29 -0.07 1.05 b_b_treatment3 -0.37 0.29 -0.94 0.18 b_b_treatment4 0.38 0.28 -0.17 0.94 lprior -16.75 1.65 -20.48 -14.05 まずは、各個体の左を引く確率をtreatmentの効果を考えずに見てみる。個体ごとの左右の選好にかなり違いがあることが分かる。特に個体2はほとんど左しか選択していない。 posterior_samples(b11.4) %&gt;% pivot_longer(contains(&quot;actor&quot;))%&gt;% mutate(probability = inv_logit_scaled(value), actor = factor(str_remove(name, &quot;b_a_actor&quot;), levels = 7:1)) %&gt;% ggplot(aes(x=probability, y = actor))+ geom_vline(xintercept=0.5, linetype=3)+ stat_pointinterval(.width = .95, size = 1)+ scale_x_continuous(limits = 0:1)+ ylab(NULL) + xlab(expression(alpha))+ theme(axis.ticks.y = element_blank()) 図11.4: モデルによって推定された各個体が左を選択する確率。エラーバーは95%信用区間を表す。 次は、個体のばらつきの影響を除いたうえで、各treatmentにおける選択確率についてみてみる。 tx &lt;- c(&quot;R/N&quot;, &quot;L/N&quot;, &quot;R/P&quot;, &quot;L/P&quot;) posterior_samples(b11.4) %&gt;% dplyr::select(contains(&quot;treatment&quot;)) %&gt;% set_names(&quot;R/N&quot;,&quot;L/N&quot;,&quot;R/P&quot;,&quot;L/P&quot;) %&gt;% pivot_longer(everything())%&gt;% mutate(probability = inv_logit_scaled(value), treatment = factor(name, levels = tx)) %&gt;% mutate(treatment = fct_rev(treatment)) %&gt;% ggplot(aes(x=probability, y = treatment))+ geom_vline(xintercept=0.5, linetype=3)+ stat_pointinterval(.width = .95, linewidth = 1)+ scale_x_continuous(limits = 0:1)+ ylab(NULL) + xlab(expression(beta))+ theme(axis.ticks.y = element_blank()) 図11.5: モデルによって推定された各条件で左が選択される確率。エラーバーは95%信用区間を表す。 最後に、treatment1と3、treatment2と4の差を見ることで、相手がいるときといないときで向社会的選択をする割合に違いがみられたか調べる。 db13は、右側が向社会的選択のときに、相手がいないときよりいるときに左側を選択することがどれだけ多くなるかを示している(ただし、係数の大小を比較しているだけということに注意)。わずかに左を選択することが多くなっているが、そこまで大きな効果はない。 db14は逆に左側が向社会的選択のときに、相手がいないときよりいるときに左側を選択することがどれだけ多くなるかを示している。今度は、むしろいないときの方がわずかだが向社会的選択をしていたことを示している。 posterior_samples(b11.4) %&gt;% mutate(db13 = b_b_treatment1 - b_b_treatment3, db24 = b_b_treatment2 - b_b_treatment4) %&gt;% pivot_longer(db13:db24) %&gt;% mutate(diffs = factor(name, levels = c(&quot;db24&quot;, &quot;db13&quot;)))%&gt;% ggplot(aes(x = value, y = diffs)) + geom_vline(xintercept = 0, linetype=3)+ stat_pointinterval(.width = .95, linewidth = 1)+ scale_x_continuous(breaks = seq(-0.5,1,0.5))+ ylab(NULL) + xlab(&quot;difference&quot;)+ theme(axis.ticks.y = element_blank()) 図11.6: 相手がいないときと相手がいるときの係数の推定値の差。上は右側が向社会的選択の場合、下は左側が向社会的選択の場合を示している。 最後に、実際のデータとモデルによる予測結果を図示する。 nd &lt;- d %&gt;% distinct(actor, treatment, labels, condition, prosoc_left) d %&gt;% group_by(actor, labels) %&gt;% summarise(prop = mean(pulled_left)) %&gt;% left_join(nd, by=c(&quot;actor&quot;,&quot;labels&quot;)) %&gt;% mutate(condition = factor(condition)) %&gt;% ggplot(aes(x=labels, y =prop ))+ geom_hline(yintercept=0.5, linetype=3)+ geom_line(aes(group=prosoc_left),size=1/4,color=&quot;navy&quot;)+ geom_point(aes(shape=condition), size=2.5, show.legend=FALSE, color=&quot;navy&quot;)+ scale_shape_manual(values=c(1,16))+ labs(subtitle = &quot;observed proportions&quot;) -&gt; p1 fitted(b11.4, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(condition = factor(condition)) %&gt;% ggplot(aes(x=labels, y =Estimate))+ geom_hline(yintercept=0.5, linetype=3)+ geom_line(aes(group=prosoc_left),size=1/4,color=&quot;black&quot;)+ geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5, shape=condition), fatten=2, show.legend=FALSE, color=&quot;black&quot;)+ scale_shape_manual(values=c(1,16))+ labs(subtitle = &quot;posterior predictions&quot;) -&gt;p2 (p1/p2)&amp; scale_y_continuous(&quot;proportion left lever&quot;, breaks = c(0, .5, 1), limits = c(0, 1))&amp; xlab(NULL) &amp; facet_wrap(~ actor, nrow = 1, labeller = label_both)&amp; theme(axis.ticks.x = element_blank(), strip.background = element_blank()) 図11.7: 実際のデータとモデルによる予測結果。白抜きは相手がいない条件を、塗りつぶしは相手がいる条件を表す。下図のエラーバーは95%信用区間を表す。 なお、モデルは相手の有無と向社会的選択の場所(左右)を別々の変数としてモデリングすることもできる。 d %&gt;% mutate(side = factor(prosoc_left +1), cond = factor(condition+1), actor=factor(actor)) -&gt; d b11.5 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + bs + bc, a ~ 0 + actor, bs ~ 0 + side, bc ~ 0 + cond, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = bs), prior(normal(0, 0.5), nlpar = bc)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.5&quot;) 比較すると、ほとんど変わらないことがある。すなわち、交互作用は予測をほとんど向上させていないことが分かる。 b11.4 &lt;- add_criterion(b11.4, c(&quot;waic&quot;,&quot;loo&quot;)) b11.5 &lt;- add_criterion(b11.5, c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b11.4, b11.5) %&gt;% print(simplify=F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.5 0.0 0.0 -265.5 9.6 7.9 0.4 531.0 19.2 ## b11.4 -0.6 0.6 -266.1 9.5 8.5 0.4 532.2 18.9 ちなみに図示してもほとんど違いがないことが分かる。 nd &lt;- d %&gt;% distinct(actor, treatment, labels, cond, side) fitted(b11.5, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(condition = factor(cond)) %&gt;% ggplot(aes(x=labels, y =Estimate))+ geom_hline(yintercept=0.5, linetype=3)+ geom_line(aes(group=side),size=1/4,color=&quot;black&quot;)+ geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5, shape=condition), fatten=2, show.legend=FALSE, color=&quot;black&quot;)+ scale_shape_manual(values=c(1,16))+ labs()+ facet_wrap(~actor, nrow=1)+ scale_y_continuous(&quot;proportion left lever&quot;, breaks = c(0, .5, 1), limits = c(0, 1))+ xlab(NULL)+ theme(axis.ticks.x = element_blank(), strip.background = element_blank()) 図11.8: 交互作用がないモデルでの予測値。白抜きは相手がいない条件を、塗りつぶしは相手がいる条件を表す。下図のエラーバーは95%信用区間を表す。 11.1.1.1 overthinking brms()パッケージでは、log_lik()を実行すれば対数尤度を得ることができる。 11.1.2 Relative shark and absolute deer 先ほどは説明変数が左を選択する確率にどのような影響を与えるかというabsolute effectに着目した。一方で、relative effect(相対的な影響)に着目することもできる。例えば、比例オッズ(proportional odds)を見ることで、相対的な効果について論じることができる。 先ほどの例でいえば、treatmentが2から4に変わったときの比例オッズ比は、0.93。 posterior_samples(b11.4) %&gt;% mutate(proportional_odds =exp(b_b_treatment4 - b_b_treatment2)) %&gt;% mean_qi(proportional_odds) %&gt;% kable(caption = &quot;treatmentが2から4に変わったときの比例オッズ比&quot;, booktabs=TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.6: treatmentが2から4に変わったときの比例オッズ比 proportional_odds .lower .upper .width .point .interval 0.9288517 0.5182757 1.521362 0.95 mean qi これは、オッズが8%減少することに相当する。ただし、比例オッズが大きい(小さい)かによって結果に大きな影響を及ぼすとは限らない点は注意が必要である。例えば、10000人に1人の割合でかかる病気になるオッズが5倍になったとしても、4人増えるだけなので大きな効果があるとは言えない。 一方で、因果推論をする場合には相対的な効果が重要な場合もある。例えば、サメとシカを比べると実際はシカの方がヒトが死亡する確率を上昇させている(絶対的な効果)。しかし、海にいるときで条件づけるならば、サメはシカよりもはるかに危険である(相対的な効果)。目的によってこれらを使い分ける必要がある。 11.1.3 Aggregated binomial: Chimpanzees again, condensed 先ほどのデータは、引く順番を気にしなければ、各説明変数の組み合わせごとに何回左のレバーを引いたかというデータに変換することもできる。 d %&gt;% group_by(actor, treatment, side, cond) %&gt;% summarise(sum = sum(pulled_left), trial = n()) -&gt; d2 kable(d2, caption = &quot;各個体が各treatmentで引いた合計回数&quot;, booktabs=TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.7: 各個体が各treatmentで引いた合計回数 actor treatment side cond sum trial 1 1 1 1 6 18 1 2 2 1 9 18 1 3 1 2 5 18 1 4 2 2 10 18 2 1 1 1 18 18 2 2 2 1 18 18 2 3 1 2 18 18 2 4 2 2 18 18 3 1 1 1 5 18 3 2 2 1 11 18 3 3 1 2 3 18 3 4 2 2 6 18 4 1 1 1 6 18 4 2 2 1 9 18 4 3 1 2 2 18 4 4 2 2 8 18 5 1 1 1 6 18 5 2 2 1 10 18 5 3 1 2 5 18 5 4 2 2 9 18 6 1 1 1 14 18 6 2 2 1 11 18 6 3 1 2 10 18 6 4 2 2 11 18 7 1 1 1 14 18 7 2 2 1 15 18 7 3 1 2 17 18 7 4 2 2 18 18 b11.4は以下のように書き換えることができる。 b11.6 &lt;- brm(data = d2, family = binomial, bf(sum | trials(trial) ~ a + b , a ~ 0+ actor, b ~ 0+ treatment, nl = TRUE), prior=c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b)), seed = 11, sample_prior = TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.6&quot;) 推定結果はほとんど変わらないことが分かる。 text &lt;- tibble(value = c(1.4, 2.6), name = &quot;b_a_actor7&quot;, fit = c(&quot;b11.6&quot;, &quot;b11.4&quot;)) bind_rows(posterior_samples(b11.4), posterior_samples(b11.6)) %&gt;% mutate(fit = rep(c(&quot;b11.4&quot;, &quot;b11.6&quot;), each = n() / 2)) %&gt;% pivot_longer(b_a_actor1:b_b_treatment4) %&gt;% ggplot(aes(x = value, y = name, color = fit)) + stat_pointinterval(.width = .95, size = 2/3, position = position_dodge(width = 0.5)) + scale_color_manual(values = c(&quot;orange2&quot;,&quot;deepskyblue4&quot;)) + geom_text(data = text, aes(label = fit), position = position_dodge(width = 2.25)) + labs(x = &quot;posterior (log-odds scale)&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) 図11.9: b11.4とb11.6の比較 データ数が違うので、PSISやWAICを比べることはできない。 b11.6 &lt;- add_criterion(b11.6, c(&quot;waic&quot;,&quot;loo&quot;)) loo_compare(b11.4, b11.6) また、今回のモデルでは\\(pareto-k\\)についてエラーメッセージが出ている。これはb11.6ではよりデータを集約しているため、LOOCVのときにより多くのデータ(今回は18個)を一度に抜いているようなものだからである。これにより、一部のデータがより影響力を持つようになってしまう。よって、PSISを計算するならば0/1のデータについてモデリングする方が望ましい。 11.2 Aggregated binomial: Graduate school admittions. 先ほどは毎回同じ試行回数(18)だったが、そうでない場合もある。以下では、UCバークレーの6つの研究科の入試における合否データを用いる。 data(UCBadmit) d3 &lt;- UCBadmit str(d3) ## &#39;data.frame&#39;: 12 obs. of 5 variables: ## $ dept : Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 1 1 2 2 3 3 4 4 5 5 ... ## $ applicant.gender: Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 1 2 1 2 1 2 1 ... ## $ admit : int 512 89 353 17 120 202 138 131 53 94 ... ## $ reject : int 313 19 207 8 205 391 279 244 138 299 ... ## $ applications : int 825 108 560 25 325 593 417 375 191 393 ... 性別によって合格率に差があるかを見たい。 \\[ \\begin{aligned} A_{i} &amp;\\sim Binomial(N_{i}, p_{i})\\\\ logit(p_{i}) &amp;= \\alpha_{GID{i}}\\\\ \\alpha_{j} &amp;\\sim Normal(0,1.5) \\end{aligned} \\] モデルを回す。 d3 %&gt;% mutate(gid = factor(applicant.gender, levels=c(&quot;male&quot;,&quot;female&quot;)), case = factor(1:n())) -&gt; d3 b11.7 &lt;- brm(data = d3, family = binomial, admit | trials(applications) ~ 0 + gid, prior= prior(normal(0, 1.5), class = b), seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.7&quot;) 男の方が合格率が高い。 posterior_summary(b11.7) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters !=&quot;lp__&quot;) %&gt;% kable(digits =2, caption = &quot;モデルb11.7の結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.8: モデルb11.7の結果 parameters Estimate Est.Error Q2.5 Q97.5 b_gidmale -0.22 0.04 -0.30 -0.14 b_gidfemale -0.83 0.05 -0.93 -0.73 lprior -2.81 0.02 -2.85 -2.78 対数オッズと確率の差の事後分布をみてみると、やはりどちらも男の方が高い。 posterior_samples(b11.7) %&gt;% dplyr::select(-lp__) %&gt;% mutate(diff_a = b_gidmale - b_gidfemale, diff_p = inv_logit_scaled(b_gidmale)- inv_logit_scaled(b_gidfemale)) %&gt;% pivot_longer(contains(&quot;diff&quot;)) %&gt;% group_by(name) %&gt;% mean_qi(value, .width=.89) %&gt;% kable(digits =2, caption = &quot;男女における対数オッズと確率の差&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.9: 男女における対数オッズと確率の差 name value .lower .upper .width .point .interval diff_a 0.61 0.51 0.72 0.89 mean qi diff_p 0.14 0.12 0.16 0.89 mean qi モデルの結果を図示する。実際には2つの研究科でのみ男性の合格率が高くなく、モデルの推定結果はデータとかなりずれていることが分かる。 nd &lt;- crossing(dept = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;), gid = factor(c(&quot;male&quot;,&quot;female&quot;), levels = c(&quot;male&quot;,&quot;female&quot;))) %&gt;% mutate(applications = d3$applications) fitted(b11.7, newdata = nd) %&gt;% data.frame() %&gt;% dplyr::select(-Est.Error) %&gt;% set_names(&quot;ci_mean&quot;, &quot;ci_l&quot;,&quot;ci_u&quot;) %&gt;% bind_cols(nd) %&gt;% mutate(case = 1:n()) -&gt; fit11.7 text &lt;- d3 %&gt;% group_by(dept) %&gt;% summarise(case = mean(as.numeric(case)), admit = mean(admit / applications) + .05) #plot d3 %&gt;% ggplot(aes(x=case))+ geom_point(aes(y = admit/applications), color = &quot;navy&quot;, shape=16)+ geom_line(aes(y = admit/applications,group=dept), color = &quot;navy&quot;, size=0.7)+ geom_text(data = text, aes(y = admit, label = dept))+ geom_pointinterval(data = fit11.7, aes(y = ci_mean/applications, ymin = ci_l/applications, ymax = ci_u/applications), shape = 1, size=4)+ scale_y_continuous(&quot;Proportion admitted&quot;, limits = 0:1) + ggtitle(&quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) 図11.10: 実測値とモデルb11.7による予測値。青が実測値、黒がモデルによる予測値を表す。 これは、研究科ごとに合格率に大きな違いがあり、女性は合格率の低い研究科に応募する割合が多いからである。 d3 %&gt;% group_by(dept) %&gt;% mutate(prop = applications/sum(applications)) %&gt;% dplyr::select(dept, gid, prop) %&gt;% pivot_wider(names_from = dept, values_from = prop) %&gt;% kable(digits=2, caption=&quot;研究科ごとの応募者の男女比&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.10: 研究科ごとの応募者の男女比 gid A B C D E F male 0.88 0.96 0.35 0.53 0.33 0.52 female 0.12 0.04 0.65 0.47 0.67 0.48 そこで今度は、研究科による合格率の違いを考慮に入れたモデルを考える。 \\[ \\begin{aligned} A_{i} &amp;\\sim Binomial(N_{i}, p_{i})\\\\ logit(p_{i}) &amp;= \\alpha_{GID{i}} + \\delta_{DEPT_{i}}\\\\ \\alpha_{j} &amp;\\sim Normal(0,1.5)\\\\ \\delta_{j} &amp;\\sim Normal(0,1.5) \\end{aligned} \\] b11.8 &lt;- brm(data = d3, family = binomial, bf(admit | trials(applications) ~ 0 + a + d, a ~ 0 + gid, d ~ 0 + dept, nl = TRUE), prior= c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 1.5), nlpar = d)), seed = 11,iter = 4000, warmup=1000, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.8&quot;) 結果を見てみると、男女で違いはないことが分かる。むしろ女性の方が少し合格率は高い。 posterior_summary(b11.8) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameters&quot;) %&gt;% filter(parameters!=&quot;lp__&quot;) %&gt;% kable(digits=2, caption=&quot;モデルb11.8の結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.11: モデルb11.8の結果 parameters Estimate Est.Error Q2.5 Q97.5 b_a_gidmale -0.54 0.51 -1.57 0.46 b_a_gidfemale -0.44 0.52 -1.48 0.56 b_d_deptA 1.12 0.52 0.12 2.16 b_d_deptB 1.08 0.52 0.08 2.12 b_d_deptC -0.14 0.52 -1.14 0.89 b_d_deptD -0.17 0.52 -1.17 0.87 b_d_deptE -0.61 0.52 -1.63 0.43 b_d_deptF -2.17 0.53 -3.20 -1.10 lprior -12.86 0.68 -14.67 -12.19 posterior_samples(b11.8) %&gt;% mutate(diff_a = b_a_gidmale - b_a_gidfemale, diff_p = inv_logit_scaled(b_a_gidmale) - inv_logit_scaled(b_a_gidfemale)) %&gt;% pivot_longer(contains(&quot;diff&quot;)) %&gt;% group_by(name) %&gt;% mean_qi(value, .width=.89) %&gt;% kable(booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) name value .lower .upper .width .point .interval diff_a -0.0975503 -0.2277221 0.0324701 0.89 mean qi diff_p -0.0218389 -0.0520788 0.0071217 0.89 mean qi おそらく、因果関係をグラフィカルモデルで表すと以下のようになる。 library(ggdag) dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;), x = c(1, 2, 3), y = c(1, 2, 1)) dagify(D ~ G, A ~ D + G, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges(edge_color = &quot;black&quot;) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme_bw() 図11.11: 性別と研究科による合格率の違いについての因果関係。Gは性別、Dは研究科、Aは合格率を表す。 今度は実測値とモデルの予測値はよくあっていることが分かる。 fitted(b11.8, newdata = nd) %&gt;% data.frame() %&gt;% dplyr::select(-Est.Error) %&gt;% set_names(&quot;ci_mean&quot;, &quot;ci_l&quot;,&quot;ci_u&quot;) %&gt;% bind_cols(nd) %&gt;% mutate(case = 1:n()) -&gt; fit11.8 #plot d3 %&gt;% ggplot(aes(x=case))+ geom_point(aes(y = admit/applications), color = &quot;navy&quot;, shape=16)+ geom_line(aes(y = admit/applications,group=dept), color = &quot;navy&quot;, size=0.7)+ geom_text(data = text, aes(y = admit, label = dept))+ geom_pointinterval(data = fit11.8, aes(y = ci_mean/applications, ymin = ci_l/applications, ymax = ci_u/applications), color = &quot;lightblue&quot;, shape = 1, size=5, alpha = 2/3)+ scale_y_continuous(&quot;Proportion admitted&quot;, limits = 0:1) + ggtitle(&quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) 図11.12: 実測値とモデルによる予測値。青が実測値、水色がモデルによる予測値を表す。 しかし、これで性別が合格率に与える効果を正確に評価できたかは分からない。未観測の要因が関わっている可能性があるからである。例えば、もし共通の要因が研究科の選択と合格率の両方に影響を与えているとき（cf. 図11.13）、研究科で条件づけてしまうと性別と合格率の間に因果関係ではないパスが生じてしまう。 dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;, &quot;U&quot;), x = c(1, 2, 3, 3), y = c(1, 2, 1, 2)) dagify(D ~ G + U, A ~ D + G + U, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_point(x = 3, y = 2, size = 5, color = &quot;orange&quot;) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges(edge_color = &quot;black&quot;) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme_bw() 図11.13: 性別と研究科による合格率の違いについての因果関係。Gは性別、Dは研究科、Aは合格率を表す。 ’b11.8’はover-parametalizedである(どちらかの性別をベースラインにしても同じ推定結果は得られる)。しかし、この方法は一方の性だけに事前分布が適用されることを防ぐことができるという利点がある。一方でパラメータ間の相関は強くなるので、それに注意が必要である。 11.3 Poisson regression 二項分布はnが大きく、pが小さいときに平均と分散が同じ値に収束する。例えば\\(n=100000\\)、\\(p=0.001\\)とときを考える。 n &lt;- 1e5 p &lt;- 1e-3 y &lt;- rbinom(n,1000,p) c(mean(y),var(y)) ## [1] 1.001700 1.003667 このようなとき、二項分布はポワソン分布に近づくことが知られており、試行回数Nが未知であったり大きいと考えられるときに有用である。ポワソン回帰では、通常log関数がリンク関数に用いられる。 11.3.1 Oceanic tool complexity オセアニアにおける道具の地域差を調べる。理論的には、人口の多い地域の方が複雑な道具が開発されていると予想される。また集団間の交流が盛んなほど人口が多く、技術的進化が起こりやすいと予想される。Kline and Boyd (2010) のデータを分析していく。 data(Kline) d4 &lt;- Kline kable(d4, caption = &quot;オセアニアにおける道具数の地域差&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.12: オセアニアにおける道具数の地域差 culture population contact total_tools mean_TU Malekula 1100 low 13 3.2 Tikopia 1500 low 22 4.7 Santa Cruz 3600 low 24 4.0 Yap 4791 high 43 5.0 Lau Fiji 7400 high 33 5.0 Trobriand 8000 high 19 4.0 Chuuk 9200 high 40 3.8 Manus 13000 low 28 6.6 Tonga 17500 high 55 5.4 Hawaii 275000 low 71 6.6 # 変数変換 d4 %&gt;% mutate(P = standardize(log(population)), cid = contact) -&gt; d4 考えるのは人口と他地域との交流の交互作用を考えた以下のモデルである。 \\[ \\begin{aligned} T_{i} &amp;\\sim Poisson(\\lambda_{i})\\\\ log \\lambda_{i} &amp;= \\alpha_{CID_{[i]}} + \\beta_{CID_{[i]}} logP_{i}\\\\ \\alpha_{j} &amp;\\sim to\\ be\\ determined\\\\ \\beta_{j} &amp;\\sim to\\ be\\ determined\\\\ \\end{aligned} \\] どのような事前分布が適切だろうか。まず、切片のみのモデルを考え、切片の事前分布を平均0、標準偏差10の正規分布と平均3、あるいは標準偏差0.5の正規分布とする。 \\(Normal(0,10)\\)のとき、事前分布は0に集まっており、右に裾が長いことが分かる。これは明らかに不適当である。一方、\\(Normal(3,0.5)\\)はある程度広い範囲に広がっていることが分かる。 tibble(x = c(3, 22), y = c(0.055, 0.04), meanlog = c(0, 3), sdlog = c(10, 0.5)) %&gt;% tidyr::expand(nesting(x, y, meanlog, sdlog), number = seq(from = 0, to = 100, length.out = 200)) %&gt;% mutate(density = dlnorm(number, meanlog, sdlog), group = str_c(&quot;alpha%~%Normal(&quot;, meanlog, &quot;, &quot;, sdlog, &quot;)&quot;)) %&gt;% ggplot(aes(fill = group, color = group)) + geom_area(aes(x = number, y = density), alpha = 3/4, size = 0, position = &quot;identity&quot;) + geom_text(data = . %&gt;% group_by(group) %&gt;% slice(1), aes(x = x, y = y, label = group), parse = TRUE, hjust = 0) + scale_fill_manual(values = c(&quot;grey&quot;, &quot;orange&quot;)) + scale_color_manual(values = c(&quot;grey&quot;, &quot;orange&quot;)) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;mean number of tools&quot;) + theme(legend.position = &quot;none&quot;) 図11.14: 切片の事前分布の比較。 それでは、\\(\\beta\\)の事前分布はどうすればよいだろうか。\\(Normal(0,10\\)と\\(Normal(0,0.2)\\)の場合を考える。\\(Normal(0,10)\\)のときには\\(log(P)\\)が0付近のときに道具数が極端に大きくなっていることが分かり、明らかに不適切。一方で\\(Normal(0,0.2)\\)のときはうまく様々な傾きを表現できている。 n &lt;- 100 set.seed(123) tibble(i = 1:n, a = rnorm(n, mean = 3, sd = 0.5)) %&gt;% mutate(`beta%~%Normal(0*&#39;, &#39;*10)` = rnorm(n, mean = 0 , sd = 10), `beta%~%Normal(0*&#39;, &#39;*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) %&gt;% pivot_longer(contains(&quot;beta&quot;), values_to = &quot;b&quot;, names_to = &quot;prior&quot;) %&gt;% tidyr::expand(nesting(i, a, b, prior), x = seq(from = -2, to = 2, length.out = 100)) %&gt;% ggplot(aes(x = x, y = exp(a + b * x), group = i)) + geom_line(size = 1/4, alpha = 2/3, color = &quot;black&quot;) + labs(x = &quot;log population (std)&quot;, y = &quot;total tools&quot;) + coord_cartesian(ylim = c(0, 100)) + facet_wrap(~ prior, labeller = label_parsed)+ theme(strip.background = element_blank()) 図11.15: 異なる標準偏差を持つbetaの事前予測分布。 \\(Normal(0,0.2)\\)を標準化する前のスケールとさらにlogを外したときのスケールで見てみる。一部極端な曲線はあるものの、概ねいい感じの範囲に収まっているといえる。 set.seed(11) prior &lt;- tibble(i = 1:n, a = rnorm(n, mean = 3, sd = 0.5), b = rnorm(n, mean = 0, sd = 0.2)) %&gt;% tidyr::expand(nesting(i, a, b), x = seq(from = log(100), to = log(200000), length.out = 100)) # left p3 &lt;- prior %&gt;% ggplot(aes(x = x, y = exp(a + b * x), group = i)) + geom_line(size = 1/4, alpha = 2/3, color = &quot;black&quot;)+ labs(subtitle = expression(beta%~%Normal(0*&#39;, &#39;*0.2)), x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = c(log(100), log(200000)), ylim = c(0, 500)) p4 &lt;- prior %&gt;% ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) + geom_line(size = 1/4, alpha = 2/3, color = &quot;black&quot;) + labs(subtitle = expression(beta%~%Normal(0*&#39;, &#39;*0.2)), x = &quot;population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = c(100, 200000), ylim = c(0, 500)) p3|p4 図11.16: 人口をもとのスケールに戻すと… それでは、\\(\\alpha \\sim Normal(3,0.5)\\)、\\(\\beta \\sim Normal(0,0.2)\\)でモデルを回す。切片だけのモデルも一緒に回す。 b11.9 &lt;- brm( data = d4, family = poisson, formula = total_tools ~ 1, prior = prior(normal(3, 0.5), class=Intercept), seed=8, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.9&quot; ) b11.10 &lt;- brm( data = d4, family = poisson, formula = bf(total_tools ~ 0 + a + b * P, a ~ 0 + cid, b ~ 0 + cid, nl = TRUE), prior = c(prior(normal(3, 0.5), nlpar=a), prior(normal(0, 0.2), nlpar=b)), seed=8, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.10&quot; ) 両モデルを比較してみる。どちらもpareto-kの警告が出ていることに注意。当然ながらb11.10のほうがPSISが小さい。 b11.9 &lt;- add_criterion(b11.9, &quot;loo&quot;) b11.10 &lt;- add_criterion(b11.10, &quot;loo&quot;) loo_compare(b11.9, b11.10) %&gt;% print(simplify = FALSE) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.10 0.0 0.0 -42.4 6.6 6.7 2.6 84.8 13.1 ## b11.9 -28.7 17.1 -71.1 17.2 8.6 3.9 142.3 34.4 モデルb11.10では1点でpareto-kが0.7以上になっているようだ。 loo(b11.10) %&gt;% pareto_k_table() %&gt;% kable(caption = &quot;モデルb11.10のpareto-k診断結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.13: モデルb11.10のpareto-k診断結果 Count Proportion Min. n_eff (-Inf, 0.5] 6 0.6 1335.03581 (0.5, 0.7] 3 0.3 163.06339 (0.7, 1] 1 0.1 52.69866 (1, Inf) 0 0.0 NA もっと詳しく見てみると、ハワイが大きな影響を与えていることが分かる。 tibble(culture = d4$culture, k = b11.10$criteria$loo$diagnostics$pareto_k) %&gt;% arrange(desc(k)) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% kable(caption = &quot;それぞれの地域のpareto-k&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.14: それぞれの地域のpareto-k culture k Tonga 0.77 Hawaii 0.70 Yap 0.61 Trobriand 0.55 Malekula 0.49 Tikopia 0.37 Lau Fiji 0.31 Chuuk 0.27 Santa Cruz 0.26 Manus 0.16 それでは、pareto-kの値と共に結果を図示する。 cultures &lt;- c(&quot;Hawaii&quot;, &quot;Tonga&quot;, &quot;Trobriand&quot;, &quot;Yap&quot;) nd &lt;- crossing(cid = c(&quot;low&quot;,&quot;high&quot;), P = seq(-1.5, 2.5, length.out=100)) fit11.10 &lt;- fitted(b11.10, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) ## 標準化したスケール p5 &lt;- fit11.10 %&gt;% ggplot(aes(x = P, group = cid))+ geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, linetype = cid, fill = cid, color = cid), stat = &quot;identity&quot;, size=1.5, alpha=1/2)+ geom_point(data = bind_cols(d4, b11.10$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, color = cid),stroke = 1.5)+ scale_shape_manual(values = c(16,1))+ scale_color_manual(values = c(&quot;lightblue3&quot;,&quot;orange3&quot;))+ scale_fill_manual(values = c(&quot;lightblue&quot;, &quot;orange&quot;))+ geom_text_repel(data = bind_cols(d4, b11.10$criteria$loo$diagnostics) %&gt;% filter(culture %in% cultures) %&gt;% mutate(label = str_c(culture, &quot; (&quot;, round(pareto_k,2), &quot;)&quot;)), aes(y = total_tools, label = label), size=3, seed=11,color=&quot;black&quot;)+ labs(x = &quot;log population (std)&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(b11.10$data$P), ylim = c(0, 80))+ theme(legend.position = &quot;none&quot;, aspect.ratio=1, axis.title = element_text(family =&quot;Japan1GothicBBB&quot;)) ## もともとのスケールに戻す。 p6 &lt;- fit11.10 %&gt;% mutate(population = exp(P*sd(log(d4$population))+mean(log(d4$population)))) %&gt;% ggplot(aes(x = population, group = cid))+ geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, linetype = cid, fill = cid, color = cid), stat = &quot;identity&quot;, size=1.5, alpha=1/2)+ geom_point(data = bind_cols(d4, b11.10$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, color = cid),stroke = 1.5,alpha=1/2)+ scale_shape_manual(values = c(16,1))+ scale_color_manual(values = c(&quot;lightblue3&quot;,&quot;orange3&quot;))+ scale_fill_manual(values = c(&quot;lightblue&quot;, &quot;orange&quot;))+ scale_x_continuous(breaks = seq(0,250000,by=50000))+ coord_cartesian(xlim = c(0,270000), ylim = c(0, 80))+ theme(legend.position = &quot;none&quot;, aspect.ratio=1, axis.title = element_text(family =&quot;Japan1GothicBBB&quot;)) p5+p6 図11.17: オセアニア諸国の道具数に関するモデルb11.10の事後分布を用いた予測。水色は周辺諸国との交流頻度が高かった国を、オレンジは低かった国を表す。また、点の大きさはpareto-kの値を表す。 このモデルの予測値に従うと、他国との接触が多かった国は人口が多くなると、接触が少なかった国よりも道具数が少ないことになる。しかし、これはおかしな現象である。これが生じるのは、モデルが切片を自由なパラメータとしたからである。本来ならば人口が0ならば道具数も0でなければいけないが、b11.10はそのような制約を設けていない。 それでは、改良したモデルを考えてみよう。人口が増えるほど道具数も増えるが、その変化量は徐々に減っていくと考える。また、道具の数が多いほど新たに発明される道具の数も減ると考えられる。そこで、単位時間当たりの道具数の変化を以下のように定義する。なお、\\(P\\)は人口を、\\(T\\)は道具数を表す。 \\[ \\Delta T = \\alpha P^B - \\gamma T \\] \\(\\Delta T\\)が0になるとき、\\(T = \\frac{\\alpha P^\\beta}{\\gamma}\\)であることを利用し、以下のモデルを考える。 ここで、\\(\\alpha\\)と\\(\\beta\\)はCIDによって変化すると考える。 また、事後分布が正に限定されるよう、\\(\\alpha\\)については指数をとった。 \\[ \\begin{aligned} T_{i} &amp;\\sim Poisson(\\lambda_{i})\\\\ \\lambda_{i} &amp;= exp(\\alpha) P_{i}^\\beta/\\gamma \\end{aligned} \\] brmsで実装する。 b11.11 &lt;- brm(data = d4, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(a) * population^b / g, a + b ~ 0 + cid, g ~ 1, nl = TRUE), prior = c(prior(normal(1, 1), nlpar = a), prior(exponential(1), nlpar = b, lb = 0), prior(exponential(1), nlpar = g, lb = 0)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, control = list(adapt_delta = .95), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.11&quot;) 結果とPSISを確認する。先ほどのモデルと大きくは変わらないがより妥当なモデルになっている。 b11.11 &lt;- add_criterion(b11.11, criterion = &quot;loo&quot;, moment_match = T) loo_compare(b11.10, b11.11) %&gt;% print(simplify = FALSE) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.11 0.0 0.0 -40.7 6.0 5.3 1.9 81.3 11.9 ## b11.10 -1.7 2.8 -42.4 6.6 6.7 2.6 84.8 13.1 最後に結果を図示する。今度は(0,0)を通っていることに加えて、逆転は生じていない。 text &lt;- distinct(d4, cid) %&gt;% mutate(population = c(210000, 72500), total_tools = c(59, 68), label = str_c(cid, &quot; contact&quot;)) # redifine the new data nd &lt;- distinct(d4, cid) %&gt;% tidyr::expand(cid, population = seq(from = 0, to = 300000, length.out = 100)) # compute the poster predictions for lambda fitted(b11.11, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = population, group = cid))+ geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, linetype = cid, fill = cid, color = cid), stat = &quot;identity&quot;, size=1.5, alpha=1/2)+ geom_point(data = bind_cols(d4, b11.11$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, color = cid),stroke = 1.5, alpha = 1/2)+ scale_shape_manual(values = c(16,1))+ scale_color_manual(values = c(&quot;lightblue3&quot;,&quot;orange3&quot;))+ scale_fill_manual(values = c(&quot;lightblue&quot;, &quot;orange&quot;))+ scale_x_continuous(breaks = seq(0,250000,by=50000))+ coord_cartesian(xlim = c(0,250000), ylim = c(0, 80))+ theme(legend.position = &quot;none&quot;, aspect.ratio=1) 図11.18: オセアニア諸国の道具数に関するモデルb11.11の事後分布を用いた予測。水色は周辺諸国との交流頻度が高かった国を、オレンジは低かった国を表す。また、点の大きさはpareto-kの値を表す。 11.3.2 Negative binomial (Gamma-Poisson) models. ポワソン回帰では、ときどき過分散が生じることがありうる。このとき、負の二項分布(またはガンマポワソン分布)を用いるとうまくいくことがある。負の二項分布は異なるポワソン分布を混合したものである。 11.3.3 Example: Exposure and the offset オフセット項を用いることで割合データを扱うことができる。 例として、\\(\\lambda\\)がある事象の回数\\(\\mu\\)を時間または距離\\(\\tau\\)で割った値だとする。 \\[ \\begin{aligned} y_{i} &amp;\\sim Poisoon(\\lambda_{i})\\\\ log \\lambda_{i} &amp;= log \\frac{\\mu_{i}}{\\tau_{i}}= \\alpha + \\beta x_{i} \\end{aligned} \\] このとき、以下のように書くことができる。 \\[ \\begin{aligned} y_{i} &amp;\\sim Poisoon(\\mu_{i})\\\\ log \\mu_{i} &amp;= log \\tau_{i} + \\alpha + \\beta x_{i} \\end{aligned} \\] もちろん\\(log\\tau_{i}\\)にパラメータを書けることもできる。 教科書のシミュレーションデータに対してこのモデルを回してみる。2つの修道院があり、それぞれ何かの製品を作っているのだが、修道院Aでは1日当たりの製造個数を記録している一方で、修道院Bでは1週間当たりの製造個数しか記録していない。このとき、オフセット項を用いることでそれぞれの修道院の製造効率に違いがあるかを検討する。 set.seed(11) ## 修道院A架空データ num_days &lt;- 70 y &lt;- rpois(num_days, lambda = 1.5) ##修道院B架空データ num_weeks &lt;- 10 y_new &lt;- rpois(num_weeks, lambda = 0.5 * 7) ( d5 &lt;- tibble(y = c(y, y_new), days= rep(c(1, 7), times = c(num_days, num_weeks)), monastery = rep(c(&quot;A&quot;,&quot;B&quot;), times = c(num_days, num_weeks)))%&gt;% mutate(log_days = log(days)) ) ## # A tibble: 80 × 4 ## y days monastery log_days ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 1 A 0 ## 2 0 1 A 0 ## 3 1 1 A 0 ## 4 0 1 A 0 ## 5 0 1 A 0 ## 6 4 1 A 0 ## 7 0 1 A 0 ## 8 1 1 A 0 ## 9 3 1 A 0 ## 10 0 1 A 0 ## # … with 70 more rows モデルを回してみる。 b11.12 &lt;- brm(data = d5, family = poisson, y ~ 1 + offset(log_days) + monastery, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.12&quot;) それぞれ修道院Aと修道院Bの製造速度を比較すると…。 真の\\(\\lambda\\)(1.5と0.5)とは違っているものの、推定はできた。 posterior_samples(b11.12) %&gt;% mutate(lambda_A = exp(b_Intercept), lambda_B = exp(b_Intercept + b_monasteryB)) %&gt;% pivot_longer(contains(&quot;lambda&quot;)) %&gt;% group_by(name) %&gt;% mean_qi(value) %&gt;% kable(digits=2,booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) name value .lower .upper .width .point .interval lambda_A 1.08 0.84 1.32 0.95 mean qi lambda_B 0.47 0.32 0.64 0.95 mean qi 11.4 Multinomial and categorical models 3つ以上の事象が起きうるとき、最大エントロピー分布は多項分布になる。多項分布のリンク関数にはソフトマックス関数(長さKのベクトルを各要素が(0,1)の範囲をとり、合計1で長さKのベクトルに変換する関数)を使用することが多い。ソフトマックス関数は以下の式の通り。なお、\\(s_{1} \\ldots s_{K}\\)は各事象に対応する長さ\\(k\\)のベクトル\\(scores\\)を表す。 ある事象Kが生じる確率は、 \\[ Pr(k|s_{1},s_{2},s_{3},\\ldots, s_{K}) = \\frac{exp(s_{k})}{\\sum_{i=1}^K exp(s_{i})} \\] 多項回帰では、K種類の事象があるとき、K-1個の線形モデルを作成する。それぞれのモデルで用いる説明変数やパラメータは必ずしも同じでなくてよい。 多項ロジスティック回帰には大きく分けて二つの場合がある。 11.4.1 Predictors matched to outcomes 若者が職業を選択する際に期待される収入が影響しているかを調べたい。このとき、それぞれのモデルで共通のパラメータ\\(\\beta_{INCOME}\\)が用いられるが、それぞれのモデルで異なる説明変数の値(ここでは収入)がパラメータにかけられる。 ここでは、3つの職業の選択に関するシミュレーションデータを用いる。ここでは、収入に0.5をかけた値をscoreとし、scoreの値をsoftmax関数に与えることによって各カテゴリーの職業が選ばれる確率が得られるものとする。職業ごとに収入は決まっている。 n &lt;- 500 # number of individuals income &lt;- c(1, 2, 5) # expected income of each career score &lt;- 0.5 * income # scores for each career, based on income # next line converts scores to probabilities p &lt;- softmax(score[1], score[2], score[3]) # now simulate choice # outcome career holds event type values, not counts career &lt;- rep(NA, n) # empty vector of choices for each individual # sample chosen career for each individual set.seed(34302) # sample chosen career for each individual for(i in 1:n) career[i] &lt;- sample(1:3, size = 1, prob = p) シミュレーションによって各職業が選ばれた回数は図11.19の通り。 d6 &lt;- tibble(career = career) %&gt;% mutate(career_income = ifelse(career ==3,5,career)) d6 %&gt;% ggplot(aes(x=career))+ geom_bar(size=0, fill=&quot;orange2&quot;) 図11.19: シミュレーションデータで各職業が選ばれた回 それでは、モデリングを行う。まずは切片だけのモデルを交わしてみる。brmsでは、refcatで基準とするカテゴリーを指定する（しなければ最初のカテゴリーが0になる）。 b11.13io &lt;- brm(data = d6, family = categorical(link = logit, refcat = 3), career ~ 1, prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1), prior(normal(0, 1), class = Intercept, dpar = mu2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.13io&quot;) 基準にした3以外のカテゴリーのパラメータが結果として出力される(‘mu1’,‘mu2’)。 posterior_summary(b11.13io) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter != &quot;lp__&quot;) %&gt;% kable(digits=2, caption = &quot;b11.13ioの結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.15: b11.13ioの結果 parameter Estimate Est.Error Q2.5 Q97.5 b_mu1_Intercept -2.01 0.15 -2.32 -1.73 b_mu2_Intercept -1.53 0.12 -1.78 -1.29 lprior -5.05 0.38 -5.82 -4.36 ’mu1’と’mu2’はそれぞれ-2と-1.5だが、これは3つめのカテゴリーのscoreを0にしたときの、1つめと2つめのカテゴリーのscoreの値に一致する。つまり、モデルの推定結果の’mu1’と’mu2’はscoreの値を返している。 tibble(income = c(1, 2, 5)) %&gt;% mutate(score = 0.5 * income) %&gt;% mutate(rescaled_score = score - 2.5) %&gt;% kable(caption = &quot;元データのscore&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.16: 元データのscore income score rescaled_score 1 0.5 -2.0 2 1.0 -1.5 5 2.5 0.0 実際の確率はこれらをsoftmax関数に入れれば得られるので、モデルによって推定された確率は以下のように求められる。 posterior_samples(b11.13io) %&gt;% mutate(b_mu3_Intercept = 0) %&gt;% mutate(p1 = exp(b_mu1_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)), p2 = exp(b_mu2_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)), p3 = exp(b_mu3_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept))) %&gt;% pivot_longer(p1:p3) %&gt;% group_by(name) %&gt;% mean_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% kable(caption = &quot;モデルb11.13ioによって推定された各職業が選ばれる確率&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.17: モデルb11.13ioによって推定された各職業が選ばれる確率 name value .lower .upper .width .point .interval p1 0.10 0.08 0.13 0.95 mean qi p2 0.16 0.13 0.19 0.95 mean qi p3 0.74 0.70 0.78 0.95 mean qi これは、シミュレーションに使用した真の値とよく一致している。 tibble(income = c(1,2,5)) %&gt;% mutate(score =0.5*income) %&gt;% mutate(p = exp(score)/sum(exp(score))) ## # A tibble: 3 × 3 ## income score p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.5 0.0996 ## 2 2 1 0.164 ## 3 5 2.5 0.736 それでは、収入を説明変数に入れたモデルを回してみる。収入にかかる係数を職業ごとに変えるか、パラメータの範囲に制限を設けるかによって4種類のモデルが考えられる。 crossing(b = factor(c(&quot;b1 &amp; b2&quot;, &quot;b&quot;), levels = c(&quot;b1 &amp; b2&quot;, &quot;b&quot;)), lb = factor(c(&quot;NA&quot;, 0), levels = c(&quot;NA&quot;, 0))) %&gt;% mutate(fit = str_c(&quot;b11.13&quot;, letters[1:n()])) %&gt;% dplyr::select(fit, everything()) %&gt;% kable(booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) fit b lb b11.13a b1 &amp; b2 NA b11.13b b1 &amp; b2 0 b11.13c b NA b11.13d b 0 それでは、4つモデルを回す。 b11.13a &lt;- brm(data = d6, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b1 * 1), nlf(mu2 ~ a2 + b2 * 2), a1 + a2 + b1 + b2 ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b1), prior(normal(0, 0.5), class = b, nlpar = b2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.13a&quot;) b11.13b &lt;- brm(data = d6, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b1 * 1), nlf(mu2 ~ a2 + b2 * 2), a1 + a2 + b1 + b2 ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b1, lb = 0), prior(normal(0, 0.5), class = b, nlpar = b2, lb = 0)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, control = list(adapt_delta = .99), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.13b&quot;) b11.13c &lt;- brm(data = d6, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b * 1), nlf(mu2 ~ a2 + b * 2), a1 + a2 + b ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.13c&quot;) b11.13d &lt;- brm(data = d6, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b * 1), nlf(mu2 ~ a2 + b * 2), a1 + a2 + b ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, control = list(adapt_delta = .99), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.13d&quot;) 全てのモデルの推定結果を示す。結果はそれぞれ異なり、どれも教科書とは違う結果…。 tibble(fit = str_c(&quot;b11.13&quot;, letters[1:4])) %&gt;% mutate(fixef = purrr::map(fit, ~get(.) %&gt;% fixef() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;))) %&gt;% unnest(fixef) %&gt;% mutate(parameter = str_remove(parameter, &quot;_Intercept&quot;), fit= factor(fit, levels = str_c(&quot;b11.13&quot;, letters[4:1]))) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = fit)) + geom_vline(xintercept = 0, color =&quot;black&quot;, linetype=2) + geom_pointrange(fatten = 3/2, color =&quot;navy&quot;) + ylab(NULL) + theme(axis.ticks.y = element_blank(), strip.background = element_blank()) + facet_wrap(~ parameter, nrow = 1)+ theme(aspect.ratio=0.8) 図11.20: モデル11.13a~dの推定結果。 モデル比較をすると、ほとんど差はないことが分かる。 b11.13a &lt;- add_criterion(b11.13a, &quot;loo&quot;) b11.13b &lt;- add_criterion(b11.13b, &quot;loo&quot;) b11.13c &lt;- add_criterion(b11.13c, &quot;loo&quot;) b11.13d &lt;- add_criterion(b11.13d, &quot;loo&quot;) loo_compare(b11.13a, b11.13b, b11.13c, b11.13d, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) %&gt;% kable(caption = &quot;b11.13a ~ b11.13dのモデル比較&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.13c 0.0 0.0 -369.5 17.1 1.9 0.1 739.0 34.2 ## b11.13d 0.0 0.2 -369.6 16.9 1.9 0.1 739.1 33.8 ## b11.13a 0.0 0.1 -369.6 17.1 2.0 0.1 739.1 34.1 ## b11.13b -0.1 0.2 -369.6 16.9 2.0 0.1 739.2 33.8 表11.18: b11.13a ~ b11.13dのモデル比較 elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic b11.13c 0.0000000 0.0000000 -369.5104 17.10985 1.935502 0.1311694 739.0208 34.21970 b11.13d -0.0435284 0.2212151 -369.5539 16.90255 1.922417 0.1286941 739.1079 33.80509 b11.13a -0.0456842 0.0542677 -369.5561 17.05629 1.970180 0.1312604 739.1122 34.11259 b11.13b -0.0984991 0.2155251 -369.6089 16.89454 1.969123 0.1318588 739.2178 33.78908 切片だけのモデルもPSISに違いはない。 b11.13io &lt;- add_criterion(b11.13io, &quot;loo&quot;) loo_compare(b11.13io, b11.13a, b11.13b, b11.13c, b11.13d, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) %&gt;% kable(caption = &quot;b11.13a ~ b11.13dと切片のみモデルの比較&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.13c 0.0 0.0 -369.5 17.1 1.9 0.1 739.0 34.2 ## b11.13d 0.0 0.2 -369.6 16.9 1.9 0.1 739.1 33.8 ## b11.13a 0.0 0.1 -369.6 17.1 2.0 0.1 739.1 34.1 ## b11.13io -0.1 0.2 -369.6 16.9 2.0 0.1 739.2 33.9 ## b11.13b -0.1 0.2 -369.6 16.9 2.0 0.1 739.2 33.8 表11.19: b11.13a ~ b11.13dと切片のみモデルの比較 elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic b11.13c 0.0000000 0.0000000 -369.5104 17.10985 1.935502 0.1311694 739.0208 34.21970 b11.13d -0.0435284 0.2212151 -369.5539 16.90255 1.922417 0.1286941 739.1079 33.80509 b11.13a -0.0456842 0.0542677 -369.5561 17.05629 1.970180 0.1312604 739.1122 34.11259 b11.13io -0.0830380 0.1724640 -369.5935 16.94163 1.974698 0.1304070 739.1869 33.88327 b11.13b -0.0984991 0.2155251 -369.6089 16.89454 1.969123 0.1318588 739.2178 33.78908 モデルb11.13dの結果から、職業2の収入が2倍になった時に選択確率を調べる。5%程度上昇することが示唆された。 posterior_samples(b11.13d) %&gt;% transmute(s1= b_a1_Intercept + b_b_Intercept * income[1], s2_orig = b_a2_Intercept+b_b_Intercept*income[2], s2_new = b_a2_Intercept + b_b_Intercept * income[2]*2) %&gt;% mutate(p_orig = purrr::map2_dbl(s1, s2_orig, ~softmax(.x, .y, 0)[2]), p_new = purrr::map2_dbl(s1, s2_new, ~softmax(.x, .y, 0)[2])) %&gt;% mutate(p_diff = p_new - p_orig) %&gt;% mean_qi(p_diff) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 1 × 6 ## p_diff .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.04 0 0.17 0.95 mean qi 11.4.2 Predictor matched to observations 以下の過程でscore1~3が生成されるデータをシミュレーションする。家庭の収入によって職業選択に影響がでると考える。今度は \\[ \\begin{aligned} s_{1} &amp;= 0.5 -2 × family \\ income\\\\ s_{2} &amp;= 1.0 +0 × family\\ income\\\\ s_{3} &amp;= 1.5 +2 × family\\ income\\\\ \\end{aligned} \\] n &lt;- 500 set.seed(11) family_income &lt;- runif(n) b &lt;- c(-2, 0, 2) career &lt;- rep(NA, n) for (i in 1:n) { score &lt;- 0.5 * (1:3) + b * family_income[i] p &lt;- softmax(score[1], score[2], score[3]) career[i] &lt;- sample(1:3, size = 1, prob = p) } 得られたシミュレーションの結果を可視化する。 d7 &lt;- tibble(career = career) %&gt;% mutate(family_income = family_income) d7 %&gt;% mutate(career = as.factor(career)) %&gt;% ggplot(aes(x=family_income, fill=career))+ geom_density(alpha=3/4, color=NA)+ scale_fill_manual(values =c(&quot;grey4&quot;,&quot;lightblue&quot;,&quot;orange&quot;))+ theme(legend.position=&quot;none&quot;, aspect.ratio=1)-&gt;p7 d7 %&gt;% mutate(career = as.factor(career)) %&gt;% mutate(fi = santoku::chop_width(family_income, width = .1, start = 0, labels = 1:10)) %&gt;% count(fi, career) %&gt;% group_by(fi) %&gt;% mutate(proportion = n / sum(n)) %&gt;% mutate(f = as.double(fi)) %&gt;% ggplot(aes(x = (f - 1) / 9, y = proportion, fill = career)) + geom_area() + scale_fill_manual(values =c(&quot;grey4&quot;,&quot;lightblue&quot;,&quot;orange&quot;))+ theme(legend.position=&quot;none&quot;, aspect.ratio=1) + xlab(&quot;family_income, descritized&quot;) -&gt;p8 p7+p8 図11.21: シミュレーションによって得られた収入ごとに選択される職業。 それでは、モデルを回してみる。 b11.14 &lt;- brm(data = d7, family = categorical(link = &quot;logit&quot;, refcat =3), bf(career ~ 1, nlf(mu1 ~ a1 + b1*family_income), nlf(mu2 ~ a2 + b2*family_income), a1 + a2 + b1 + b2 ~1), prior = c(prior(normal(0, 1.5),class = b,nlpar = a1), prior(normal(0, 1.5), class= b, nlpar = a2), prior(normal(0, 1), class = b, nlpar = b1), prior(normal(0, 1), class = b, nlpar = b2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.14&quot;) モデルの推定結果は以下の通り。 fixef(b11.14) %&gt;% kable(digits=2, caption = &quot;モデルb11.14の結果。家庭の収入によってそれぞれの職業を選択する確率。&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.20: モデルb11.14の結果。家庭の収入によってそれぞれの職業を選択する確率。 Estimate Est.Error Q2.5 Q97.5 a1_Intercept -1.28 0.26 -1.81 -0.78 a2_Intercept -1.01 0.22 -1.45 -0.60 b1_Intercept -2.52 0.57 -3.61 -1.39 b2_Intercept -1.22 0.42 -2.03 -0.40 モデル結果を可視化する。 nd &lt;- tibble(family_income = seq(0,1,length.out=100)) fit11.14 &lt;- fitted(b11.14,newdata=nd) rbind(fit11.14[,,1], fit11.14[,,2], fit11.14[,,3]) %&gt;% data.frame() %&gt;% bind_cols(nd %&gt;% tidyr::expand(career = 1:3, family_income)) %&gt;% mutate(career = str_c(&quot;career: &quot;, career)) %&gt;% ggplot(aes(x = family_income, y = Estimate, ymin = Q2.5, ymax = Q97.5))+ geom_smooth(aes(fill=career, color = career), stat = &quot;identity&quot;)+ scale_fill_manual(values =c(&quot;grey4&quot;,&quot;lightblue&quot;,&quot;orange&quot;))+ scale_color_manual(values =c(&quot;grey4&quot;,&quot;lightblue&quot;,&quot;orange&quot;))+ scale_x_continuous(breaks = seq(0,1,by=0.5))+ scale_y_continuous(&quot;probability&quot;, limits = c(0, 1), breaks = 0:3 / 3, labels = c(&quot;0&quot;, &quot;.33&quot;, &quot;.67&quot;, &quot;1&quot;))+ theme(aspect.ratio=1)+ facet_wrap(~career)+ theme(strip.background = element_blank(), axis.text.y = element_text(hjust = 0), legend.position = &quot;none&quot;) 図11.22: モデルb11.14の推定結果。家庭の収入によって各職業を選ぶ確率。 全体に占める割合についてもグラフを書いてみる。 元データ（図11.23の右）と似ているので、うまく推定できてそう。 text &lt;- tibble(family_income = c(.45, .3, .15), proportion = c(.65, .8, .95), label = str_c(&quot;career: &quot;, 3:1), color = c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;)) rbind(fit11.14[, , 1], fit11.14[, , 2], fit11.14[, , 3]) %&gt;% data.frame() %&gt;% bind_cols(nd %&gt;% tidyr::expand(career = 1:3, family_income)) %&gt;% group_by(family_income) %&gt;% mutate(proportion = Estimate / sum(Estimate), career = factor(career)) %&gt;% ggplot(aes(x = family_income, y = proportion)) + geom_area(aes(fill = career)) + geom_text(data = text, aes(label = label, color = color),size = 4.25) + scale_color_manual(values =c(&quot;grey4&quot;,&quot;lightblue&quot;,&quot;orange&quot;)) + scale_fill_manual(values = c(&quot;grey4&quot;,&quot;lightblue&quot;,&quot;orange&quot;)) + theme(legend.position = &quot;none&quot;, aspect.ratio=1) 図11.23: モデルb11.14の推定結果。収入ごとに各職業を選択する割合。 11.4.3 Multinomial in disguise as Poisson 多項分布をポワソン分布に変換することもできる。ロジスティック回帰の章で用いたUCバークレーの例をポワソン分布で回帰してみよう。合格者数と不合格者数がそれぞれポワソン分布から得られると考える。’brm’で’mvbind’は多変量が応答変数になるときに用いることができるオプションである。二項分布でもモデリングして比較する。 b11.binom &lt;- brm(data = d3, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.binom&quot;) b11.pois &lt;- brm(data = d3 %&gt;% mutate(rej = reject), family = poisson, mvbind(admit, rej) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 11, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.pois&quot;) モデルから推定された合格者数と不合格者数。 post &lt;- posterior_samples(b11.pois) post %&gt;% mutate(admit = exp(b_admit_Intercept), reject = exp(b_rej_Intercept)) %&gt;% pivot_longer(admit:reject) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + stat_halfeye(point_interval = median_qi, .width = .95) + scale_fill_manual(values = c(&quot;lightblue&quot;,&quot;orange&quot;)) + labs(title = &quot; Mean admit/reject rates across departments&quot;, x = &quot;# applications&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) 図11.24: b11.poisから推定された同格者数とふ不合格者数 それぞれのモデルの結果から、合格する確率の事後平均を求める。 ほとんど一致することが分かる。 ## 二項分布 fixef(b11.binom)[,1] %&gt;% inv_logit_scaled() ## [1] 0.3880132 ## ポワソン分布 fixef(b11.pois) %&gt;% as.numeric() -&gt;k exp(k[1])/(exp(k[1])+exp(k[2])) ## [1] 0.3879595 合格確率の事後分布もほとんど同じになる。このことからも、カテゴリカル回帰（ロジスティック回帰）はポワソン回帰でも可能であることが分かった。 bind_cols( posterior_samples(b11.pois) %&gt;% mutate(`the Poisson` = exp(b_admit_Intercept) / (exp(b_admit_Intercept) + exp(b_rej_Intercept))), posterior_samples(b11.binom) %&gt;% mutate(`the binomial`=inv_logit_scaled(b_Intercept)) ) %&gt;% pivot_longer(starts_with(&quot;the&quot;)) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + stat_halfeye(point_interval = median_qi, .width =c(.95, .5)) + scale_fill_manual(values = c(&quot;lightblue&quot;,&quot;orange&quot;)) + labs(title = &quot;Two models, same marginal posterior&quot;, x = &quot;admissions probability&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 2.25)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, aspect.ratio=1) 図11.25: b11.binomとb11.poisによる合格確率の事後分布。 11.5 Practice 11.5.1 11E1 11E1. If an event has probability 0.35, what are the log-odds of this event? log(0.35 / (1 - 0.35)) ## [1] -0.6190392 11.5.2 11E2 If an event has log-odds 3.2, what is the probability of this event? exp(3.2) / (exp(3.2) + 1) ## [1] 0.9608343 11.5.3 11E3 Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome? exp(1.7) ## [1] 5.473947 11.5.4 11E4 Why do Poisson regressions sometimes require the use of an offset? Provide an example. 観察ごとに観察時間が異なる場合に必要になる(e.g., 頻度データなど)。 11.5.5 11M7 Use quap to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each action, m11.4 (page 330). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions? Relax the prior on the actor intercepts to Normal(0,10). Re-estimate the posterior using both ulam and quap. Do the difference increase or decrease? Why? 二次近似によるモデルの結果と比較する。 m11.4quap &lt;- quap( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a[actor] + b[treatment], a[actor] ~ dnorm(0, 1.5), b[treatment] ~ dnorm(0, 0.5) ), data = d ) 大きな違いはないことが分かる。 precis(m11.4quap, depth=2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(digits =2, caption = &quot;二次近似による推定結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.21: 二次近似による推定結果 parameter mean sd X5.5. X94.5. a[1] -0.44 0.33 -0.96 0.08 a[2] 3.71 0.72 2.55 4.86 a[3] -0.73 0.33 -1.26 -0.20 a[4] -0.73 0.33 -1.26 -0.20 a[5] -0.44 0.33 -0.96 0.08 a[6] 0.47 0.33 -0.06 1.00 a[7] 1.91 0.41 1.24 2.57 b[1] -0.04 0.28 -0.49 0.41 b[2] 0.47 0.28 0.02 0.93 b[3] -0.38 0.29 -0.83 0.08 b[4] 0.36 0.28 -0.09 0.82 posterior_summary(b11.4, probs = c(0.045, 0.955)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(!str_detect(parameter,c(&quot;prior&quot;))) %&gt;% filter(parameter!= &quot;lp__&quot;) %&gt;% kable(digits =2 , caption = &quot;MCMCによる推定結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.22: MCMCによる推定結果 parameter Estimate Est.Error Q4.5 Q95.5 b_a_actor1 -0.46 0.32 -1.01 0.07 b_a_actor2 3.89 0.74 2.73 5.26 b_a_actor3 -0.76 0.33 -1.33 -0.19 b_a_actor4 -0.76 0.33 -1.30 -0.20 b_a_actor5 -0.47 0.33 -1.02 0.09 b_a_actor6 0.47 0.33 -0.08 1.01 b_a_actor7 1.94 0.43 1.25 2.69 b_b_treatment1 -0.03 0.28 -0.49 0.44 b_b_treatment2 0.49 0.29 0.00 0.98 b_b_treatment3 -0.37 0.29 -0.86 0.11 b_b_treatment4 0.38 0.28 -0.10 0.86 続いて、切片の事前分布が\\(Normal(0,10)\\)のときを考える。 # quap m11.4quap_2 &lt;- quap( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a[actor] + b[treatment], a[actor] ~ dnorm(0, 10), b[treatment] ~ dnorm(0, 0.5) ), data = d ) ## MCMC b11.4_2 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b , a ~ 0+ actor, b ~ 0+ treatment, nl = TRUE), prior=c(prior(normal(0, 10), nlpar = a), prior(normal(0, 0.5), nlpar = b)), seed = 11, sample_prior = TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.4_2&quot;) 個体2の推定結果が大きく異なるが、それ以外はあまり変わらない。 precis(m11.4quap_2, depth=2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(digits =2, caption = &quot;二次近似による推定結果(Normal(0,10))&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.23: 二次近似による推定結果(Normal(0,10)) parameter mean sd X5.5. X94.5. a[1] -0.35 0.35 -0.91 0.20 a[2] 6.99 3.55 1.33 12.66 a[3] -0.65 0.35 -1.22 -0.09 a[4] -0.65 0.35 -1.22 -0.09 a[5] -0.35 0.35 -0.91 0.20 a[6] 0.58 0.35 0.02 1.14 a[7] 2.12 0.45 1.40 2.84 b[1] -0.14 0.30 -0.62 0.34 b[2] 0.38 0.30 -0.10 0.86 b[3] -0.49 0.30 -0.97 -0.01 b[4] 0.27 0.30 -0.21 0.75 posterior_summary(b11.4_2, probs = c(0.045, 0.955)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(!str_detect(parameter,c(&quot;prior&quot;))) %&gt;% filter(parameter!= &quot;lp__&quot;) %&gt;% kable(digits =2 , caption = &quot;MCMCによる推定結果(Normal(1,10))&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.24: MCMCによる推定結果(Normal(1,10)) parameter Estimate Est.Error Q4.5 Q95.5 b_a_actor1 -0.39 0.34 -0.97 0.19 b_a_actor2 11.10 5.36 4.65 21.87 b_a_actor3 -0.69 0.35 -1.29 -0.11 b_a_actor4 -0.70 0.35 -1.30 -0.09 b_a_actor5 -0.39 0.35 -1.01 0.20 b_a_actor6 0.56 0.36 -0.06 1.16 b_a_actor7 2.15 0.46 1.41 2.97 b_b_treatment1 -0.12 0.30 -0.64 0.39 b_b_treatment2 0.42 0.30 -0.11 0.92 b_b_treatment3 -0.47 0.30 -0.99 0.03 b_b_treatment4 0.30 0.30 -0.21 0.81 11.5.6 11M8 Revisit the data(Kline) islands example. This time drop Hawaii from the sample and refit the models. What changes do you observe? d4 %&gt;% filter(culture != &quot;Hawaii&quot;) -&gt; d4_2 b11.10_2 &lt;- brm( data = d4_2, family = poisson, formula = bf(total_tools ~ 0 + a + b * P, a ~ 0 + cid, b ~ 0 + cid, nl = TRUE), prior = c(prior(normal(3, 0.5), nlpar=a), prior(normal(0, 0.2), nlpar=b)), seed=8, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11.10_2&quot; ) ハワイを除いたとき、contactが低い国の傾きが半分程度になっていることが分かる。 posterior_summary(b11.10, probs = c(0.045, 0.955)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(!str_detect(parameter,c(&quot;prior&quot;))) %&gt;% filter(parameter!= &quot;lp__&quot;) %&gt;% kable(digits =2 , caption = &quot;ハワイありの推定結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.25: ハワイありの推定結果 parameter Estimate Est.Error Q4.5 Q95.5 b_a_cidhigh 3.61 0.07 3.48 3.73 b_a_cidlow 3.32 0.09 3.16 3.47 b_b_cidhigh 0.20 0.16 -0.08 0.46 b_b_cidlow 0.38 0.05 0.29 0.47 posterior_summary(b11.10_2, probs = c(0.045, 0.955)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(!str_detect(parameter,c(&quot;prior&quot;))) %&gt;% filter(parameter!= &quot;lp__&quot;) %&gt;% kable(digits =2 , caption = &quot;ハワイなしの推定結果&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.26: ハワイなしの推定結果 parameter Estimate Est.Error Q4.5 Q95.5 b_a_cidhigh 3.61 0.07 3.48 3.74 b_a_cidlow 3.18 0.13 2.95 3.40 b_b_cidhigh 0.19 0.16 -0.07 0.47 b_b_cidlow 0.19 0.13 -0.03 0.41 推定結果を図示してみても、違いは明らか。 nd &lt;- crossing(cid = c(&quot;low&quot;,&quot;high&quot;), P = seq(-1.5, 2.5, length.out=100)) fit11.10_2 &lt;- fitted(b11.10_2, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) b11.10_2 &lt;- add_criterion(b11.10_2,&quot;loo&quot;) fit11.10_2 %&gt;% mutate(population = exp(P*sd(log(d4$population))+mean(log(d4$population)))) %&gt;% ggplot(aes(x = population, group = cid))+ geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, linetype = cid, fill = cid, color = cid), stat = &quot;identity&quot;, size=1.5, alpha=1/2)+ geom_point(data = bind_cols(d4_2, b11.10_2$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, color = cid),stroke = 1.5, alpha=1/2)+ scale_shape_manual(values = c(16,1))+ scale_color_manual(values = c(&quot;lightblue3&quot;,&quot;orange3&quot;))+ scale_fill_manual(values = c(&quot;lightblue&quot;, &quot;orange&quot;))+ scale_x_continuous(breaks = seq(0,250000,by=50000))+ coord_cartesian(xlim = c(0,250000), ylim = c(0, 80))+ theme(legend.position = &quot;none&quot;, aspect.ratio=1, plot.title = element_text(family =&quot;Japan1GothicBBB&quot;))+ labs(title = &quot;ハワイを除いたモデル&quot;) -&gt;p9 p6 + labs(title=&quot;ハワイを含めたモデル&quot;)+ theme(plot.title =element_text(family =&quot;Japan1GothicBBB&quot;))-&gt; p6 p6 + p9 図11.26: ハワイがある場合と除いた場合のモデルの推定結果 11.5.7 11H1 Use WAIC or PSIS to compare the chimpanzee model that includes a unique intercept for each actor, m11.4 (page 330), to the simpler models fit in the same section. Interpret the results. b11.1~b11\\2までのモデルを比較する。 b11.1 &lt;- add_criterion(b11.1, criterion = &quot;loo&quot;) b11.2 &lt;- add_criterion(b11.2, criterion = &quot;loo&quot;) b11.3 &lt;- add_criterion(b11.3, criterion = &quot;loo&quot;) b11.4 &lt;- add_criterion(b11.4, criterion = &quot;loo&quot;) loo_compare(b11.1, b11.2, b11.3, b11.4) ## elpd_diff se_diff ## b11.4 0.0 0.0 ## b11.3 -75.1 9.2 ## b11.2 -75.6 9.2 ## b11.1 -77.9 9.5 11.5.8 11H2 The data contained in library(MASS); data(eagles) are records of salmon pirating attempts by Bald Eagles in Washington State. See ? eagles for details. While one eagle feeds, sometime another will swoop in and try to steal the salmon from it. Call the feeding eagle the “victim” and the thief the “pirate.” Use the available data to build a binomial GLM of successful pirating attempts. 11.5.8.1 a Consider the following model: where y is the number of successful attempts, n is the total number of attempts, P is a dummy variable indicating whether or not the pirate had large body size, V is a dummy variable indicating whether or not the victim had large body size, and finally A is a dummy variable indicating whether or not the pirate was an adult. Fit the model above to the eagles data, using both quap and ulam. Is the quadratic approximation okay? 鷲が他の個体から餌を横取りできる確率をモデリングする。なお、Pは盗む個体(pirates)の身体が大きいか否か、Vはとられる個体(victim)の身体が大きいか否か、Aはpiratesがオトナか否かを表す。 \\[ \\begin{aligned} y_{i} &amp;\\sim Binomial(n_{i}, p_{i})\\\\ logit(p_{i}) &amp;= \\alpha + \\beta_{P}P_{i} + \\beta_{V}V_{i} + \\beta_{A}A_{i}\\\\ \\alpha &amp;\\sim Normal(0,1.5)\\\\ \\beta_{j} &amp;\\sim Normal(0,0.5)\\\\ \\end{aligned} \\] library(MASS) data(&quot;eagles&quot;) dat &lt;- eagles kable(dat, caption = &quot;データ&#39;eagle&#39;&quot;, booktabs = TRUE) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.27: データ’eagle’ y n P A V 17 24 L A L 29 29 L A S 17 27 L I L 20 20 L I S 1 12 S A L 15 16 S A S 0 28 S I L 1 4 S I S それでは、モデリングしてみる。 b11H2 &lt;- brm(data = dat, family = binomial, y|trials(n) ~ 1 + P + A + V, prior = c(prior(normal(0,1.5), class = Intercept), prior(normal(0,0.5), class=b)), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11H2&quot;) 推定結果は以下の通り。 posterior_summary(b11H2, probs = c(0.045, 0.955)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(!str_detect(parameter,c(&quot;prior&quot;))) %&gt;% filter(parameter!= &quot;lp__&quot;) %&gt;% kable(digits =2 , caption = &quot;b11H2の推定結果&quot;, booktabs = T) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 表11.28: b11H2の推定結果 parameter Estimate Est.Error Q4.5 Q95.5 b_Intercept 0.89 0.31 0.38 1.43 b_PS -1.64 0.32 -2.17 -1.10 b_AI -0.65 0.32 -1.19 -0.10 b_VS 1.71 0.32 1.17 2.26 11.5.8.2 b Now interpret the estimates. If the quadratic approximation turned out okay, then it’s okay to use the quap estimates. Otherwise stick to ulam estimates. Then plot the posterior predictions. Compute and display both (1) the predicted probability of success and its 89% interval for each row (i) in the data, as well as (2) the predicted success count and its 89% interval. What different information does each type of posterior prediction provide? # 回数スケール fitted(b11H2) %&gt;% data.frame() %&gt;% bind_cols(dat) %&gt;% mutate(cond = c(&quot;L,L,A&quot;,&quot;L,S,A&quot;,&quot;L,L,I&quot;,&quot;L,S,I&quot;,&quot;S,L,A&quot;,&quot;S,S,A&quot;,&quot;S,L,I&quot;,&quot;S,S,I&quot;)) -&gt; fit11H2 dat %&gt;% mutate(cond = c(&quot;L,L,A&quot;,&quot;L,S,A&quot;,&quot;L,L,I&quot;,&quot;L,S,I&quot;,&quot;S,L,A&quot;,&quot;S,S,A&quot;,&quot;S,L,I&quot;,&quot;S,S,I&quot;)) %&gt;% ggplot(aes(x=cond))+ geom_point(aes(y=y),size=4,color = &quot;navy&quot;, alpha = 1/2)+ geom_pointinterval(data = fit11H2, aes(y=Estimate, ymin=Q2.5, ymax=Q97.5), shape = 1,point_size=3)+ theme(aspect.ratio=0.7, axis.title.y = element_text(family =&quot;Japan1GothicBBB&quot;), text = element_text(size=10))+ labs(y=&quot;成功数&quot;) -&gt; p11 ## 成功確率 fitted(b11H2, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(dat) %&gt;% mutate(cond = c(&quot;L,L,A&quot;,&quot;L,S,A&quot;,&quot;L,L,I&quot;,&quot;L,S,I&quot;,&quot;S,L,A&quot;,&quot;S,S,A&quot;,&quot;S,L,I&quot;,&quot;S,S,I&quot;)) %&gt;% mutate(prob_est = inv_logit_scaled(Estimate), lb = inv_logit_scaled(Q2.5), ub = inv_logit_scaled(Q97.5)) -&gt; fit11H2_2 fit11H2_2 %&gt;% mutate(prob = y/n) %&gt;% ggplot(aes(x=cond))+ geom_point(aes(y=prob),size=4,color = &quot;navy&quot;, alpha = 1/2)+ geom_pointinterval(data = fit11H2_2, aes(y=prob_est, ymin=lb, ymax=ub), shape = 1,point_size=3)+ labs(y=&quot;成功確率&quot;)+ theme(aspect.ratio=0.7, axis.title.y = element_text(family =&quot;Japan1GothicBBB&quot;), text = element_text(size=10))-&gt; p12 p11 + p12 図11.27: モデルb11H2による推定結果。青丸は実測値、エラーバー付きの点はモデルによる予測値を表す。左は成功回数、右は成功確率を示す。 11.5.8.3 c Now try to improve the model. Consider an interaction between the pirate’s size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret. piratesの大きさと年齢の交互作用を入れたモデルを作成する。 b11H2c &lt;- brm(data = dat, family = binomial, y|trials(n) ~ 1 + P + A + V + P:A, prior = c(prior(normal(0,1.5), class = Intercept), prior(normal(0,0.5), class=b)), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11H2c&quot;) 先ほどのモデルと比較すると、交互作用モデルの方がよい。 b11H2 &lt;- add_criterion(b11H2, &quot;loo&quot;) b11H2c &lt;- add_criterion(b11H2c,&quot;loo&quot;) loo_compare(b11H2,b11H2c) ## elpd_diff se_diff ## b11H2c 0.0 0.0 ## b11H2 -3.3 3.5 交互作用を入れたモデルの方がより実データにマッチしている。 # 回数スケール fitted(b11H2c) %&gt;% data.frame() %&gt;% bind_cols(dat) %&gt;% mutate(cond = c(&quot;L,L,A&quot;,&quot;L,S,A&quot;,&quot;L,L,I&quot;,&quot;L,S,I&quot;,&quot;S,L,A&quot;,&quot;S,S,A&quot;,&quot;S,L,I&quot;,&quot;S,S,I&quot;)) -&gt; fit11H2c dat %&gt;% mutate(cond = c(&quot;L,L,A&quot;,&quot;L,S,A&quot;,&quot;L,L,I&quot;,&quot;L,S,I&quot;,&quot;S,L,A&quot;,&quot;S,S,A&quot;,&quot;S,L,I&quot;,&quot;S,S,I&quot;)) %&gt;% ggplot(aes(x=cond))+ geom_point(aes(y=y),size=4,color = &quot;navy&quot;, alpha = 1/2)+ geom_pointinterval(data = fit11H2c, aes(y=Estimate, ymin=Q2.5, ymax=Q97.5), shape = 1,point_size=3)+ labs(y=&quot;成功数&quot;)+ theme(aspect.ratio=0.7, axis.title.y = element_text(family =&quot;Japan1GothicBBB&quot;), text = element_text(size = 10)) -&gt; p13 ## 成功確率 fitted(b11H2c, scale = &quot;linear&quot;) %&gt;% data.frame() %&gt;% bind_cols(dat) %&gt;% mutate(cond = c(&quot;L,L,A&quot;,&quot;L,S,A&quot;,&quot;L,L,I&quot;,&quot;L,S,I&quot;,&quot;S,L,A&quot;,&quot;S,S,A&quot;,&quot;S,L,I&quot;,&quot;S,S,I&quot;)) %&gt;% mutate(prob_est = inv_logit_scaled(Estimate), lb = inv_logit_scaled(Q2.5), ub = inv_logit_scaled(Q97.5)) -&gt; fit11H2_2c fit11H2_2c %&gt;% mutate(prob = y/n) %&gt;% ggplot(aes(x=cond))+ geom_point(aes(y=prob),size=4,color = &quot;navy&quot;, alpha = 1/2)+ geom_pointinterval(data = fit11H2_2c, aes(y=prob_est, ymin=lb, ymax=ub), shape = 1,point_size=3)+ labs(y=&quot;成功確率&quot;)+ theme(aspect.ratio=0.7, axis.title.y = element_text(family =&quot;Japan1GothicBBB&quot;), text = element_text(size = 10))-&gt; p14 p13 + p14 図11.28: モデルb11H2cによる推定結果。青丸は実測値、エラーバー付きの点はモデルによる予測値を表す。左は成功回数、右は成功確率を示す。 11.5.9 11H3 The data contained in data(salamanders) are counts of salamanders (Plethodon elongatus) from 47 different 49-m2 plots in northern California. The column SALAMAN is the count in each plot, and the columns PCTCOVER and FORESTAGE are percent of ground cover and age of trees in the plot, respectively. You will model SALAMAN as a Poisson variable. 11.5.9.1 a Model the relationship between density and percent cover, using a log-link (same as the example in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing quap to ulam. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? A bad job? サラマンダーの数(SALAMAN)と地面に覆われた割合?(PCTCOVER)の関係をポワソン回帰で調べる（表 11.29）。ひとまず、事前分布は\\(\\alpha \\sim Normal(0,1.5)\\)、\\(\\beta \\sim Normal(0,0.5)\\)とする。 data(&quot;salamanders&quot;) dat2 &lt;- salamanders head(salamanders) %&gt;% kable(booktabs = TRUE, caption = &quot;データsalamanderの中身&quot;) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) 表11.29: データsalamanderの中身 SITE SALAMAN PCTCOVER FORESTAGE 1 13 85 316 2 11 86 88 3 11 90 548 4 9 88 64 5 8 89 43 6 7 83 368 分析の結果、強い影響がみられた（表 11.30）。 dat2 %&gt;% mutate(P = standardize(PCTCOVER)) -&gt; dat2 fit11H3 &lt;- brm( data = dat2, family=poisson, formula = SALAMAN ~ 1 + P, prior = c(prior(normal(0,1.5), class = Intercept), prior(normal(0,0.5), class = b)), sample_prior = TRUE, backend = &quot;cmdstanr&quot;, seed=4, file = &quot;output/Chapter11/fit11H3&quot; ) fixef(fit11H3, probs = c(0.025,0.975)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = TRUE, caption = &quot;fit11H3の結果。&quot;, digits=2) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) 表11.30: fit11H3の結果。 parameter Estimate Est.Error Q2.5 Q97.5 Intercept 0.49 0.14 0.23 0.75 P 1.05 0.16 0.74 1.37 モデル’fit11H3’の結果を図示すると、以下の通り(図 11.29)。 nd &lt;- tibble(P = seq(-2,1,length.out=100)) fitted(fit11H3, newdata = nd, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd) -&gt; fitH3 dat2 %&gt;% ggplot(aes(x = P))+ geom_point(aes(y = SALAMAN), color = &quot;navy&quot;, size = 4, alpha =2/3, shape = 16)+ geom_smooth(data = fitH3, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = &quot;black&quot;,fill=&quot;grey&quot;, alpha = 2/3, stat = &quot;identity&quot;)+ theme(aspect.ratio=1) 図11.29: fit11H3による推定結果 11.5.9.2 b Can you improve the model by using the other predictor, FORESTAGE? Try any models you think useful. Can you explain why FORESTAGE helps or does not help with prediction? FORESTAGE(樹齢？)も入れたモデルを考える。初めに、交互作用がないモデルを立てる。結果は以下の通り。樹齢はほとんど影響を与えていないよう（表11.31）。 dat2 &lt;- dat2 %&gt;% mutate(F = standardize(FORESTAGE)) fit11H3_2 &lt;- brm( data = dat2, family=poisson, formula = SALAMAN ~ 1 + P + F, prior = c(prior(normal(0,1.5), class = Intercept), prior(normal(0,0.5), class = b)), sample_prior = TRUE, backend = &quot;cmdstanr&quot;, seed=4, file = &quot;output/Chapter11/fit11H3_2&quot; ) fixef(fit11H3_2, probs = c(0.025,0.975)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = TRUE, digits =2, caption = &quot;fit11H3_2の結果&quot;) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) 表11.31: fit11H3_2の結果 parameter Estimate Est.Error Q2.5 Q97.5 Intercept 0.49 0.14 0.19 0.76 P 1.03 0.17 0.71 1.39 F 0.02 0.09 -0.16 0.20 結果を図示しても、ほとんど効果はないよう(図11.30)。 nd &lt;- crossing(P = 0, F = seq(-1,3,length.out=100)) fitted(fit11H3_2, nd, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd) -&gt; fitH3_2 dat2 %&gt;% ggplot(aes(x = F))+ geom_point(aes(y = SALAMAN), color = &quot;navy&quot;, size = 4, alpha =2/3, shape = 16)+ geom_smooth(data = fitH3_2, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = &quot;black&quot;,fill=&quot;grey&quot;, alpha = 2/3, stat = &quot;identity&quot;)+ theme(aspect.ratio=1) 図11.30: fit11H4_2の推定結果 一応、交互作用のあるモデルも考える。 fit11H3_3 &lt;- brm( data = dat2, family=poisson, formula = SALAMAN ~ 1 + P + F + P:F, prior = c(prior(normal(0,1.5), class = Intercept), prior(normal(0,0.5), class = b)), sample_prior = TRUE, backend = &quot;cmdstanr&quot;, seed=4, file = &quot;output/Chapter11/fit11H3_3&quot; ) 結果はほとんど変わらないよう。 fixef(fit11H3_3, probs = c(0.025,0.975)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = TRUE, digits=2) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) parameter Estimate Est.Error Q2.5 Q97.5 Intercept 0.58 0.20 0.18 0.96 P 0.92 0.24 0.44 1.41 F 0.15 0.24 -0.31 0.64 P:F -0.17 0.29 -0.76 0.38 結果は以下の通り(図11.31)。 # F = 0 nd_1 &lt;- crossing(P = seq(-2,1,length.out=50), F = -0.5) fitted(fit11H3_3, nd_1, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd_1)-&gt; fitH3_3 dat2 %&gt;% filter(F&gt;-1, F &lt;0) %&gt;% ggplot(aes(x = P, group=1))+ geom_point(aes(y = SALAMAN), color = &quot;navy&quot;, size = 4, alpha =2/3, shape = 16)+ geom_smooth(data = fitH3_3, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = &quot;black&quot;,fill=&quot;grey&quot;, alpha = 2/3, stat = &quot;identity&quot;)+ theme(aspect.ratio=1)+ labs(subtitle = &quot;A: F = -0.5 (-1&lt;F&lt;0)&quot;) -&gt; p17 # F = 1 nd_2 &lt;- crossing(P = seq(-2,1,length.out=50), F = 0.5) fitted(fit11H3_3, nd_2, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd_1)-&gt; fitH3_3_2 dat2 %&gt;% filter(F&gt;0, F&lt;1) %&gt;% ggplot(aes(x = P, group=1))+ geom_point(aes(y = SALAMAN), color = &quot;navy&quot;, size = 4, alpha =2/3, shape = 16)+ geom_smooth(data = fitH3_3_2, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = &quot;black&quot;,fill=&quot;grey&quot;, alpha = 2/3, stat = &quot;identity&quot;)+ theme(aspect.ratio=1)+ labs(subtitle = &quot;B: F = 0.5(0&lt;F&lt;1)&quot;) -&gt; p18 # F = 2 nd_3 &lt;- crossing(P = seq(-2,1,length.out=50), F = 1.5) fitted(fit11H3_3, nd_3, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd_1)-&gt; fitH3_3_3 dat2 %&gt;% filter(F&gt;1) %&gt;% ggplot(aes(x = P, group=1))+ geom_point(aes(y = SALAMAN), color = &quot;navy&quot;, size = 4, alpha =2/3, shape = 16)+ geom_smooth(data = fitH3_3_3, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = &quot;black&quot;,fill=&quot;grey&quot;, alpha = 2/3, stat = &quot;identity&quot;)+ theme(aspect.ratio=1)+ labs(subtitle = &quot;C: F = 1.5(F&gt;1)&quot;) -&gt; p19 p17 + p18 + p19 図11.31: fit11H3_3の推定結果を図示したもの。AはFが-1&lt;F&lt;0のデータを示し、F=-0.5の結果を示したもの。BはFが0&lt;F&lt;1のデータを示し、F=0.5の結果を示したもの。CはFが1&lt;Fのデータを示し、F=1.5の結果を示したもの。 11.5.10 11H4 he data in data(NWOGrants) are outcomes for scientific funding applications for the Netherlands Organization for Scientific Research (NWO) from 2010–2012 (see van der Lee &amp; Ellemers, 2015, for data and context). These data have a very similar structure to the UCBAdmit data discussed in the chapter. I want you to consider a similar question: What are the total and direct causal effects of gender on grant awards? Consider a mediation path (a pipe) through discipline. Draw the corresponding DAG and then use one or more binomial GLMs to answer the question. What is your causal interpretation? If NWO’s goal is to equalize rates of funding between men and women, what type of intervention would be most effective? オランダ科学協会における研究費の応募結果をしてしたデータセットを用いる(Van der Lee and Ellemers 2015)。分野(discipline)と性別(gender)ごとに応募数(applications)と採用数(awards)が収められている(表 11.32。年齢と分野が採用率にどのように影響するかを確認する。 data(NWOGrants) dat3 &lt;- NWOGrants head(dat3) %&gt;% kable(booktabs=TRUE, caption = &quot;NWOGrantsデータの中身&quot;) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) 表11.32: NWOGrantsデータの中身 discipline gender applications awards Chemical sciences m 83 22 Chemical sciences f 39 10 Physical sciences m 135 26 Physical sciences f 39 9 Physics m 67 18 Physics f 9 2 設問で想定されているDAGは以下のようになる(gender: D, discipline: D, awards: A)。 dagify(A ~ G + D, D ~ G) %&gt;% ggdag()+ theme_dag() まず、性別だけを入れたときのGLMを行う。これは、分野(D)を介した影響も含む総合効果の推定にあたる。分野を考慮しなければ、性別間で大きな違いはない。 fit11H4_1 &lt;- brm(data = dat3, family = binomial, formula = awards|trials(applications) ~ 0 + gender, prior = c(prior(normal(0,1.5), class=b)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter11/fit11H4_1&quot;) fixef(fit11H4_1) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = T, digits=2) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) parameter Estimate Est.Error Q2.5 Q97.5 genderf -1.74 0.08 -1.90 -1.58 genderm -1.53 0.06 -1.66 -1.41 結果を図示する(図11.32)。実際のデータには全くマッチしていない。それぞれの性別が採用される期待値は、女性は0.15、男性は0.18である。 cond &lt;- c(&quot;che M&quot;,&quot;che F&quot;,&quot;phcl M&quot;,&quot;phcl F&quot;, &quot;phcs M&quot;,&quot;phcs F&quot;,&quot;hum M&quot;,&quot;hum F&quot;, &quot;tech M&quot;, &quot;tech F&quot;, &quot;inter M&quot;, &quot;inter F&quot;, &quot;earth M&quot;, &quot;earth F&quot;, &quot;social M&quot;, &quot;social F&quot;, &quot;med M&quot;, &quot;med F&quot;) fitted(fit11H4_1, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(dat3) %&gt;% mutate(n = 1:n()) %&gt;% mutate(cond = cond) %&gt;% mutate(prop = awards/applications, prop_est = Estimate/applications, ub = Q5.5/applications, lb = Q94.5/applications) %&gt;% ggplot(aes(x=cond))+ geom_point(aes(y = prop), color = &quot;navy&quot;, size=4)+ geom_pointinterval(aes(y = prop_est, ymin = lb, ymax = ub), shape =1, point_size=3)+ labs(y = &quot;採用率&quot;, x = NULL)+ theme(axis.title.y = element_text(family =&quot;Yu Mincho&quot;), axis.title = element_text(family =&quot;Yu Mincho&quot;), axis.text.x = element_text(size = 8)) 図11.32: 性別だけ考慮したモデルfit11H4_1の結果 続いて、分野も説明変数に入れたモデルを考える。分析結果は以下の通り。やはり性別はそこまで影響していない。 fit11H4_2 &lt;- brm(data = dat3, family = binomial, formula = awards|trials(applications) ~ 0 + gender + discipline, prior = c(prior(normal(0,1.5), class=b)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter11/fit11H4_2&quot;) fixef(fit11H4_2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = T, digits=2) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) parameter Estimate Est.Error Q2.5 Q97.5 genderf -1.15 0.20 -1.54 -0.77 genderm -1.01 0.19 -1.39 -0.65 disciplineEarthDlifesciences -0.33 0.24 -0.79 0.14 disciplineHumanities -0.56 0.23 -1.01 -0.12 disciplineInterdisciplinary -0.60 0.27 -1.14 -0.07 disciplineMedicalsciences -0.66 0.23 -1.10 -0.22 disciplinePhysicalsciences -0.34 0.27 -0.86 0.19 disciplinePhysics -0.02 0.32 -0.67 0.58 disciplineSocialsciences -0.79 0.21 -1.20 -0.38 disciplineTechnicalsciences -0.54 0.25 -1.02 -0.05 結果を図示すると以下の通り(図11.33)。データへの当てはまりは良くなったが、分野によっては男女の採用率は逆転している。 fitted(fit11H4_2, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(dat3) %&gt;% mutate(n = 1:n()) %&gt;% mutate(cond = cond) %&gt;% mutate(prop = awards/applications, prop_est = Estimate/applications, ub = Q5.5/applications, lb = Q94.5/applications) %&gt;% ggplot(aes(x=cond))+ geom_point(aes(y = prop), color = &quot;navy&quot;, size=4)+ geom_pointinterval(aes(y = prop_est, ymin = lb, ymax = ub), shape =1, point_size=5)+ labs(y = &quot;採用率&quot;, x = NULL)+ theme(axis.title.y = element_text(family =&quot;Yu Mincho&quot;), axis.title = element_text(family =&quot;Yu Mincho&quot;), axis.text.x = element_text(size = 8)) 図11.33: 性別と分野を考慮したモデルfit11H4_2の結果 最後に交互作用を考えたモデルを回す。結果は以下の通り。 fit11H4_3 &lt;- brm(data = dat3, family = binomial, formula = awards|trials(applications) ~ 0 + gender*discipline, prior = c(prior(normal(0,1.5), class=b)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter11/fit11H4_3&quot;) fixef(fit11H4_3) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = T, digits=2) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) parameter Estimate Est.Error Q2.5 Q97.5 genderf -1.16 0.28 -1.71 -0.63 genderm -1.02 0.23 -1.49 -0.58 disciplineEarthDlifesciences -0.62 0.36 -1.33 0.11 disciplineHumanities -0.29 0.34 -0.93 0.37 disciplineInterdisciplinary -0.16 0.38 -0.92 0.59 disciplineMedicalsciences -0.91 0.33 -1.54 -0.26 disciplinePhysicalsciences -0.09 0.44 -0.97 0.74 disciplinePhysics -0.12 0.67 -1.48 1.18 disciplineSocialsciences -0.88 0.31 -1.48 -0.26 disciplineTechnicalsciences -0.21 0.39 -0.96 0.55 genderm:disciplineEarthDlifesciences 0.49 0.44 -0.37 1.35 genderm:disciplineHumanities -0.48 0.42 -1.29 0.32 genderm:disciplineInterdisciplinary -0.86 0.50 -1.86 0.09 genderm:disciplineMedicalsciences 0.45 0.40 -0.34 1.23 genderm:disciplinePhysicalsciences -0.34 0.51 -1.29 0.65 genderm:disciplinePhysics 0.11 0.71 -1.23 1.59 genderm:disciplineSocialsciences 0.18 0.37 -0.55 0.93 genderm:disciplineTechnicalsciences -0.45 0.46 -1.36 0.44 結果を図示すると以下の通り(図11.34)。やはり分野によって男女のどちらかが有利なのかは一致しない。 fitted(fit11H4_3, probs = c(0.055, 0.945)) %&gt;% data.frame() %&gt;% bind_cols(dat3) %&gt;% mutate(n = 1:n()) %&gt;% mutate(cond = cond) %&gt;% mutate(prop = awards/applications, prop_est = Estimate/applications, ub = Q5.5/applications, lb = Q94.5/applications) %&gt;% ggplot(aes(x=cond))+ geom_point(aes(y = prop), color = &quot;navy&quot;, size=4)+ geom_pointinterval(aes(y = prop_est, ymin = lb, ymax = ub), shape =1, point_size=5)+ labs(y = &quot;採用率&quot;, x = NULL)+ theme(axis.title.y = element_text(family =&quot;Yu Mincho&quot;), axis.title = element_text(family =&quot;Yu Mincho&quot;), axis.text.x = element_text(size = 8)) 図11.34: 交互作用を考慮したモデルfit11H4_3の結果 以上の結果を総合すると、性別よりも分野が大きく影響していると考えられる。 11.5.11 11H5 Suppose that the NWO Grants sample has an unobserved confound that influences both choice of discipline and the probability of an award. One example of such a confound could be the career stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the grants. In other disciplines, scholars from all career stages compete. As a result, career stage influences discipline as well as the probability of being awarded a grant. Add these influences to your DAG from the previous problem. What happens now when you condition on discipline? Does it provide an un-confounded estimate of the direct path from gender to an award? Why or why not? Justify your answer with the backdoor criterion. If you have trouble thinking this though, try simulating fake data, assuming your DAG is true. Then analyze it using the model from the previous problem. What do you conclude? Is it possible for gender to have a real direct causal influence but for a regression conditioning on both gender and discipline to suggest zero influence? このとき、DAGは以下のようになる(\\(S\\)を未観測のcareer stageとする)。 このときDは合流点になるので、これを説明変数に加えると新たに\\(G \\rightarrow D \\leftarrow S \\rightarrow A\\)というバックドアパスが生じてしまう。よって、このDAGでGからAへの直接因果効果を正確に推定することはできない。 dagify(A ~ G + D + S, D ~ G, D ~ S) %&gt;% ggdag()+ theme_dag() これはシミュレーションからも明らかになる。たとえ性別の影響が0であるという仮定の下でデータをシミュレートしても、Dで条件づけるとGの影響がある(係数が正になる)と推定されてしまう。 n &lt;- 1000 g &lt;- rbernoulli(n, p = 0.5) s &lt;- rbernoulli(n, p = 0.5) d &lt;- rbernoulli(n, p = inv_logit_scaled(2 * g - s)) ## 性別の影響は0であるというシミュレーションを行う a &lt;- rbernoulli(n, p = inv_logit_scaled(0 * g + d + s - 2)) dat &lt;- tibble(g, d, a) %&gt;% mutate(across(everything(), as.integer), across(everything(), as.factor)) b11H5 &lt;- brm(a ~ 1 + d + g, data = dat, family = bernoulli, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b)), iter = 4000, warmup = 2000, chains = 4, cores = 4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter11/b11H5&quot;) as_draws_df(b11H5, variable = &quot;b_g1&quot;) %&gt;% mean_hdi(b_g1, .width = 0.89) ## # A tibble: 1 × 6 ## b_g1 .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.127 -0.132 0.372 0.89 mean hdi 11.5.12 11H6 The data in data(Primates301) are 301 primate species and associated measures. In this problem, you will consider how brain size is associated with social learning. There are three parts. 301種の霊長類で、脳サイズが社会学習とどのように関連しているかを考える（表11.33）。データは Street et al. (2017) を用いている。 data(&quot;Primates301&quot;) dat4 &lt;- Primates301 head(Primates301) %&gt;% data.frame() %&gt;% dplyr::select(social_learning,brain, research_effort) %&gt;% kable(digits=2, booktabs = T, caption = &quot;data&#39;Primates301&#39;の内容&quot;) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) 表11.33: data’Primates301’の内容 social_learning brain research_effort 0 58.02 6 0 NA 6 0 52.84 15 0 52.63 45 0 51.70 37 3 49.88 79 11.5.12.1 a Model the number of observations of social_learning for each species as a function of the log brain size. Use a Poisson distribution for the social_learning outcome variable. Interpret the resulting posterior. 社会学習と脳容量(logをとる)の関係をモデリングする。 dat4_2 &lt;- dat4 %&gt;% mutate(B = log(brain)) %&gt;% dplyr::select(social_learning, B) ggplot(dat4_2 %&gt;% na.omit(), aes(x=B, y=social_learning))+ geom_point()+ labs(x = &quot;log(brain size)&quot;)+ theme(aspect.ratio=1) 図11.35: log(脳サイズ)と社会学習の関連 結果は以下の通り。脳容量が影響を与えていることが分かる。 fit11H6_a &lt;- brm(data = dat4_2, family = poisson, formula = social_learning ~ 1 + B, prior = c(prior(normal(0,1.5), class = Intercept), prior(normal(0,0.5), class = b)), seed = 4, file = &quot;output/Chapter11/fit11H6_a&quot;) fixef(fit11H6_a) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = T, digits=2) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) parameter Estimate Est.Error Q2.5 Q97.5 Intercept -9.51 0.33 -10.18 -8.87 B 2.29 0.06 2.17 2.41 結果を図示する（図11.36）。データとよく合っている。 nd &lt;- tibble(B = seq(0,6.5, length.out=50)) fitted(fit11H6_a, nd) %&gt;% data.frame() %&gt;% bind_cols(nd) -&gt; h6a dat4_2 %&gt;% ggplot(aes(x=B))+ geom_point(aes(y = social_learning), size= 3, color = &quot;navy&quot;)+ geom_smooth(data = h6a, aes(y=Estimate, ymin = Q2.5, ymax = Q97.5), alpha = 2/3, color = &quot;black&quot;, stat = &quot;identity&quot;)+ theme(aspect.ratio=1)+ labs(x = &quot;log(brain size)&quot;) 図11.36: fit11H6_aの推定結果 11.5.12.2 b Some species are studied much more than others. So the number of reported instances of social_learning could be a product of research effort. Use the research_effort variable, specifically its logarithm, as an additional predictor variable. Interpret the coefficient for log research_effort. How does this model differ from the previous one? 研究努力(research_effort)が影響している可能性もあるため、それも対数変換して説明変数に加える。分析の結果、脳容量の効果はあるものの、小さくなった。また、研究努力がかなり影響していることが分かった。 dat4_3 &lt;- dat4 %&gt;% mutate(R = log(research_effort), B = log(brain)) %&gt;% dplyr::select(social_learning, B, R) fit11H6_b &lt;- brm(data = dat4_3, family = poisson, formula = social_learning ~ 1 + B +R, prior = c(prior(normal(0,1.5), class = Intercept), prior(normal(0,0.5), class = b)), seed = 4, file = &quot;output/Chapter11/fit11H6_b&quot;) fixef(fit11H6_b) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% kable(booktabs = T, digits=2) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) parameter Estimate Est.Error Q2.5 Q97.5 Intercept -7.72 0.30 -8.32 -7.14 B 0.32 0.07 0.19 0.46 R 1.65 0.07 1.51 1.79 結果を図示すると以下の通り（図11.37）。 ## 脳容量 nd_1 &lt;- crossing(R = mean(dat4_3$B, na.rm =T), B = seq(0,6.5, length.out=50)) fitted(fit11H6_b, nd_1) %&gt;% data.frame() %&gt;% bind_cols(nd_1) -&gt; h6b_1 dat4_3 %&gt;% ggplot(aes(x=B))+ geom_point(aes(y = social_learning), size= 3, color = &quot;navy&quot;)+ geom_smooth(data = h6b_1, aes(y=Estimate, ymin = Q2.5, ymax = Q97.5), alpha = 2/3, color = &quot;black&quot;, stat = &quot;identity&quot;)+ theme(aspect.ratio=1)+ labs(x = &quot;log(brain size)&quot;)+ coord_cartesian(ylim=c(0,50)) -&gt;p20 ## 研究努力 nd &lt;- crossing(B = mean(dat4_3$B, na.rm =T), R = seq(0,6.5, length.out=50)) fitted(fit11H6_b, nd) %&gt;% data.frame() %&gt;% bind_cols(nd) -&gt; h6b_2 dat4_3 %&gt;% ggplot(aes(x=R))+ geom_point(aes(y = social_learning), size= 3, color = &quot;navy&quot;)+ geom_smooth(data = h6b_2, aes(y=Estimate, ymin = Q2.5, ymax = Q97.5), alpha = 2/3, color = &quot;black&quot;, stat = &quot;identity&quot;)+ theme(aspect.ratio=1)+ labs(x = &quot;log(research effort)&quot;) -&gt; p21 p20+p21 図11.37: fit11H6_aの推定結果 References "],["monsters-and-mixtures.html", "12 Monsters and Mixtures 12.1 Over-dispersed counts 12.2 Zero-inflated outcomes 12.3 Ordered categorical outcomes 12.4 Ordered categorical predictors 12.5 Practice", " 12 Monsters and Mixtures 12.1 Over-dispersed counts 現実のデータは様々な過程がミックスされて生成されることがあるため、理想的な分布よりも幅が広くなることがある。二項分布やポワソン分布では平均と分散が1つのパラメータで表されるため、これがおきやすい。例えば、二項分布の期待値が\\(Np\\)であれば、その分散は\\(Np(1-p)\\)となり、ポワソン分布の平均と分散は同じ\\(\\lambda\\)である。 本節では、continuous mixtureモデルと呼ばれるものを扱う。これでは、観察そのものではなく、観察の分布に対して線形モデルが適用される。実際上は、より柔軟なモデリングができる階層モデルを用いる方が過分散への対処は簡単である。 12.1.1 Beta binomial. ベータ二項分布は、二項分布の混合分布であり、それぞれのカウントに対する確率が推定される。また、それぞれのカウントが得られる確率も共通のベータ分布から得られると仮定する。ここでは、UCバークレーの入試結果の例を再び用いて例を見ていく。 ベータ分布は、平均確率\\(\\bar{p}\\)とshape parameterの\\(\\theta\\)の2つのパラメータで定義される。\\(\\theta\\)は分布の広さに関するパラメータで、\\(\\theta\\)が2のときに一様分布になる。\\(\\theta\\)が2以上になると分布は狭くなり、\\(\\theta\\)が2以下になると0と1に分布が集中していく。なお、通常ベータ分布は\\(\\alpha\\)と\\(\\beta\\)を用いて、 \\[ Beta(y|\\alpha, \\beta) = \\frac{y^{\\alpha-1} (1-y)^{\\beta-1}}{B(\\alpha, \\beta)} \\] で表される（テキストの\\(\\bar{p}\\)と\\(\\theta\\)は\\(\\alpha\\)と\\(\\beta\\)とは一致していない）。\\(\\bar{p} = \\frac{\\alpha}{\\alpha + \\beta}\\)、\\(\\theta = \\alpha + \\beta\\)であるので、\\(\\alpha = \\bar{p} \\theta\\)、\\(\\beta = \\theta(1-\\bar{p})\\) で表せる。 ## p,Θからα、βを得る関数。 transbeta &lt;- function(p, theta) { if (p &lt;= 0 | p &gt;= 1) stop(&quot;must have 0 &lt; p &lt; 1&quot;) if (theta &lt;= 0) stop(&quot;theta must be &gt; 0&quot;) a &lt;- p * theta b &lt;- (1.0 - p) * theta return(list(a = a, b = b)) } transbeta(0.5, 0.5) ## $a ## [1] 0.25 ## ## $b ## [1] 0.25 様々なベータ分布を書いてみる（図12.1）。 crossing(p = c(0.25, 0.5, 0.75), theta = c(0.5,2,5,15,30)) %&gt;% tidyr::expand(nesting(p,theta), x = seq(0,1,length.out=50)) %&gt;% mutate(density = dbeta2(x, p, theta), mu = str_c(&quot;mu == &quot;, p %&gt;% str_remove(.,&quot;0&quot;)), kappa = factor(str_c(&quot;kappa == &quot;, theta), levels = c(&quot;kappa == 30&quot;, &quot;kappa == 15&quot;, &quot;kappa == 5&quot;, &quot;kappa == 2&quot;, &quot;kappa == 0.5&quot;)))%&gt;% ggplot(aes( x=x, y = density))+ geom_area(fill = &quot;navy&quot;)+ scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, labels = NULL) + theme(axis.ticks.y = element_blank()) + facet_grid(kappa ~ mu, labeller = label_parsed) 図12.1: 様々なパラメータのベータ分布 それでは、モデリングを行う。モデル式は以下の通り。\\(\\theta\\)を2以上にするために工夫がしてある。 \\[ \\begin{aligned} A_{i} &amp;\\sim BetaBinomial(N_{i}, \\bar{p_{i}}, \\theta)\\\\ logit(\\bar{p_{i}})&amp; = \\alpha_{GID_{[i]}}\\\\ \\alpha_{j} &amp;\\sim Normal(0,1.5)\\\\ \\theta &amp;= \\phi +2\\\\ \\phi &amp;\\sim Exponential(1) \\end{aligned} \\] data(&quot;UCBadmit&quot;) d &lt;- UCBadmit %&gt;% mutate(gid = ifelse(applicant.gender == &quot;male&quot;,&quot;1&quot;,&quot;2&quot;)) brmsパッケージはβ二項分布を実装していないので、自作する必要がある。 beta_binomial2 &lt;- custom_family( &quot;beta_binomial2&quot;, dpars = c(&quot;mu&quot;, &quot;kappa&quot;), links = c(&quot;logit&quot;, &quot;log&quot;), lb = c(NA, 2), type = &quot;int&quot;, vars = &quot;vint1[n]&quot; ) stanvars &lt;- stanvar(scode = &quot; real beta_binomial2_lpmf(int y, real mu, real kappa, int T) { return beta_binomial_lpmf(y | T, mu * kappa, (1 - mu) * kappa); } int beta_binomial2_rng(real mu, real kappa, int T) { return beta_binomial_rng(T, mu * kappa, (1 - mu) * kappa); } &quot;, block = &quot;functions&quot;) それでは、モデルにフィットさせる。 b12.1 &lt;- brm(data = d, family = beta_binomial2, # here&#39;s our custom likelihood admit | vint(applications) ~ 0 + gid, prior = c(prior(normal(0, 1.5), class = b), prior(exponential(1), class = kappa)), iter = 2000, warmup = 1000, cores = 4, chains = 4, stanvars = stanvars, # note our `stanvars` seed = 12, file = &quot;output/Chapter12/b12.1&quot;) 結果は以下の通り（表\\(\\ref{res-b12-1}\\)）。 posterior_samples(b12.1) %&gt;% data.frame() %&gt;% mutate(diff = b_gid1 - b_gid2) %&gt;% pivot_longer(-lp__) %&gt;% group_by(name) %&gt;% mean_qi(value, .width=0.89) %&gt;% kable(booktabs =T, digits=2, caption = &quot;b12.1の結果果&quot;, align = &quot;lcccccc&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.1: b12.1の結果果 name value .lower .upper .width .point .interval b_gid1 -0.43 -1.13 0.22 0.89 mean qi b_gid2 -0.33 -0.96 0.31 0.89 mean qi diff -0.11 -1.06 0.82 0.89 mean qi kappa 3.04 2.10 4.51 0.89 mean qi lprior -3.83 -5.30 -2.85 0.89 mean qi 事後分布を可視化する（図12.2）。 post &lt;- posterior_samples(b12.1) ## 確率の事後分布 post %&gt;% mutate(iter= 1:n(), p_bar = inv_logit_scaled(b_gid2)) %&gt;% slice_sample(n=100) %&gt;% tidyr::expand(nesting(iter, p_bar, kappa), x = seq(0,1,by=.005)) %&gt;% mutate(density = dbeta2(x, p_bar, kappa)) %&gt;% ggplot(aes(x=x, y=density))+ stat_function(fun = dbeta2, args = list(prob = mean(inv_logit_scaled(post[, &quot;b_gid2&quot;])), theta = mean(post[, &quot;kappa&quot;])), size = 1.5, color = &quot;navy&quot;)+ geom_line(aes(group=iter), alpha = .2, color = &quot;navy&quot;)+ scale_y_continuous(NULL, breaks = NULL, limits = c(0, 3)) + labs(subtitle = &quot;distribution of female admission rates&quot;, x = &quot;probability admit&quot;)+ theme(aspect.ratio=1) 図12.2: 女性の合格率の事後分布 expose_functions(b12.1, vectorize = TRUE) # required to use `predict()` log_lik_beta_binomial2 &lt;- function(i, prep) { mu &lt;- prep$dpars$mu[, i] kappa &lt;- prep$dpars$kappa trials &lt;- prep$data$vint1[i] y &lt;- prep$data$Y[i] beta_binomial2_lpmf(y, mu, kappa, trials) } posterior_predict_beta_binomial2 &lt;- function(i, prep, ...) { mu &lt;- prep$dpars$mu[, i] kappa &lt;- prep$dpars$kappa trials &lt;- prep$data$vint1[i] beta_binomial2_rng(mu, kappa, trials) } # required to use `fitted()` posterior_epred_beta_binomial2 &lt;- function(prep) { mu &lt;- prep$dpars$mu trials &lt;- prep$data$vint1 trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE) mu * trials } 推定された結果を図示すると以下の通り(図12.3)。予測分布の範囲にうまくデータが納まっているが、広すぎて結果の解釈が難しい。 fitted(b12.1) %&gt;% data.frame() %&gt;% bind_cols(d) %&gt;% mutate(case=1:n()) -&gt; fitb12.1 #上手く回らず predict(b12.1) %&gt;% data.frame() %&gt;% bind_cols(d) %&gt;% mutate(case=1:n()) -&gt; pre12.1 d %&gt;% mutate(case = 1:n()) %&gt;% ggplot(aes(x = case))+ geom_point(aes(y = admit/applications), color = &quot;navy&quot;, size=2)+ geom_pointinterval(data = fitb12.1, aes(y=Estimate/applications, ymin = Q2.5/applications, ymax = Q97.5/applications), point_size = 3, shape=1)+ geom_point(data = pre12.1, aes(y = Q2.5/applications), size=3, shape=3)+ geom_point(data = pre12.1, aes(y = Q97.5/applications), size=3, shape=3)+ scale_x_continuous(breaks = 1:12)+ scale_y_continuous(breaks = seq(0,1,0.2), limits=c(0,1))+ labs(y = &quot;A&quot;) 図12.3: b12.1の事後予測分布。白抜き点とエラーバーは95%信用区間を表す。十字の点は95%予測区間の上限と下限を表す。 12.1.2 Negative binomial or gamma-Poisson. 続いて、ポワソン分布で過分散が起きた際に用いる負の二項分布(もしくはガンマ-ポワソン分布)について学ぶ。負の二項分布はそれぞれの観察に対して平均が与えられ、それらは共通のガンマ分布からもたらされるものとする。 ガンマ分布は、shape(\\(\\alpha\\))とrate(\\(\\beta\\))の2つのパラメータで表される。 \\[ Gamma(y| \\alpha, \\beta) = \\frac{\\beta^\\alpha y^{\\alpha-1}e^{-\\beta y}}{\\Gamma(\\alpha)} \\] それでは、実際にモデリングしてみよう。下式で、\\(\\lambda\\)はrateを、\\(\\phi\\)はshapeを表す。 \\[ y_{i} \\sim Gamma-Poisson(\\lambda_{i}, \\phi) \\] オセアニアの道具数に関するモデリングを再び考える(Kline and Boyd 2010)。 data(Kline) d2 &lt;- Kline %&gt;% mutate(P = standardize(log(population)), contact_id = ifelse(contact == &quot;high&quot;, 2L, 1L), cid = contact) -&gt; d2 モデル式は以下の通り。 \\[ \\begin{aligned} total_tools_{i} &amp;\\sim GammaPoisson(\\mu_{i}, \\alpha)\\\\ \\mu_{i} &amp;\\sim exp(\\beta_{0,cid_{[i]}}) P_{i}^{\\beta_{1,cid_{[i]}}}/\\gamma\\\\ \\beta_{0,j} &amp;\\sim Normal(1,1)\\\\ \\beta_{1,j} &amp;\\sim Exponential(1)\\\\ \\gamma &amp;\\sim Exponential(1)\\\\ \\alpha &amp;\\sim Exponential(1) \\end{aligned} \\] b12.2 &lt;- brm(data = d2, family = negbinomial(link = &quot;identity&quot;), bf(total_tools ~ exp(b0)*population^b1/g, b0 + b1 ~ 0 + cid, g ~1, nl = TRUE), prior = c(prior(normal(1, 1), nlpar = b0), prior(exponential(1), nlpar = b1, lb = 0), prior(exponential(1), nlpar = g, lb = 0), prior(exponential(1), class = shape)), seed = 12, control = list(adapt_delta = .95), chains =4, cores =4, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter12/b12.2&quot;) 分析の結果は以下の通り（表12.2）。 posterior_summary(b12.2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter != &quot;lp__&quot;) %&gt;% kable(digits =2, booktabs = TRUE, caption = &quot;b12.2の結果&quot;, align = &quot;lcccc&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.2: b12.2の結果 parameter Estimate Est.Error Q2.5 Q97.5 b_b0_cidhigh 1.05 0.92 -0.77 2.80 b_b0_cidlow 0.92 0.80 -0.68 2.51 b_b1_cidhigh 0.26 0.13 0.03 0.52 b_b1_cidlow 0.24 0.10 0.07 0.43 b_g_Intercept 1.01 0.80 0.15 3.01 shape 3.61 1.59 1.23 7.33 lprior -7.72 2.00 -12.26 -4.48 PSISを計算すると、ポワソン分布を使った場合に比べると、pareto-kが改善している。 b11.11 &lt;- readRDS(&quot;output/Chapter11/b11.11.rds&quot;) b12.2 &lt;- add_criterion(b12.2, &quot;loo&quot;) #ポワソン loo(b11.11) ## ## Computed from 4000 by 10 log-likelihood matrix ## ## Estimate SE ## elpd_loo -40.7 6.0 ## p_loo 5.3 1.9 ## looic 81.3 11.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 7 70.0% 288 ## (0.5, 0.7] (ok) 3 30.0% 151 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. #負の二項分布 loo(b12.2) ## ## Computed from 4000 by 10 log-likelihood matrix ## ## Estimate SE ## elpd_loo -41.5 1.7 ## p_loo 1.2 0.2 ## looic 83.0 3.3 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 9 90.0% 1588 ## (0.5, 0.7] (ok) 1 10.0% 564 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. ## Poisson nd &lt;- distinct(d2, cid) %&gt;% tidyr::expand(cid, population = seq(from = 0, to = 300000, length.out = 100)) # compute the poster predictions for lambda fitted(b11.11, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = population, group = cid))+ geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, linetype = cid, fill = cid, color = cid), stat = &quot;identity&quot;, size=1.5, alpha=1/2)+ geom_point(data = bind_cols(d2, b11.11$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, color = cid),stroke = 1.5, alpha = 1/2)+ scale_shape_manual(values = c(16,1))+ scale_color_manual(values = c(&quot;lightblue3&quot;,&quot;orange3&quot;))+ scale_fill_manual(values = c(&quot;lightblue&quot;, &quot;orange&quot;))+ scale_x_continuous(breaks = seq(0,250000,by=50000))+ coord_cartesian(xlim = c(0,280000), ylim = c(0, 80))+ theme(legend.position = &quot;none&quot;, aspect.ratio=1)+ labs(subtitle = &quot;pure Poisson model&quot;)-&gt; p1 ## negbinomial text &lt;- distinct(d2, cid) %&gt;% mutate(population = c(150000, 110000), total_tools = c(57, 69), label = str_c(cid, &quot; contact&quot;)) # compute the poster predictions for lambda fitted(b12.2, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = population, group = cid))+ geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, linetype = cid, fill = cid, color = cid), stat = &quot;identity&quot;, size=1.5, alpha=1/2)+ geom_point(data = bind_cols(d2, b12.2$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, color = cid),stroke = 1.5, alpha = 1/2)+ scale_shape_manual(values = c(16,1))+ scale_color_manual(values = c(&quot;lightblue3&quot;,&quot;orange3&quot;))+ scale_fill_manual(values = c(&quot;lightblue&quot;, &quot;orange&quot;))+ scale_x_continuous(breaks = seq(0,250000,by=50000))+ coord_cartesian(xlim = c(0,280000), ylim = c(0, 80))+ theme(legend.position = &quot;none&quot;, aspect.ratio=1) + geom_text(data = text, aes(y = total_tools, label = label))+ labs(subtitle = &quot;gamma-Poisson model&quot;)-&gt;p2 結果を示すと以下の通り（図12.4）。ポワソン分布に比べて、かなり広がりが大きくなっていることが分かる。 p1+p2 図12.4: ポワソン分布（左）と負の二項分布（右）による結果 また、国ごとの予測分布は以下のようになる（図12.5）。 predict(b12.2, summary = FALSE) %&gt;% data.frame() %&gt;% set_names(d2$culture) %&gt;% pivot_longer(everything(), names_to = &quot;culture&quot;, values_to = &quot;tools&quot;) %&gt;% left_join(d2) -&gt; predict ggplot(predict, aes(x = tools))+ stat_halfeye(point_interval= mean_qi, .width =.5, fill = &quot;navy&quot;, color =&quot;grey&quot;)+ geom_vline(aes(xintercept = total_tools), color = &quot;grey&quot;)+ scale_x_continuous(expression(lambda[&quot;[culture]&quot;]), breaks = 0:2 * 100) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 210)) + facet_wrap(~ culture, nrow = 2) 図12.5: 国ごとの予測分布 12.1.3 Over-dispersion, entropy, and information criteria. これらのモデルにWAICやPSISを用いることはできない。 12.2 Zero-inflated outcomes データが複数のプロセスを混合して得られることがある。そのようなデータに対しては混合モデルが有用である。本節では、特に0が多いカウントデータを扱う。 12.2.1 Example: Zero-inflated Poisson. 11章で考えた修道院の例を考える。何日かに一度僧侶が全員休みを取って酒を飲む場合を考える。この場合、ある日に1つも原稿ができないのは、働いているのに完成しなかった場合と、酒を飲んでいる場合が両方含まれている。ここで、酒を飲む確率を\\(p\\)、働いているときに1日当たり原稿ができる数の平均を\\(\\lambda\\)とする。 この現象を考えるため、二つのプロセスが混合された尤度関数を考える。まず、コイン投げで酒を飲むかか否かが決まり（ベルヌーイ分布）、その後働いた場合の原稿製造数がポワソン分布に従うとする。このとき、ある日の製造数が0である確率は以下の通り。 \\[ \\begin{aligned} Pr(0|p, \\lambda) &amp;= Pr(drink|p) + Pr(work|p) \\times Pr(0|\\lambda)\\\\ &amp;= p + (1-p)exp(-\\lambda) \\end{aligned} \\] また、0以外の値が得られる確率は以下の通り。 \\[ Pr(y|y&gt;0, p,\\lambda) = (1-p) \\frac{\\lambda^y exp(-\\lambda)}{y!} \\] モデル式にすると以下の通り。２つの線形モデルとリンク関数が含まれる点に注意。 \\[ \\begin{aligned} y_{i} &amp;\\sim ZIPoisson(p_{i}, \\lambda)\\\\ logit(p_{i}) &amp;= \\alpha_{p} + \\beta_{p}x_{i}\\\\ log(\\lambda_{i}) &amp;= \\alpha_{\\lambda} + \\beta_{\\lambda}x_{i}\\\\ \\end{aligned} \\] データをシミュレートしてモデリングする。 p &lt;- 0.2 lambda &lt;- 1 N &lt;- 365 set.seed(365) drink &lt;- rbinom(N, 1, p) y &lt;- (1-drink)*rpois(N, lambda) データをヒストグラムに書くと以下のようになる（図12.6）。 d3 &lt;- tibble(drink = factor(drink, levels = 1:0), y = y) ggplot(d3, aes(x=y, fill=drink))+ geom_histogram(binwidth=1, size=1/10, color = &quot;grey92&quot;)+ scale_fill_manual(values = c(&quot;lemonchiffon4&quot;, &quot;olivedrab4&quot;))+ xlab(&quot;manuscript completed&quot;)+ theme(legend.position =&quot;none&quot;, aspect.ratio=0.7) 図12.6: 1日の原稿完成数。緑は働いた日、灰色は休みの日を表す。 それでは、モデリングを行う。 b12.3 &lt;- brm(data = d3, family = zero_inflated_poisson, y~1, prior = c(prior(normal(1, 0.5), class = Intercept), prior(beta(2, 6), class = zi)), backend = &quot;cmdstanr&quot;, seed=12, file = &quot;output/Chapter12/b12.3&quot;) 結果は表12.3の通り。\\(exp(0.02)=\\) 1.0202013なので、うまく推定できているよう。 posterior_summary(b12.3) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter != &quot;lp__&quot;) %&gt;% kable(digits=2, booktabs =T, caption = &quot;b12.3の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.3: b12.3の結果 parameter Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.02 0.09 -0.16 0.18 zi 0.23 0.06 0.12 0.33 lprior -1.26 0.31 -1.95 -0.75 12.3 Ordered categorical outcomes 目的変数がカテゴリカルでかつ順序があるとき（ex. 心理テストで7件法で回答を求めるとき）、その目的変数を連続変数として扱ってはいけない。なぜなら1から2に変わるときと、5から6に変わるときでは意味が違ってくる可能性があるからである。このような変数を扱うために通常用いられるのは、累積リンク関数(cumulative link function)と呼ばれるものである。累積確率とは、それより低い値の確率を累計したもので、たとえば3になる累積確率は1, 2, 3になる確率を合計したものである。 12.3.1 Example: Moral intuition. いわゆるトロッコ問題では、問題の設定を少し変えるだけで、生じる結果が同じであっても異なる判断につながることがある。これまでの研究では、このようなヒトの道徳的判断の変化を説明する無意識の推論には少なくとも3つの原理があることを明らかにしている。 The action principle 行動によってもたらされた害は、何もしなかったことによってもたらされた同等の害よりも道徳的に悪いとされる。 The intention principle 目的のためになされた意図的な害は、副産物的な害よりも道徳的に悪いとされる。 The contact problem 物理的な接触によってもたらされた害は、そうでない害よりも道徳的に悪いとされる。 以下では、トロッコ問題について331人に7件法に回答してもらったデータを用いる。 data(Trolley) d4 &lt;- Trolley glimpse(d4) ## Rows: 9,930 ## Columns: 12 ## $ case &lt;fct&gt; cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa, fkbo… ## $ response &lt;int&gt; 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, … ## $ order &lt;int&gt; 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18, 15, … ## $ id &lt;fct&gt; 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;4… ## $ age &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, … ## $ male &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ edu &lt;fct&gt; Middle School, Middle School, Middle School, Middle School, … ## $ action &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, … ## $ intention &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, … ## $ contact &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ story &lt;fct&gt; aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car, spe, … ## $ action2 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, … 12.3.2. Describing an ordered distribution with intercepts. 問題に対する回答の分布は以下の通り（図12.7のA）。また、累積確率を表したものが図12.7のBである。最後に、対数累積オッズを考える。ある値\\(k\\)に対する対数累積オッズ\\(\\alpha_{k}\\)配下の通り。 \\[ \\alpha_{k} = log \\frac{Pr(y_{i} \\leq k )}{1 - Pr(y_{i} \\leq k)} \\] 対数累積オッズを表したものが図12.7のCである。最も大きい値の対数累積オッズは無限大になることに注意(\\(\\frac{1}{1-1} = \\infty\\))。 ## hist d4 %&gt;% ggplot(aes(x=response))+ geom_histogram(binwidth = 0.1)+ scale_x_continuous(breaks=seq(1,7,by=1))+ labs(subtitle = &quot;A: Histogram&quot;)+ theme(aspect.ratio=1) -&gt; p3 ## cumulative probability d4 %&gt;% count(response) %&gt;% mutate(pr_k = n/nrow(d4), cum = cumsum(pr_k)) %&gt;% ggplot(aes(x = response, y = cum))+ geom_point(shape=21, color=&quot;grey92&quot;, size=2.5, stroke =1.5, fill = &quot;khaki4&quot;)+ geom_line(color = &quot;khaki4&quot;)+ scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;, aspect.ratio=1)+ labs(subtitle = &quot;B: cumulative proportion&quot;) -&gt; p4 d4 %&gt;% count(response) %&gt;% mutate(pr_k = n/nrow(d4), cum = cumsum(pr_k), alpha = logit(cum)) %&gt;% filter(response&lt;7) %&gt;% ggplot(aes(x = response, y = alpha))+ geom_point(shape=21, color=&quot;grey92&quot;, size=2.5, stroke =1.5, fill = &quot;khaki4&quot;)+ geom_line(color = &quot;khaki4&quot;)+ scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;log-cumulative-odds&quot;, breaks = c(-2, -1, 0,1,2)) + coord_cartesian(xlim = c(1,7))+ theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;, aspect.ratio=1)+ labs(subtitle = &quot;C: log-cumulative-odds&quot;) -&gt; p5 p3+p4+p5 図12.7: トロッコ問題に対する回答。A: ヒストグラム、B: 累積確率、C: 対数累積オッズ. モデリングでは、これらの対数累積オッズについての事後分布を得ることが目的なので、累積確率からそれぞれの値を得る確率を計算する必要がある。 \\[ p_{k} = Pr(y_{i} = k) = Pr(y_{i} \\leq k) - Pr(y_{i} \\leq k-1) \\] さて、それでは準備が整ったのでモデリングに移ろう。モデル式は累積確率を\\(q_{k}\\)とすると以下の通り。 \\[ \\begin{aligned} R_{i} &amp;\\sim Categorical(P)\\\\ p_{1} &amp;= q_{1}\\\\ p_{k} &amp;= q_{k} - q_{k-1} \\;\\;\\; for K&gt;k&gt;1\\\\ p_{K} &amp;= 1 - q_{K-1}\\\\ logit(q_{k}) &amp;= \\alpha_{k} - \\phi_{i}\\\\ \\phi_{i} &amp;= terms \\;of\\;linear\\;model\\\\ \\alpha_{k} &amp;\\sim Normal(0,1.5) \\end{aligned} \\] それではモデリングを行う。 inits &lt;- list(`Intercept[1]` = -2, `Intercept[2]` = -1, `Intercept[3]` = 0, `Intercept[4]` = 1, `Intercept[5]` = 2, `Intercept[6]` = 2.5) inits_list &lt;- list(inits, inits, inits, inits) b12.4 &lt;- brm(data = d4, family = cumulative, response ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 4, chains = 4, inits = inits_list, file = &quot;output/Chapter12/b12.4&quot;) 結果は以下の通り（表12.4）。 posterior_samples(b12.4) %&gt;% mutate_all(inv_logit_scaled) %&gt;% pivot_longer(starts_with(&quot;b_&quot;), names_to = &quot;parameter&quot;) %&gt;% group_by(parameter) %&gt;% summarise(mean = mean(value), sd = sd(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% kable(digits = 3, booktabs = TRUE, caption = &quot;b12.4の結果。&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.4: b12.4の結果。 parameter mean sd ll ul b_Intercept[1] 0.128 0.003 0.122 0.135 b_Intercept[2] 0.220 0.004 0.212 0.228 b_Intercept[3] 0.328 0.005 0.319 0.337 b_Intercept[4] 0.562 0.005 0.552 0.571 b_Intercept[5] 0.709 0.005 0.700 0.718 b_Intercept[6] 0.854 0.003 0.847 0.861 元のデータと推定結果を比較しても、かなりうまく推定できたことが分かる（表12.5）。 d4 %&gt;% count(response) %&gt;% mutate(pr_k = n / nrow(d4), cum = cumsum(pr_k)) %&gt;% dplyr::select(response, cum) %&gt;% kable(digits =3, booktabs = TRUE, caption = &quot;元データにおける累積確率&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.5: 元データにおける累積確率 response cum 1 0.128 2 0.220 3 0.328 4 0.562 5 0.709 6 0.854 7 1.000 12.3.2 Adding predictor variables. 説明変数を加えるためには、それぞれの値\\(k\\)について対数累積オッズを以下のようにあらわす。このとき、\\(\\beta\\)が増えるほど対数累積オッズも増えることに注意。 \\[ \\begin{aligned} log \\frac{Pr(y_{i} \\leq k )}{1 - Pr(y_{i} \\leq k)} &amp;= \\alpha_{k} -\\phi_{i} \\\\ \\phi_{i} &amp;= \\beta x_{i} \\end{aligned} \\] contactには、actionも伴う。そのため、このデータではcontactとactionを互いに排他的な変数としており、contactとactionを同時に調べることはしていない。よって、条件は以下の6つの場合に分かれるはずである。 (1) No action, contact, or intention (2) Action only (3) Intention only (4) Contact only (5) Action and intention (6) Contact and intention よって以下のようなモデル式になる。 \\[ \\begin{aligned} R_{i} &amp;\\sim Categorical(P)\\\\ p_{1} &amp;= q_{1}\\\\ p_{k} &amp;= q_{k} - q_{k-1} \\;\\;\\; for K&gt;k&gt;1\\\\ p_{K} &amp;= 1 - q_{K-1}\\\\ logit(q_{k}) &amp;= \\alpha_{k} - \\phi_{i}\\\\ \\phi_{i} &amp;= \\beta_{1}action_{i} + \\beta_{2}contact_{i} +(\\beta_{3} +\\beta_{4}action_{i} + \\beta_{5} contact_{i})intention_{i} \\\\ \\alpha_{k} &amp;\\sim Normal(0,1.5)\\\\ \\beta_{1},...,\\beta_{5} &amp;\\sim Normal(0,0.5) \\end{aligned} \\] それでは、モデリングを行う。 b12.5 &lt;- brm(data = d4, family = cumulative, response ~ 1 + action + contact + intention + intention:action + intention:contact, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter12/b12.5&quot;) 結果は以下の通り（表12.6）。全ての係数は負なので、いずれの場合も物語への許容性が下がることが分かる。また、\\(\\beta_{5}\\)すなわち、意図的かつアクションが伴う場合に倫理的評価はもっとも下がることが分かる（図12.8）。これは、intentionとaction単体ではそこまで大きく影響しないことを考えれば興味深い。 labs &lt;- str_c(&quot;beta[&quot;, 1:5, &quot;]&quot;) fixef(b12.5) %&gt;% data.frame() %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;b12-5の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.6: b12-5の結果 Estimate Est.Error Q2.5 Q97.5 Intercept[1] -2.64 0.05 -2.74 -2.54 Intercept[2] -1.94 0.05 -2.04 -1.85 Intercept[3] -1.35 0.05 -1.44 -1.26 Intercept[4] -0.31 0.04 -0.40 -0.23 Intercept[5] 0.36 0.04 0.27 0.45 Intercept[6] 1.26 0.05 1.17 1.36 action -0.48 0.05 -0.58 -0.37 contact -0.34 0.07 -0.48 -0.21 intention -0.29 0.06 -0.41 -0.18 action:intention -0.43 0.08 -0.59 -0.27 contact:intention -1.23 0.10 -1.42 -1.04 labs &lt;- str_c(&quot;beta[&quot;, 1:5, &quot;]&quot;) posterior_samples(b12.5) %&gt;% dplyr::select(b_action:`b_contact:intention`) %&gt;% set_names(labs) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, alpha = 1/2, linetype = 3) + stat_gradientinterval(.width = .5, size = 1, point_size = 3, shape = 21, point_fill = &quot;khaki4&quot;, fill = &quot;khaki2&quot;, color = &quot;khaki2&quot;) + scale_x_continuous(&quot;marginal posterior&quot;, breaks = -5:0 / 4) + scale_y_discrete(NULL, labels = parse(text = labs)) + coord_cartesian(xlim = c(-1.4, 0)) 図12.8: 係数の事後平均と95%信用区間 それでは、結果を図示していく。 nd &lt;- d4 %&gt;% distinct(action, contact, intention) %&gt;% mutate(combination = str_c(action, contact, intention, sep = &quot;_&quot;)) f12.5 &lt;- fitted(b12.5, nd, summary=F) f &lt;- rbind(f12.5[,,1], f12.5[,,2], f12.5[,,3], f12.5[,,4], f12.5[,,5], f12.5[,,6], f12.5[,,7]) %&gt;% data.frame() %&gt;% set_names(pull(nd, combination)) %&gt;% mutate(response = rep(1:7, each = n()/7), iter = rep(1:4000, times=7)) %&gt;% pivot_longer(-c(iter,response), names_to = c(&quot;action&quot;, &quot;contact&quot;, &quot;intention&quot;), names_sep = &quot;_&quot;, values_to = &quot;pk&quot;) %&gt;% mutate(intention = intention %&gt;% as.integer()) levels &lt;- c(&quot;action=0, contact=0&quot;, &quot;action=1, contact=0&quot;, &quot;action=0, contact=1&quot;) ## 各条件ごとの確率 f %&gt;% filter(response &lt;7) %&gt;% mutate(facet = factor(str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% group_by(iter, facet, intention) %&gt;% arrange(iter, facet, intention, response) %&gt;% mutate(prob = cumsum(pk)) %&gt;% ungroup() %&gt;% nest(data=-iter) %&gt;% slice_sample(n=50) %&gt;% unnest(data) %&gt;% ggplot(aes(x = intention, y = prob)) + geom_line(aes(group = interaction(iter, response), color = prob),alpha = 1/10) + geom_point(data = d4 %&gt;% group_by(intention, contact, action) %&gt;% count(response) %&gt;% mutate(prob = cumsum(n / sum(n)), facet = factor( str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% filter(response &lt; 7), color = &quot;olivedrab3&quot;) + scale_color_gradient(low = &quot;olivedrab4&quot;, high = &quot;olivedrab1&quot;) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = 0:1) + theme(legend.position = &quot;none&quot;, strip.background = element_blank()) + facet_wrap(~ facet) -&gt; p6 ## 予測分布 predict &lt;- predict(b12.5, nd, scale=&quot;response&quot;, summary=F) predict %&gt;% data.frame() %&gt;% set_names(pull(nd, combination)) %&gt;% pivot_longer(everything(), names_to = c(&quot;action&quot;, &quot;contact&quot;, &quot;intention&quot;), names_sep = &quot;_&quot;, values_to = &quot;response&quot;) %&gt;% mutate(facet = factor(str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% ggplot(aes(x = response, fill = intention)) + geom_bar(width = 1/3, position = position_dodge(width = .4)) + scale_fill_manual(values = c(&quot;black&quot;, &quot;navy&quot;)) + scale_x_continuous(&quot;response&quot;, breaks = 1:7) + theme(legend.position = &quot;none&quot;, strip.background = element_blank()) + facet_wrap(~ facet) -&gt; p7 結果は以下の通り（図12.9）。 p6/p7 図12.9: モデルb12.5の推定結果。各プロットは、intentionの有無によってどのように回答が変わるかを示している。上段はそれぞれの事後累積確率を、下段はモデルから予測される回答の分布を図示している。なお、下段で黒はintentionが0のときを、青はintentionが1のときを示している。 12.4 Ordered categorical predictors 説明変数にカテゴリカルな順序尺度を用いることもできる。先ほどのトロッコ問題のデータでは、回答者の最終学歴が変数として格納されている（表12.7）。順番が教育の程度とは対応していないので、順番を付け直している。それぞれの値の間の差は等間隔ではないので、連続変数としてはモデリングできない。 d4 &lt;-d4 %&gt;% mutate(edu_new = recode(edu, &quot;Elementary School&quot; = 1, &quot;Middle School&quot; = 2, &quot;Some High School&quot; = 3, &quot;High School Graduate&quot; = 4, &quot;Some College&quot; = 5, &quot;Bachelor&#39;s Degree&quot; = 6, &quot;Master&#39;s Degree&quot; = 7, &quot;Graduate Degree&quot; = 8) %&gt;% as.integer()) d4 %&gt;% distinct(edu, edu_new) %&gt;% arrange(edu_new) %&gt;% kable(booktabs = TRUE, align = &quot;lc&quot;, caption = &quot;`Trolley`に含まれる回答者の学歴&quot;) %&gt;% kable_styling(latex_options = &quot;stripe&quot;) 表12.7: Trolleyに含まれる回答者の学歴 edu edu_new Elementary School 1 Middle School 2 Some High School 3 High School Graduate 4 Some College 5 Bachelor’s Degree 6 Master’s Degree 7 Graduate Degree 8 そこで、実際のモデリングでは1つ教育の程度ががるたびの増分をパラメータとして組み込む。例えば、以下の式で\\(\\delta_{1}\\)は小学校から中学校に上がるときの増分、\\(\\delta_{2}\\)は中学校から高校中退に上がるときの増分を示す。 \\[ \\begin{aligned} \\phi_{i} &amp;= \\delta_{1} + ... + \\delta_{7} + other \\; stuff\\\\ &amp;= \\sum^{7}_{j=1} \\delta_{j} + other \\; stuff \\end{aligned} \\] 特定の人iについて\\(\\phi_{i}\\)を以下のように定める。なお、\\(E_{i}\\)はその人の最終学歴を、\\(\\delta_{j}\\)はすべてを足し合わせると1になるとする。つまり、\\(\\\\beta_{E}\\)は学歴が与える最大の効果（博士課程を卒業した人の効果）を表している。 \\[ \\phi_{i} = \\beta_{E}\\sum^{E_{i}-1}_{j=0} \\delta_{j} + other \\; stuff \\] 以下のようなモデルを考える。今回は交互作用は考えない。 \\[ \\begin{aligned} R_{i} &amp;\\sim Ordered\\;logit(\\phi_{i},\\kappa)\\\\ \\phi_{i} &amp;= \\beta_{E}\\sum^{E_{i}-1}_{j=0} \\delta_{j} +\\beta_{1}action_{i} + \\beta_{2}contact_{i} +\\beta_{3}intention_{i} \\\\ \\kappa_{k} &amp;\\sim Normal(0,1.5)\\\\ \\beta_{1},\\beta_{2},\\beta_{3},\\beta_{E} &amp;\\sim Normal(0,1)\\\\ \\delta &amp;\\sim Dirichlet(\\alpha) \\end{aligned} \\] ディリクレ分布とは、ベータ分布の3変量以上のバージョンであり、パラメータベクトル\\(\\alpha\\)によって分布の形が変わる。図12.10は、\\(\\alpha = \\{2,2,2,2,2,2,2\\}\\)で10回シミュレーションを行った結果。 library(gtools) set.seed(1805) delta &lt;- rdirichlet(10, alpha = rep(2, 7)) delta %&gt;% data.frame() %&gt;% set_names(1:7) %&gt;% mutate(row = 1:n()) %&gt;% pivot_longer(-row, names_to = &quot;index&quot;) %&gt;% ggplot(aes(x = index, y = value, group = row, alpha = row == 3, color = row == 3)) + geom_line() + geom_point() + scale_alpha_manual(values = c(1/3, 1)) + scale_color_manual(values = c(&quot;olivedrab3&quot;,&quot;olivedrab4&quot;)) + ylab(&quot;probability&quot;) + theme(legend.position = &quot;none&quot;, aspect.ratio=0.7) 図12.10: ディリクレ分布から10回シミュレートしたデータ。太線はそのうちの1つを表している。 それでは、モデリングを行おう。brmsパッケージでは、順序カテゴリカルの変数をmo()の中に入れるだけでよい。推定には時間がかなりかかる（1時間以上かかる…）。 b12.6 &lt;- brm(data = d4, family = cumulative, response ~ 1 + action + contact + intention + mo(edu_new), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = b), prior(normal(0, 0.5), class = b, coef = moedu_new), prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter12/b12.6&quot;) 結果は以下の通り（表12.8）。\\(\\beta_{E}\\)の推定値が教科書よりもかなり小さい。これは、brmsでは教科書と違って平均をとっているからである。つまり7倍すれば概ね同じ値になる。係数は負なので、より教育レベルの高い人ほど倫理的に許容できないと答えるということになる。 fixef(b12.6) %&gt;% data.frame() %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;b12.6の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.8: b12.6の結果 Estimate Est.Error Q2.5 Q97.5 Intercept[1] -3.14 0.17 -3.53 -2.85 Intercept[2] -2.45 0.17 -2.85 -2.17 Intercept[3] -1.87 0.17 -2.26 -1.59 Intercept[4] -0.85 0.17 -1.24 -0.57 Intercept[5] -0.18 0.17 -0.58 0.09 Intercept[6] 0.73 0.17 0.34 1.01 action -0.71 0.04 -0.78 -0.63 contact -0.96 0.05 -1.06 -0.86 intention -0.72 0.04 -0.79 -0.65 moedu_new -0.05 0.03 -0.11 -0.01 posterior_samples(b12.6) %&gt;% transmute(bE = bsp_moedu_new * 7) %&gt;% median_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% kable(booktabs = TRUE) bE .lower .upper .width .point .interval -0.36 -0.69 -0.11 0.89 median qi なお、\\(\\delta\\)は以下の通り(表12.9。教科書の推定値とほとんど一致している。 ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 + action + contact + intention + mo(edu_new) ## Data: d4 (Number of observations: 9930) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -3.14 0.17 -3.53 -2.85 1.00 1691 2033 ## Intercept[2] -2.45 0.17 -2.85 -2.17 1.00 1686 1728 ## Intercept[3] -1.87 0.17 -2.26 -1.59 1.00 1685 1755 ## Intercept[4] -0.85 0.17 -1.24 -0.57 1.00 1709 1751 ## Intercept[5] -0.18 0.17 -0.58 0.09 1.00 1694 1804 ## Intercept[6] 0.73 0.17 0.34 1.01 1.00 1716 1707 ## action -0.71 0.04 -0.78 -0.63 1.00 4061 2904 ## contact -0.96 0.05 -1.06 -0.86 1.00 4532 3235 ## intention -0.72 0.04 -0.79 -0.65 1.00 4798 3101 ## moedu_new -0.05 0.03 -0.11 -0.01 1.00 1744 1661 ## ## Simplex Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## moedu_new1[1] 0.26 0.15 0.04 0.60 1.00 2339 2608 ## moedu_new1[2] 0.14 0.09 0.02 0.37 1.00 4295 2403 ## moedu_new1[3] 0.19 0.11 0.03 0.42 1.00 3236 2379 ## moedu_new1[4] 0.16 0.09 0.03 0.38 1.00 3436 2300 ## moedu_new1[5] 0.04 0.04 0.00 0.14 1.00 3390 2293 ## moedu_new1[6] 0.09 0.06 0.01 0.24 1.00 3948 3016 ## moedu_new1[7] 0.12 0.07 0.02 0.29 1.00 4222 2879 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). p$mo %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;delta&quot;) %&gt;% dplyr::select(-Bulk_ESS,-Tail_ESS,-Rhat) %&gt;% kable(digits=2, booktabs = TRUE, caption = &quot;b12.6によるδの推定値&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.9: b12.6によるδの推定値 delta Estimate Est.Error l.95..CI u.95..CI moedu_new1[1] 0.26 0.15 0.04 0.60 moedu_new1[2] 0.14 0.09 0.02 0.37 moedu_new1[3] 0.19 0.11 0.03 0.42 moedu_new1[4] 0.16 0.09 0.03 0.38 moedu_new1[5] 0.04 0.04 0.00 0.14 moedu_new1[6] 0.09 0.06 0.01 0.24 moedu_new1[7] 0.12 0.07 0.02 0.29 \\(\\delta\\)の事後分布をプロットすると以下の通り（図12.11）。それぞれが負に相関しているのは、合計が1になるという制約があるためである（一方が大きくなると、他方は小さくなる）。 library(GGally) delta_labels &lt;- c(&quot;Elem&quot;, &quot;MidSch&quot;, &quot;SHS&quot;, &quot;HSG&quot;, &quot;SCol&quot;, &quot;Bach&quot;, &quot;Mast&quot;, &quot;Grad&quot;) posterior_samples(b12.6) %&gt;% dplyr::select(contains(&quot;moedu_new1&quot;)) %&gt;% set_names(str_c(delta_labels[2:8],&quot;~(delta[&quot;,1:7,&quot;])&quot;)) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller=label_parsed) + theme(strip.text = element_text(size = 8)) 図12.11: deltaの事後分布とそれぞれの相関 12.5 Practice 12.5.1 12M1 At a certain university, employees are annually rated from 1 to 4 on their productivity, with 1 being least productive and 4 most productive. In a certain department at this certain university in a certain year, the numbers of employees receiving each rating were (from 1 to 4): 12, 36, 7, 41. Compute the log cumulative odds of each rating. rating &lt;- tibble(rating = c(1,2,3,4),p = c(12,36,7,41)) rating %&gt;% mutate(prop = p/sum(p), cum = cumsum(prop)) %&gt;% mutate(logsum = logit(cum)) %&gt;% kable(digits=2, booktabs=T, align = &quot;ccccc&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) rating p prop cum logsum 1 12 0.12 0.12 -1.95 2 36 0.38 0.50 0.00 3 7 0.07 0.57 0.29 4 41 0.43 1.00 Inf 12.5.2 12M2 Make a version of Figure 12.5 for the employee ratings data given just above. rating %&gt;% mutate(prop = p/sum(p), cum = cumsum(prop), distinct = cum-prop) %&gt;% ggplot(aes(x = rating, y = cum))+ geom_point(shape=21, color=&quot;grey92&quot;, size=2.5, stroke =1.5, fill = &quot;khaki4&quot;)+ geom_line(color = &quot;khaki4&quot;)+ geom_linerange(aes(ymin = 0, ymax = cum), alpha = 1/2, color = &quot;khaki2&quot;) + geom_linerange(aes(x = rating + .025, ymin = ifelse(rating == 1, 0,distinct), ymax = cum), color = &quot;black&quot;) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + geom_text(aes(x = rating + 0.2, y = cum-0.02,label=rating))+ theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;, aspect.ratio=1) 12.5.3 12M3 Can you modify the derivation of the zero-inflated Poisson distribution (ZIPoisson) from the chapter to construct a zero-inflated binomial distribution? まずデータが得られない確率を\\(p\\),そのうえで1が得られる確率を\\(q\\)とする。このとき、0が得られる確率は以下の通り。 \\[ Pr(0|p,q,n) = p + (1-p)(1-q)^n \\] 0以外の値(1)が得られる確率は以下の通り。 \\[ Pr(y=1, p,\\lambda) = (1-p) \\frac{n!}{y!(n-y)!} \\] 12.5.4 12H1 In 2014, a paper was published that was entitled “Female hurricanes are deadlier than male hurricanes.” As the title suggests, the paper claimed that hurricanes with female names have caused greater loss of life, and the explanation given is that people unconsciously rate female hurricanes as less dangerous and so are less likely to evacuate. Statisticians severely criticized the paper after publication. Here, you’ll explore the complete data used in the paper and consider the hypothesis that hurricanes with female names are deadlier. Load the data with: Acquaint yourself with the columns by inspecting the help ?Hurricanes. In this problem, you’ll focus on predicting deaths using femininity of each hurricane’s name.Fit and interpret the simplest possible model, a Poisson model of deaths using femininity as a predictor. You can use quap or ulam. Compare the model to an intercept-only Poisson model of deaths. How strong is the association between femininity of name and deaths? Which storms does the model fit (retrodict) well? Which storms does it fit poorly? 2014年、女性名のハリケーンの方が男性名のものよりも被害が大きいという論文が出版された(Jung et al. 2014)。論文の著者は、女性名の場合に人々は無意識にリスクを少なく見積もってしまい、避難することが少なくなってしまうと考察している。統計学者はこの論文を批判している(Bakkensen and Larson 2014)。データを見てみよう（表12.10）。 data(Hurricanes) dat &lt;- Hurricanes head(dat) %&gt;% kable(booktabs=T, caption = &quot;Jung et al.(2014)のデータ&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.10: Jung et al.(2014)のデータ name year deaths category min_pressure damage_norm female femininity Easy 1950 2 3 960 1590 1 6.77778 King 1950 4 3 955 5350 0 1.38889 Able 1952 3 1 985 150 0 3.83333 Barbara 1953 1 1 987 58 1 9.83333 Florence 1953 0 1 985 15 1 8.33333 Carol 1954 60 3 960 19321 1 8.11111 台風の名前の女性らしさ(feminity)が死者数(death)に影響しているかをモデリングする。 dat &lt;- dat %&gt;% mutate(F = standardize(femininity)) b12H1 &lt;- brm(data = dat, family = poisson, deaths ~ 1 + F, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H1&quot;) 結果は以下の通り（表12.11）。名前の女性らしさは影響しているという結果が出た。 posterior_summary(b12H1, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H1の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.11: b12H1の結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept 3.00 0.02 2.96 3.04 b_F 0.24 0.03 0.20 0.28 lprior -1.17 0.01 -1.19 -1.17 PSISを求めてみると、\\(pareto\\)-\\(k\\)が高い点があることが分かる。 b12H1 &lt;- add_criterion(b12H1, &quot;loo&quot;) loo(b12H1) %&gt;% pareto_k_ids(threshold = .7) -&gt; k tibble(name = dat$name[k], pareto_k = pareto_k_values(loo(b12H1))[k]) %&gt;% kable(booktabs = TRUE, digits =2) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) name pareto_k Diane 2.01 Camille 2.14 Agnes 0.73 Andrew 0.73 Ike 1.10 Sandy 1.21 実際に推定結果を描写してみる（図12.12）。傾きはかなり小さく、当てはまりもあまりよくなさそうだということが分かる。 nd &lt;- tibble(F = seq(-2,1.2, length.out=100)) fitted(b12H1,nd) %&gt;% data.frame() %&gt;% bind_cols(nd) -&gt; fit12H1 dat &lt;- dat %&gt;% mutate(pareto_k = pareto_k_values(loo(b12H1))) dat %&gt;% ggplot(aes(x = F))+ geom_point(aes(y = deaths, size = pareto_k), alpha = 1/2)+ geom_smooth(data = fit12H1, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;navy&quot;,color = &quot;navy&quot;, alpha = 1/2)+ geom_text_repel(data = dat %&gt;% filter(pareto_k &gt; .7), aes(y = deaths,label = name), color = &quot;red&quot;)+ theme(aspect.ratio=1, legend.position = &quot;none&quot;)+ scale_y_continuous(breaks= seq(0,250,50))+ labs(x = &quot;femininity (std)&quot;, subtitle = &quot;Poisson&quot;) 図12.12: b12H1の結果を図示したもの 切片だけのモデルと比較してみると、いちおう説明変数がある方が予測は向上している。 b12H1_2 &lt;- brm(data = dat, family = poisson, deaths ~ 1, prior = prior(normal(3, 0.5),class=Intercept), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H1_2&quot;) b12H1_2 &lt;- add_criterion(b12H1_2, &quot;loo&quot;) loo_compare(b12H1, b12H1_2) %&gt;% data.frame() %&gt;% dplyr::select(-se_p_loo) %&gt;% kable(booktabs = TRUE, digits=2) %&gt;% kable_styling(latex_options = &quot;hold_position&quot;) elpd_diff se_diff elpd_loo se_elpd_loo p_loo looic se_looic b12H1 0.00 0.00 -2193.33 494.30 118.94 4386.65 988.59 b12H1_2 -17.54 70.56 -2210.87 532.54 66.43 4421.74 1065.08 12.5.5 12H2 Counts are nearly always over-dispersed relative to Poisson. So fit a gamma-Poisson (aka negative-binomial) model to predict deaths using femininity. Show that the over-dispersed model no longer shows as precise a positive association between femininity and deaths, with an 89% interval that overlaps zero. Can you explain why the association diminished in strength? 負の二項分布へのあてはめを行う。 b12H2 &lt;- brm(data = dat, family = negbinomial, deaths ~ 1 + F, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b), prior(exponential(1), class = shape)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H2&quot;) 結果、先ほどよりも係数が小さくなり、89%信用区間も0をまたいでいることが分かる(表12.12)。 posterior_summary(b12H2, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H2の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.12: b12H2の結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept 3.02 0.15 2.78 3.26 b_F 0.21 0.16 -0.04 0.46 shape 0.45 0.06 0.36 0.56 lprior -1.68 0.10 -1.85 -1.55 一方、\\(pareto\\)-\\(k\\)が0.7以上のデータは一つもなくなり、過分散は解消された。 b12H2 &lt;- add_criterion(b12H2, &quot;loo&quot;) loo(b12H2) %&gt;% pareto_k_ids(threshold = .7) ## integer(0) 結果を図示すると以下の通り（図12.13。推定の幅が広くなっており、女性らしさの影響はほとんどないことが分かる。 nd &lt;- tibble(F = seq(-2,1.2, length.out=100)) fitted(b12H2,nd) %&gt;% data.frame() %&gt;% bind_cols(nd) -&gt; fit12H2 dat &lt;- dat %&gt;% mutate(pareto_k = pareto_k_values(loo(b12H2))) dat %&gt;% ggplot(aes(x = F))+ geom_point(aes(y = deaths, size = pareto_k), alpha = 1/2)+ geom_smooth(data = fit12H2, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;navy&quot;,color = &quot;navy&quot;, alpha = 1/3)+ theme(aspect.ratio=1, legend.position = &quot;none&quot;)+ scale_y_continuous(breaks= seq(0,250,50))+ labs(x = &quot;femininity (std)&quot;, subtitle = &quot;negative binomial&quot;) 図12.13: b12H2の結果を図示したもの 12.5.6 12H3 In order to infer a strong association between deaths and femininity, it’s necessary to include an interaction effect. In the data, there are two measures of a hurricane’s potential to cause death: damage_norm and min_pressure. Consult ?Hurricanes for their meanings. It makes some sense to imagine that femininity of a name matters more when the hurricane is itself deadly. This implies an interaction between femininity and either or both of damage_norm and min_pressure. Fit a series of models evaluating these interactions. Interpret and compare the models. In interpreting the estimates, it may help to generate counterfactual predictions contrasting hurricanes with masculine and feminine names. Are the effect sizes plausible? 台風の規模そのものの影響も考慮したモデルを考えるため、min pressure（小さいほど規模は大きい）も入れたモデルを考える。交互作用ありのモデル(b12H3)と交互作用なしのモデル(b12H3b)の両方を考える。 dat %&gt;% mutate(M = standardize(min_pressure)) -&gt; dat b12H3 &lt;- brm(data = dat, family = negbinomial, deaths ~ 1 + F + M + F:M, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b), prior(exponential(1), class = shape)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H3&quot;) b12H3b &lt;- brm(data = dat, family = negbinomial, deaths ~ 1 + F + M, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b), prior(exponential(1), class = shape)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H3b&quot;) 交互作用ありモデルの結果は以下の通り（表12.13）。交互作用がかなり効いているという結果になった。この結果は、規模が小さくなるほど女性らしさが死者数に与える影響が強くなる、もしくは名前が女性らしくなるほど規模が死者数に与える影響が弱くなることを意味している。 posterior_summary(b12H3, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H3の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.13: b12H3の結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept 2.81 0.14 2.59 3.03 b_F 0.30 0.15 0.06 0.53 b_M -0.67 0.14 -0.90 -0.45 b_F:M 0.30 0.15 0.06 0.54 shape 0.55 0.08 0.43 0.69 lprior -4.01 0.19 -4.35 -3.73 当てはまりの悪い点は1点のみ。 b12H3 &lt;- add_criterion(b12H3, &quot;loo&quot;) loo(b12H3) %&gt;% pareto_k_ids(threshold = .7) ## [1] 10 交互作用も入れた方が予測は若干向上する(ただし、ほとんど変わらない)。 b12H3b &lt;- add_criterion(b12H3b, &quot;loo&quot;) loo_compare(b12H3,b12H3b, b12H2) %&gt;% data.frame() %&gt;% kable(booktabs = TRUE, digits=2, caption = &quot;b12H3,b12H3b, b12H2の比較&quot;) %&gt;% kable_styling(latex_options = &quot;hold_position&quot;) 表12.14: b12H3,b12H3b, b12H2の比較 elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic b12H3b 0.00 0.00 -348.39 19.86 7.80 4.98 696.79 39.72 b12H3 -0.33 1.48 -348.73 19.93 9.79 6.14 697.46 39.86 b12H2 -6.24 9.37 -354.63 16.04 3.43 1.00 709.26 32.08 名前が女性らしいとき(F = 1)と最も男性らしいとき(F = -1)で台風の規模が死者数に与える影響が異なるかを図示すると以下のようになる（図12.14）。 nd &lt;- crossing(F = c(-1,1), M = seq(-3,2,length.out=100)) fitted(b12H3, nd, probs=c(0.055,0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(MorF = factor(F)) -&gt; fit12H3 dat %&gt;% mutate(MorF = ifelse(F &gt; mean(F),&quot;1&quot;,&quot;-1&quot;)) %&gt;% ggplot(aes(x = M))+ geom_point(aes(y=deaths, color = MorF), shape =1, stroke = 1.5, size=1)+ geom_smooth(data = fit12H3, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, color = MorF, fill = MorF, linetype=MorF), stat = &quot;identity&quot;)+ scale_color_manual(values = c(&quot;steelblue1&quot;,&quot;lightcoral&quot;))+ scale_fill_manual(values = c(&quot;steelblue1&quot;,&quot;lightcoral&quot;))+ coord_cartesian(ylim = c(0,300))+ labs(x=&quot;minimum pressure (std)&quot;)+ theme(aspect.ratio=1, legend.position = &quot;none&quot;) 図12.14: b12H3の推定結果。点は、Fが0以下のものを水色、0以上のものをピンクとしており、回帰曲線は水色がF=-1, ピンクがF=1のときを表す。塗りつぶしは89%信用区間である。 続いて、台風の損害(ドル換算)と名前の女性らしさの交互作用を考慮したモデルを考える。 dat %&gt;% mutate(D = standardize(damage_norm)) -&gt; dat b12H3_2 &lt;- brm(data = dat, family = negbinomial, deaths ~ 1 + F + D + F:D, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b), prior(exponential(1), class = shape)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H3_2&quot;) b12H3_2b &lt;- brm(data = dat, family = negbinomial, deaths ~ 1 + F + D, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b), prior(exponential(1), class = shape)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H3_2b&quot;) 交互作用ありモデルの結果は以下の通り（表12.15）。被害の大きさがかなり強く影響しており、名前の女性らしさの影響が小さくなっていることが分かる。交互作用もありそう。 posterior_summary(b12H3_2, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H3_2の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.15: b12H3_2の結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept 2.62 0.13 2.41 2.84 b_F 0.09 0.13 -0.13 0.29 b_D 1.24 0.22 0.91 1.60 b_F:D 0.31 0.20 -0.03 0.63 shape 0.68 0.10 0.53 0.85 lprior -4.88 0.38 -5.52 -4.30 当てはまりの悪い点はなさそう。 b12H3_2 &lt;- add_criterion(b12H3_2, &quot;loo&quot;) b12H3_2b &lt;- add_criterion(b12H3_2b, &quot;loo&quot;) loo(b12H3) %&gt;% pareto_k_ids(threshold = .7) ## [1] 10 交互作用が入ったモデルの方が若干よいが、あまり変わらない。 loo_compare(b12H3_2,b12H3_2b) %&gt;% data.frame() %&gt;% kable(booktabs = TRUE, digits=2) elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic b12H3_2b 0.00 0.00 -335.36 16.57 5.33 1.30 670.72 33.14 b12H3_2 -0.04 2.15 -335.40 16.74 6.73 1.67 670.80 33.49 名前が女性らしいとき(F = 1)と最も男性らしいとき(F = -1)で被害規模が死者数に与える影響が異なるかを図示すると以下のようになる（図12.15）。名前が女性らしいときに若干死者数は大きくなる。 nd &lt;- crossing(F = c(-1,1), D = seq(-1,5.5,length.out=100)) fitted(b12H3_2, nd, probs=c(0.055,0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(MorF = factor(F)) -&gt; fit12H3_2 dat %&gt;% mutate(MorF = ifelse(F &gt; mean(F),&quot;1&quot;,&quot;-1&quot;)) %&gt;% ggplot(aes(x = D))+ geom_point(aes(y=deaths, color = MorF), shape =1, stroke = 1.5, size=1)+ geom_smooth(data = fit12H3_2, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, color = MorF, fill = MorF, linetype=MorF), stat = &quot;identity&quot;)+ scale_color_manual(values = c(&quot;steelblue1&quot;,&quot;lightcoral&quot;))+ scale_fill_manual(values = c(&quot;steelblue1&quot;,&quot;lightcoral&quot;))+ coord_cartesian(ylim = c(0,300))+ labs(x = &quot;damage (std)&quot;)+ theme(aspect.ratio=1, legend.position = &quot;none&quot;) 図12.15: b12H3_2の推定結果。点は、Fが0以下のものを水色、0以上のものをピンクとしており、回帰曲線は水色がF=-1, ピンクがF=1のときを表す。塗りつぶしは89%信用区間である。 ここで、これまで出てきたすべての変数とその交互作用を入れたモデルを考える(なお、MとDの交互作用は入れない)。 b12H3_all &lt;- brm(data = dat, family = negbinomial, deaths ~ 1 + F + D + F:D + M + F:M, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b), prior(exponential(1), class = shape)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H3_all&quot;) b12H3_all &lt;- add_criterion(b12H3_all, &quot;loo&quot;) これまでの全てのモデルのPSISを比べてみると、全部入れたモデル(b12H3_all)が最も予測がよいことが分かる。b12H3_2とb12H3_2bはそれより少し悪くなり、それ以外はもっと悪い。 loo_compare(b12H3_2,b12H2,b12H3, b12H3b, b12H3_2b, b12H3_all) %&gt;% data.frame() %&gt;% kable(booktabs = TRUE, digits=2, caption = &quot;全モデルの比較&quot;) %&gt;% kable_styling(latex_options = &quot;hold_position&quot;) 表12.16: 全モデルの比較 elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic b12H3_all 0.00 0.00 -333.35 18.08 10.06 3.88 666.70 36.16 b12H3_2b -2.01 5.09 -335.36 16.57 5.33 1.30 670.72 33.14 b12H3_2 -2.05 4.81 -335.40 16.74 6.73 1.67 670.80 33.49 b12H3b -15.04 7.17 -348.39 19.86 7.80 4.98 696.79 39.72 b12H3 -15.38 7.00 -348.73 19.93 9.79 6.14 697.46 39.86 b12H2 -21.28 6.89 -354.63 16.04 3.43 1.00 709.26 32.08 全部入れたモデルの結果は以下の通り（表12.17）。D, Mが強く効いており、D:F, M:Fも効いていそう。 posterior_summary(b12H3_all, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H3_allの結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.17: b12H3_allの結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept 2.57 0.13 2.37 2.78 b_F 0.11 0.13 -0.10 0.32 b_D 0.93 0.22 0.58 1.29 b_M -0.52 0.16 -0.79 -0.25 b_F:D 0.59 0.21 0.25 0.91 b_F:M 0.34 0.18 0.06 0.62 shape 0.75 0.12 0.58 0.94 lprior -6.91 0.37 -7.53 -6.33 12.5.7 12H4 In the original hurricanes paper, storm damage (damage_norm) was used directly. This assumption implies that mortality increases exponentially with a linear increase in storm strength, because a Poisson regression uses a log link. So it’s worth exploring an alternative hypothesis: that the logarithm of storm strength is what matters. Explore this by using the logarithm of damage_norm as a predictor. Using the best model structure from the previous problem, compare a model that uses log(damage_norm) to a model that uses damage_norm directly. Compare their DIC/WAIC values as well as their implied predictions. What do you conclude? damageの対数をとったものを説明変数とする。 dat &lt;- dat %&gt;% mutate(logD = standardize(log(damage_norm))) b12H4 &lt;- brm(data = dat, family = negbinomial, deaths ~ 1 + F + logD + F:logD + M + F:M, prior = c(prior(normal(3, 0.5),class=Intercept), prior(normal(0,1),class =b), prior(exponential(1), class = shape)), backend = &quot;cmdstanr&quot;, seed=11, file = &quot;output/Chapter12/b12H4&quot;) b12H4 &lt;- add_criterion(b12H4, &quot;loo&quot;) b12H3_allとモデル比較をしてみると、b12H4の方がかなり良いことが分かる。 loo_compare(b12H3_all, b12H4) %&gt;% data.frame() %&gt;% kable(booktabs = TRUE, digits=2) %&gt;% kable_styling(latex_options = &quot;hold_position&quot;) elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic b12H4 0.00 0.00 -318.82 16.06 8.81 2.68 637.65 32.11 b12H3_all -14.53 6.85 -333.35 18.08 10.06 3.88 666.70 36.16 結果は以下の通り（表12.18）。logD以外はほとんど推定値が0に近くなり、いずれも89%CIが0にかぶっている。 posterior_summary(b12H4, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H4の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.18: b12H4の結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept 2.30 0.12 2.11 2.49 b_F 0.03 0.12 -0.16 0.22 b_logD 1.31 0.19 1.01 1.62 b_M -0.07 0.17 -0.34 0.20 b_F:logD 0.17 0.20 -0.16 0.49 b_F:M -0.01 0.19 -0.30 0.29 shape 1.03 0.17 0.78 1.32 lprior -7.80 0.49 -8.63 -7.05 結果を図示する（図12.16）。データにはかなり良く当てはまっていることが分かる。名前の女性らしさによる影響はほとんどない。 nd &lt;- crossing(F = c(-1,1), logD = seq(-3.5,2,length.out=100), M = 0) fitted(b12H4, nd, probs=c(0.055,0.945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(MorF = factor(F)) -&gt; fit12H4 dat %&gt;% mutate(MorF = ifelse(F &gt; mean(F),&quot;1&quot;,&quot;-1&quot;)) %&gt;% ggplot(aes(x = logD))+ geom_point(aes(y=deaths, color = MorF), shape =1, stroke = 1.5, size=1)+ geom_smooth(data = fit12H4, aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, color = MorF, fill = MorF, linetype=MorF), stat = &quot;identity&quot;)+ scale_color_manual(values = c(&quot;steelblue1&quot;,&quot;lightcoral&quot;))+ scale_fill_manual(values = c(&quot;steelblue1&quot;,&quot;lightcoral&quot;))+ coord_cartesian(ylim = c(0,300))+ labs(x = &quot;damage (std)&quot;)+ theme(aspect.ratio=1, legend.position = &quot;none&quot;) 図12.16: b12H3_2の推定結果。点は、Fが0以下のものを水色、0以上のものをピンクとしており、回帰曲線は水色がF=-1, ピンクがF=1のときを表す。塗りつぶしは89%信用区間である。} 12.5.8 12H5 One hypothesis from developmental psychology, usually attributed to Carol Gilligan, proposes that women and men have different average tendencies in moral reasoning. Like most hypotheses in social psychology, it is merely descriptive. The notion is that women are more concerned with care (avoiding harm), while men are more concerned with justice and rights. Culture-bound nonsense? Yes. Descriptively accurate? Maybe. Evaluate this hypothesis, using the Trolley data, supposing that contact provides a proxy for physical harm. Are women more or less bothered by contact than are men, in these data? Figure out the model(s) that is needed to address this question. 性別とcontactを説明変数とし、交互作用があるかを調べる。 d4 &lt;- d4 %&gt;% mutate(gid = ifelse(male==1,&quot;2&quot;,&quot;1&quot;)) b12H5 &lt;- brm(data = d4, family = cumulative, response ~ 1 + contact + gid + contact:gid, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter12/b12H5&quot;) 結果は以下の通り（表12.19）。交互作用が影響していそう。 男性の方がむしろcontactの効果を強く受けているという結果。 posterior_summary(b12H5, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H5の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.19: b12H5の結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept[1] -1.75 0.04 -1.81 -1.70 b_Intercept[2] -1.10 0.03 -1.15 -1.05 b_Intercept[3] -0.54 0.03 -0.59 -0.49 b_Intercept[4] 0.46 0.03 0.41 0.51 b_Intercept[5] 1.13 0.03 1.07 1.18 b_Intercept[6] 2.03 0.04 1.97 2.09 b_contact -0.48 0.07 -0.58 -0.37 b_gid2 0.59 0.04 0.53 0.66 b_contact:gid2 -0.21 0.09 -0.36 -0.07 disc 1.00 0.00 1.00 1.00 lprior -13.32 0.06 -13.43 -13.23 結果は以下の通り（図12.17）。あまり交互作用はよくわからない。 nd &lt;- crossing(gid = c(&quot;1&quot;,&quot;2&quot;), contact = c(0,1)) %&gt;% mutate(combi = str_c(gid, contact, sep=&quot;_&quot;)) f &lt;- fitted(b12H5, nd, summary= F) f2 &lt;- rbind(f[,,1], f[,,2], f[,,3], f[,,4], f[,,5], f[,,6], f[,,7]) %&gt;% data.frame() %&gt;% set_names(nd$combi) %&gt;% mutate(response = rep(1:7, each =4000), iter = rep(1:4000, times =7)) %&gt;% pivot_longer(-c(response, iter), names_to = c(&quot;gid&quot;, &quot;contact&quot;), names_sep=&quot;_&quot;, values_to = &quot;p&quot;) f2 %&gt;% filter(response &lt; 7) %&gt;% mutate(facet = ifelse(gid == &quot;1&quot;,&quot;female&quot;,&quot;male&quot;)) %&gt;% group_by(iter, facet, contact) %&gt;% arrange(iter, facet, contact,response) %&gt;% mutate(prob = cumsum(p)) %&gt;% ungroup() %&gt;% nest(data = -iter) %&gt;% slice_sample(n=100) %&gt;% unnest(data) %&gt;% mutate(contact = as.integer(contact)) %&gt;% ggplot(aes(x = contact, y = prob))+ geom_line(aes(group=interaction(iter, response), color = prob), alpha = 1/10)+ geom_point(data =d4 %&gt;% group_by(gid, contact) %&gt;% count(response) %&gt;% mutate(prob = cumsum(n/sum(n)), facet=ifelse(gid == &quot;1&quot;,&quot;female&quot;,&quot;male&quot;)) %&gt;% filter(response&lt;7), color = &quot;olivedrab3&quot;)+ scale_color_gradient(low = &quot;olivedrab4&quot;, high = &quot;olivedrab1&quot;)+ scale_x_continuous(&quot;contact&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = 0:1) + theme(legend.position = &quot;none&quot;, strip.background = element_blank()) + facet_wrap(~ facet) 図12.17: b12H5の結果を図示したもの。 予測分布を表すと以下のようになる図12.18。全体として女性の方が道徳的に許容できない人が多い（小さいresponseの割合が高い）が、contactの効果は男性の方がある。 predict &lt;- predict(b12H5, nd, scale=&quot;response&quot;, summary=F) predict %&gt;% data.frame() %&gt;% set_names(pull(nd, combi)) %&gt;% pivot_longer(everything(), names_to = c(&quot;gid&quot;, &quot;contact&quot;), names_sep=&quot;_&quot;, values_to = &quot;response&quot;) %&gt;% mutate(facet = ifelse(gid == &quot;1&quot;,&quot;female&quot;,&quot;male&quot;))%&gt;% ggplot(aes(x = response, fill = contact)) + geom_bar(width = 1/3, position = position_dodge(width = .4)) + scale_fill_manual(values = c(&quot;black&quot;, &quot;blue&quot;)) + scale_x_continuous(&quot;response&quot;, breaks = 1:7) + theme(legend.position = &quot;none&quot;, strip.background = element_blank()) + facet_wrap(~ facet) 図12.18: b12H5による予測分布。黒いバーがcontactが0のとき、青いバーがcontactが1の時を表す。 12.5.9 12H6 The data in data(Fish) are records of visits to a national park. See ?Fish for details. The question of interest is how many fish an average visitor takes per hour, when fishing. The problem is that not everyone tried to fish, so the fish_caught numbers are zero-inflated. As with the monks example in the chapter, there is a process that determines who is fishing (working) and another process that determines fish per hour (manuscripts per day), conditional on fishing (working). We want to model both. Otherwise we’ll end up with an underestimate of rate of fish extraction from the park. You will model these data using zero-inflated Poisson GLMs. Predict fish_caught as a function of any of the other variables you think are relevant. One thing you must do, however, is use a proper Poisson offset/exposure in the Poisson portion of the zero-inflated model. Then use the hours variable to construct the offset. This will adjust the model for the differing amount of time individuals spent in the park. キャンプ場を訪れた人たちが魚をどれだけ釣ったのかをモデリングする。問題は、キャンプ場を訪れたすべての人が魚釣りをしたわけではないということである。よって、ゼロ過剰ポワソンモデルを用いてモデリングを行う。 含まれる変数は以下のものである。ここでは、図12.19をもとにcamperとchild, personが釣りをするか否かの判断に影響し、livebaitとperson, camper, childが釣った魚の数に影響していたとする。hoursは宿泊した客とそうでない客で大きく異なるため除外する。 livebait: 魚釣りに生餌を使ったか否か。 camper: キャンプをするか否か。 person: 大人の数。 child: 子供の数。 hours: キャンプ場にいた時間。 data(Fish) dat3 &lt;- Fish head(dat3) %&gt;% kable(booktabs=T, caption = &quot;データ`Fish`&quot;) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;,&quot;stripe&quot;)) 表12.20: データFish fish_caught livebait camper persons child hours 0 0 0 1 0 21.124 0 1 1 1 0 5.732 0 1 0 1 0 1.323 0 1 1 2 1 0.548 1 1 0 1 0 1.695 0 1 1 4 2 0.493 dat3 %&gt;% pivot_longer(-c(fish_caught,hours)) %&gt;% ggplot(aes(x=value, y = fish_caught))+ geom_point(aes(shape=name), color = &quot;navy&quot;, size=3, alpha = 1/2)+ labs(y = &quot;Number of fish&quot;)+ theme(strip.background = element_blank(), strip.text = element_text(size=12), legend.position = &quot;none&quot;, aspect.ratio=0.7)+ facet_wrap(~name, nrow = 2, scales = &quot;free_x&quot;) 図12.19: データFishのplot それではモデリングを行う。 dat3 %&gt;% mutate(C = ifelse(camper==&quot;1&quot;,&quot;2&quot;,&quot;1&quot;), L = ifelse(livebait == &quot;1&quot;,&quot;2&quot;,&quot;1&quot;)) -&gt; dat3 b12H6 &lt;- brm(data = dat3, family = zero_inflated_poisson, bf(fish_caught ~ C + child +L + persons, zi ~ C + child + persons), prior = c(prior(normal(3,0.5), class = Intercept), prior(normal(0, 10), class = b), prior(normal(0, 1.5), class = Intercept, dpar = zi), prior(normal(0, 10), class =b, dpar=zi)), backend = &quot;cmdstanr&quot;, seed = 11, file = &quot;output/Chapter12/b12H6&quot;) 結果は以下の通り（表12.21）。全ての説明変数が影響を与えていることが分かる。 posterior_summary(b12H6, probs = c(0.055,0.945)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% filter(parameter!=&quot;lp__&quot;) %&gt;% kable(booktabs=T, digits=2, caption = &quot;b12H5の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;stripe&quot;,&quot;hold_position&quot;)) 表12.21: b12H5の結果 parameter Estimate Est.Error Q5.5 Q94.5 b_Intercept -2.27 0.28 -2.72 -1.85 b_zi_Intercept 1.63 0.52 0.80 2.46 b_C2 0.58 0.09 0.44 0.74 b_child -1.14 0.09 -1.28 -1.00 b_L2 1.70 0.24 1.33 2.10 b_persons 0.82 0.04 0.75 0.89 b_zi_C2 -0.87 0.36 -1.46 -0.30 b_zi_child 2.06 0.34 1.54 2.61 b_zi_persons -0.98 0.21 -1.33 -0.65 lprior -33.52 0.68 -34.65 -32.47 conditional_effects(b12H6) %&gt;% plot(points = TRUE, jitter_width = 0.1, stype = &quot;contour&quot;, theme=theme(aspect.ratio=1)) 12.5.10 12H7 In the trolley data we saw how education level (models as an ordered category) is associated with responses. But is this association causal? One possible confound is that education is also associated with age, through a causal process: People are older when they finish school thatn when they begin it. Reconsider the Trolley data in this light. Draw a DAG that represents hypothetical causal relationships among response, education, and age. Which statical model or models do you need to evaluate the causal influence of education on responses ? Fit these models to the trolley data. This will adjust the model for the differing amount of time individuals spent in the park. d4 &lt;- mutate(d4, A = standardize(age)) b12H7 &lt;- brm(data = d4, family = cumulative, response ~ 1 + action + contact + intention +A+ mo(edu_new), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = b), prior(normal(0, 0.5), class = b, coef = moedu_new), prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter12/b12H7&quot;) 結果は以下の通り（表12.22）。年齢を説明変数に入れると、学歴の影響は小さくなり、係数の正負も反対になった。 fixef(b12H7) %&gt;% data.frame() %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;b12H7の結果。&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.22: b12H7の結果。 Estimate Est.Error Q2.5 Q97.5 Intercept[1] -2.70 0.10 -2.97 -2.54 Intercept[2] -2.02 0.10 -2.28 -1.86 Intercept[3] -1.43 0.10 -1.69 -1.28 Intercept[4] -0.41 0.10 -0.68 -0.25 Intercept[5] 0.26 0.10 0.00 0.42 Intercept[6] 1.17 0.10 0.89 1.33 action -0.71 0.04 -0.79 -0.63 contact -0.96 0.05 -1.06 -0.87 intention -0.72 0.04 -0.79 -0.65 A -0.10 0.02 -0.14 -0.05 moedu_new 0.03 0.02 -0.02 0.06 posterior_samples(b12H7) %&gt;% transmute(bE = bsp_moedu_new * 7) %&gt;% median_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% kable(booktabs=T) %&gt;% kable_styling(latex_options = &quot;hold_position&quot;) bE .lower .upper .width .point .interval 0.24 -0.01 0.36 0.89 median qi なお、\\(\\delta\\)は以下の通り(表12.23)。 ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 + action + contact + intention + A + mo(edu_new) ## Data: d4 (Number of observations: 9930) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -2.70 0.10 -2.97 -2.54 1.00 984 496 ## Intercept[2] -2.02 0.10 -2.28 -1.86 1.00 972 497 ## Intercept[3] -1.43 0.10 -1.69 -1.28 1.00 981 493 ## Intercept[4] -0.41 0.10 -0.68 -0.25 1.00 1005 475 ## Intercept[5] 0.26 0.10 -0.00 0.42 1.00 993 460 ## Intercept[6] 1.17 0.10 0.89 1.33 1.00 1010 482 ## action -0.71 0.04 -0.79 -0.63 1.00 2828 3058 ## contact -0.96 0.05 -1.06 -0.87 1.00 3010 3059 ## intention -0.72 0.04 -0.79 -0.65 1.00 3125 2823 ## A -0.10 0.02 -0.14 -0.05 1.00 1448 1145 ## moedu_new 0.03 0.02 -0.02 0.06 1.00 957 446 ## ## Simplex Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## moedu_new1[1] 0.11 0.08 0.01 0.31 1.00 3368 2378 ## moedu_new1[2] 0.12 0.08 0.02 0.31 1.00 3878 2486 ## moedu_new1[3] 0.09 0.07 0.01 0.26 1.00 2869 2368 ## moedu_new1[4] 0.07 0.06 0.01 0.24 1.00 1632 989 ## moedu_new1[5] 0.42 0.15 0.04 0.67 1.00 1052 503 ## moedu_new1[6] 0.09 0.06 0.01 0.23 1.00 3956 2896 ## moedu_new1[7] 0.10 0.06 0.01 0.25 1.00 4953 3026 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). p$mo %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;delta&quot;) %&gt;% dplyr::select(-Bulk_ESS,-Tail_ESS,-Rhat) %&gt;% kable(digits=2, booktabs = TRUE, caption = &quot;b12.6によるδの推定値&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表12.23: b12.6によるδの推定値 delta Estimate Est.Error l.95..CI u.95..CI moedu_new1[1] 0.11 0.08 0.01 0.31 moedu_new1[2] 0.12 0.08 0.02 0.31 moedu_new1[3] 0.09 0.07 0.01 0.26 moedu_new1[4] 0.07 0.06 0.01 0.24 moedu_new1[5] 0.42 0.15 0.04 0.67 moedu_new1[6] 0.09 0.06 0.01 0.23 moedu_new1[7] 0.10 0.06 0.01 0.25 delta_labels &lt;- c(&quot;Elem&quot;, &quot;MidSch&quot;, &quot;SHS&quot;, &quot;HSG&quot;, &quot;SCol&quot;, &quot;Bach&quot;, &quot;Mast&quot;, &quot;Grad&quot;) posterior_samples(b12H7) %&gt;% dplyr::select(contains(&quot;moedu_new1&quot;)) %&gt;% set_names(str_c(delta_labels[2:8],&quot;~(delta[&quot;,1:7,&quot;])&quot;)) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller=label_parsed) + theme(strip.text = element_text(size = 8)) References "],["models-with-memory.html", "13 Models with Memory 13.1 Multilevel tadpoles 13.2 Varying effects and the underfitting/overfitting trade-off 13.3 More than one type of cluster 13.4 Divergent transitions and non-centered priors 13.5 Multilevel posterior predictions 13.6 Post stratification 13.7 Practice", " 13 Models with Memory 13.1 Multilevel tadpoles オタマジャクシの生存率に関するデータを用いる(Vonesh and Bolker 2005)。各行はそれぞれ異なる水槽を表しおり、水槽ごとに観測できない違いがあると考える。 data(reedfrogs) d &lt;- reedfrogs head(d) ## density pred size surv propsurv ## 1 10 no big 9 0.9 ## 2 10 no big 10 1.0 ## 3 10 no big 7 0.7 ## 4 10 no big 10 1.0 ## 5 10 no small 9 0.9 ## 6 10 no small 9 0.9 d %&gt;% mutate(tank = 1:nrow(d)) -&gt;d まずは、それぞれの水槽が違う切片を持つモデルを考える。 \\[ \\begin{aligned} S_{i} &amp;\\sim Binomial(N_{i}, p_{i})\\\\ logit(p_{i}) &amp;= \\alpha_{tank[i]}\\\\ \\alpha_{j} &amp;\\sim Normal(0,1.5) \\;\\; for \\; j = 1 .. 48 \\end{aligned} \\] b13.1 &lt;- brm(data = d, family = binomial, formula = surv | trials(density) ~ 0 + factor(tank), prior(normal(0,1.5), class = b), backend = &quot;cmdstanr&quot;, seed = 13, file = &quot;output/Chapter13/b13.1&quot;) 結果は以下の通り（表13.1）。48個の切片が推定されている。 表13.1: b13.1の結果。 Estimate Est.Error Q2.5 Q97.5 factortank1 1.71 0.76 0.32 3.30 factortank2 2.40 0.88 0.83 4.37 factortank3 0.76 0.63 -0.41 2.03 factortank4 2.41 0.91 0.77 4.29 factortank5 1.72 0.78 0.31 3.42 factortank6 1.71 0.79 0.30 3.42 factortank7 2.42 0.90 0.81 4.41 factortank8 1.72 0.79 0.27 3.35 factortank9 -0.37 0.61 -1.58 0.78 factortank10 1.70 0.75 0.33 3.30 続いて、階層モデルを考える。 \\[ \\begin{aligned} S_{i} &amp;\\sim Binomial(N_{i}, p_{i})\\\\ logit(p_{i}) &amp;= \\alpha_{tank[i]}\\\\ \\alpha_{j} &amp;\\sim Normal(\\bar{\\alpha},\\sigma) \\\\ \\bar{\\alpha} &amp;\\sim Normal(0,1.5)\\\\ \\sigma &amp;\\sim Exponential(1) \\end{aligned} \\] b13.2 &lt;- brm(data =d, family = binomial, formula = surv | trials(density) ~ 1 + (1|tank), prior = c(prior(normal(0,1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 5000, warmup =1000, sample_prior = &quot;yes&quot;, backend = &quot;cmdstanr&quot;, seed = 13, file = &quot;output/Chapter13/b13.2&quot;) 結果は以下の通り（表13.2）。 表13.2: b13.1の結果。 param Estimate Est.Error Q2.5 Q97.5 b_Intercept 1.35 0.26 0.85 1.86 sd_tank__Intercept 1.62 0.22 1.25 2.10 r_tank[1,Intercept] 0.80 0.90 -0.79 2.74 r_tank[2,Intercept] 1.74 1.12 -0.16 4.19 r_tank[3,Intercept] -0.35 0.71 -1.68 1.13 r_tank[4,Intercept] 1.73 1.12 -0.18 4.26 r_tank[5,Intercept] 0.80 0.89 -0.80 2.72 r_tank[6,Intercept] 0.80 0.90 -0.79 2.75 r_tank[7,Intercept] 1.73 1.10 -0.16 4.14 r_tank[8,Intercept] 0.79 0.89 -0.78 2.68 モデルを比較してみる（表13.3）。b13.2の方が圧倒的に良いモデル。 ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.2 0.0 0.0 -100.1 3.6 21.0 0.8 200.2 7.3 ## b13.1 -6.9 1.9 -107.0 2.3 25.3 1.2 214.0 4.5 表13.3: b13.1とb13.2のモデル比較。 elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic b13.2 0.00 0.00 -100.08 3.64 20.96 0.79 200.17 7.28 b13.1 -6.91 1.86 -107.00 2.26 25.25 1.24 214.00 4.52 モデルの推定結果は、実データよりも事後分布の中央値に近づいている（= 縮合）。また、小さい水槽（= サンプルサイズが小さい）ではより縮合が起きている。これは、サンプルサイズが小さいほど結果に対する影響が小さいため。さらに、事後分布中央値から離れているデータほど縮合が起きている。 13.2 Varying effects and the underfitting/overfitting trade-off 先ほどのオタマジャクシの例で、もしすべてのデータをプールしたとすると、全体の平均はより正確に推定できるが、それぞれの水槽の平均にはあまり当てはまらない（underfitting）。一方、もしすべての水槽に異なる切片を推定するとすれば、それぞれの水槽の平均値を推定できるがデータ数が少ないので不正確になってしまう（overfitting）。階層モデルを用いれば、overfittingとunderfittingの両方を防ぐことができる。それぞれのクラスター内のデータ数が十分に多ければすべての水槽に異なる切片を推定するモデルの結果に近くなるが、少なければ縮合が起きてoverfittingを防いでくれる。 13.2.1 The model b13.2のモデル式を用いてシミュレーションデータを作成する。 a_bar &lt;- 1.5 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(55) dsim &lt;- tibble(pond = 1:n_ponds, ni= rep(c(5,10,25,35),each=n_ponds/4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a_bar, sd = sigma)) head(dsim) ## # A tibble: 6 × 3 ## pond ni true_a ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 1.68 ## 2 2 5 -1.22 ## 3 3 5 1.73 ## 4 4 5 -0.179 ## 5 5 5 1.50 ## 6 6 5 3.28 13.2.2 Simulate survivors 生存数をシミュレートする。 set.seed(5005) dsim &lt;- dsim %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size=ni), p_nopool = si/ni) pond ni true_a si p_nopool 47 35 2.1398394 33 0.9428571 59 35 -0.9496280 9 0.2571429 60 35 0.7680259 24 0.6857143 17 10 5.0330458 10 1.0000000 5 5 1.5028623 5 1.0000000 39 25 2.2360616 23 0.9200000 33 25 0.0979725 18 0.7200000 12 5 0.2351493 2 0.4000000 31 25 3.8471316 25 1.0000000 42 25 1.0700611 19 0.7600000 13.2.2.1 Computing the non-pooling estimates それでは、実際にモデリングを行う。 b13.3 &lt;- brm(data = dsim, family = binomial, formula = si|trials(ni) ~ 1 + (1|pond), prior = c(prior(normal(0, 1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 5000, warmup =1000, backend = &quot;cmdstanr&quot;, seed = 13, file = &quot;output/Chapter13/b13.3&quot;) 結果は以下の通り（表13.4）。 表13.4: b13.3の結果。 Estimate Est.Error Q2.5 Q97.5 b_Intercept 1.31 0.20 0.93 1.71 sd_pond__Intercept 1.28 0.18 0.97 1.66 r_pond[1,Intercept] 0.99 1.00 -0.80 3.13 r_pond[2,Intercept] -1.13 0.79 -2.69 0.42 r_pond[3,Intercept] 0.14 0.86 -1.45 1.95 r_pond[4,Intercept] -1.70 0.81 -3.36 -0.19 r_pond[5,Intercept] 0.98 0.99 -0.79 3.07 r_pond[6,Intercept] 1.00 1.00 -0.78 3.14 r_pond[7,Intercept] -0.52 0.81 -2.05 1.11 r_pond[8,Intercept] -0.53 0.80 -2.08 1.10 p_partpool &lt;- coef(b13.3)$pond[, , ] %&gt;% data.frame() %&gt;% transmute(p_partpool = inv_logit_scaled(Estimate)) dsim2 &lt;- dsim %&gt;% bind_cols(p_partpool) %&gt;% mutate(p_true = inv_logit_scaled(true_a)) %&gt;% mutate(nopool_error = abs(p_nopool - p_true), partpool_error = abs(p_partpool - p_true)) slice_sample(dsim2 %&gt;% dplyr::select(pond, p_partpool, p_true, p_nopool, nopool_error, partpool_error), n=10) ## # A tibble: 10 × 6 ## pond p_partpool p_true p_nopool nopool_error partpool_error ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 48 0.265 0.385 0.229 0.156 0.120 ## 2 30 0.879 0.869 0.9 0.0309 0.0100 ## 3 38 0.771 0.805 0.76 0.0451 0.0338 ## 4 47 0.929 0.895 0.943 0.0481 0.0348 ## 5 52 0.832 0.880 0.829 0.0512 0.0479 ## 6 12 0.545 0.559 0.4 0.159 0.0139 ## 7 53 0.509 0.489 0.486 0.00344 0.0203 ## 8 42 0.772 0.745 0.76 0.0154 0.0270 ## 9 33 0.736 0.524 0.72 0.196 0.211 ## 10 18 0.880 0.959 0.9 0.0585 0.0789 実際の値（青）も推定値（黒）も水槽内の個体数が多いほど真の確率に近づいている。また、推定値の方がほとんどの場合真の値に近くなっている。これは、縮合によってoverfittingが抑えられていることを示している。加えて、実際の値と推定値の違いは水槽内の個体数が多いほど小さくなっている。これは、サンプルサイズが小さいほど縮合が生じていることを示している。 13.3 More than one type of cluster Chapter11で用いたチンパンジーの実験データ(Silk et al. 2005)を用いて、2つ以上のクラスターのあるデータを扱う。データ内には対象個体（actor）と実験日（block）の2つのクラスターが存在する。 \\[ \\begin{aligned} L_{i} &amp;\\sim Binomial(1,p_{i})\\\\ logit(p_{i}) &amp;= \\bar{\\alpha} + \\beta_{treatment[i]}+z_{actor[i]}\\sigma_{\\alpha} + x_{block[i]}\\sigma_{\\gamma}\\\\ \\beta_{j} &amp;\\sim Normal(0,0.5)\\\\ \\bar{\\alpha}_{} &amp;\\sim Normal(0,1.5)\\\\ z_{j} &amp;\\sim Normal(0,1)\\\\ x_{j} &amp;\\sim Normal(0,1)\\\\ \\sigma_{\\alpha} &amp;\\sim Exponential(1)\\\\ \\sigma_{\\gamma} &amp;\\sim Exponential(1)\\\\ \\end{aligned} \\] data(&quot;chimpanzees&quot;) d2 &lt;- chimpanzees d2 %&gt;% mutate(treatment = factor(1 + prosoc_left + 2*condition), block = factor(block), actor = factor(actor)) -&gt; d2 head(d2) ## actor recipient condition block trial prosoc_left chose_prosoc pulled_left ## 1 1 NA 0 1 2 0 1 0 ## 2 1 NA 0 1 4 0 0 1 ## 3 1 NA 0 1 6 1 0 0 ## 4 1 NA 0 1 8 0 1 0 ## 5 1 NA 0 1 10 1 1 1 ## 6 1 NA 0 1 12 1 1 1 ## treatment ## 1 1 ## 2 1 ## 3 2 ## 4 1 ## 5 2 ## 6 2 それでは、モデルを回す。 b13.4 &lt;- brm(data = d2, family = bernoulli, bf(pulled_left ~ a + b, a ~ 1 + (1|actor) + (1|block), b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = b), prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a), prior(exponential(1), class = sd, group = actor, nlpar = a), prior(exponential(1), class = sd, group = block, nlpar = a)), backend = &quot;cmdstanr&quot;, seed = 13, file = &quot;output/Chapter13/b13.4&quot; ) 収束のチェックをする。問題なさそう。 結果は以下の通り（表13.5）。 表13.5: b13.4の結果。 Estimate Est.Error Q2.5 Q97.5 b_a_Intercept 0.60 0.73 -0.85 2.01 b_b_treatment1 -0.13 0.30 -0.71 0.46 b_b_treatment2 0.40 0.30 -0.17 0.98 b_b_treatment3 -0.47 0.31 -1.07 0.13 b_b_treatment4 0.29 0.29 -0.30 0.86 sd_actor__a_Intercept 2.00 0.66 1.09 3.56 sd_block__a_Intercept 0.20 0.17 0.01 0.65 r_actor__a[1,Intercept] -0.96 0.72 -2.40 0.48 r_actor__a[2,Intercept] 4.05 1.32 1.97 7.15 r_actor__a[3,Intercept] -1.26 0.73 -2.73 0.22 blockの標準偏差は小さいため、これを除いても影響は小さいと考えられる。 b13.5 &lt;- brm(data = d2, family = bernoulli, bf(pulled_left ~ a + b, a ~ 1 + (1|actor), b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = b), prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a), prior(exponential(1), class = sd, group = actor, nlpar = a)), backend = &quot;cmdstanr&quot;, seed = 13, file = &quot;output/Chapter13/b13.5&quot; ) モデル比較をすると、b13.5の方宇賀PSISが小さい（表@ref{tab:comp-b13-4}）。このことも、block間でそこまで大きなばらつきがないことを示している。 b13.4 &lt;- add_criterion(b13.4, &quot;loo&quot;) b13.5 &lt;- add_criterion(b13.5, &quot;loo&quot;) r &lt;- loo_compare(b13.4, b13.5, criterion = &quot;loo&quot;) print(r, simplify = F) %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;b13.4とb13.5の比較。&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b13.5 0.0 0.0 -265.7 9.6 8.7 0.4 531.4 19.2 ## b13.4 -0.7 0.8 -266.4 9.7 10.8 0.6 532.8 19.4 表13.6: b13.4とb13.5の比較。 elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic b13.5 0.0 0.00 -265.71 9.61 8.66 0.44 531.41 19.21 b13.4 -0.7 0.79 -266.41 9.68 10.83 0.55 532.82 19.37 13.3.1 Even more clusters treatmentもランダム効果に入れることができる。 b13.6 &lt;- brm(data = d2, family = bernoulli, pulled_left ~ 1 + (1 | actor) + (1 | block) + (1 | treatment), prior = c(prior(normal(0, 1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13.6&quot;) 固定効果と変量効果に入れた場合で、treatmentの推定値は大きく変わらない（表13.7）。これは、各条件で十分にデータ数があるためである。b13.6で少しだけ縮合が起きている。 表13.7: b13.4とb14.6のtreatmentの値 b13.4 b13.6 par Est_fixed se_fixed Q2.5_fixed Q97.5_fixed Est_random se_random Q2.5_random Q97.5_random b_treatment1 -0.1252342 0.2985188 -0.7130660 0.4626933 -0.1464338 0.4206498 -0.9350493 0.5975902 b_treatment2 0.4009403 0.2960419 -0.1682906 0.9831511 0.3490351 0.4238662 -0.3693319 1.1606370 b_treatment3 -0.4709128 0.3066623 -1.0746720 0.1270108 -0.4695689 0.4373799 -1.3439747 0.2151697 b_treatment4 0.2863133 0.2936503 -0.3017152 0.8585155 0.2414440 0.4149398 -0.4727484 1.0509438 13.4 Divergent transitions and non-centered priors 事後分布の傾きに急な部分があるとき、divergent transitionが生じることがある。これは、階層モデルを扱うときによく生じる。これに対処する方法は大きく分けて2つある。 warmup期間でうまく調節が行われるようにする（i.e., adapt_deltaの値を上げる）。 最パラメータ化を行う。 13.4.1 The Devil’s Funnel 以下のモデルを考える。 \\[ \\begin{aligned} v &amp;\\sim Normal(0,3)\\\\ x &amp;\\sim Normal(0, exp(v))\\\\ \\end{aligned} \\] モデルを回してみると、非常に多くのdivergent transitionが生じる。 #m13.7 &lt;- # ulam( # data = list(N = 1), # alist( # v ~ normal(0, 3), # x ~ normal(0, exp(v)) #), #chains = 4 #) m13.7 &lt;- readRDS(&quot;output/Chapter13/m13.7.rds&quot;) precis(m13.7) ## mean sd 5.5% 94.5% n_eff Rhat4 ## v 2.318110 2.189705 -1.950641 5.800031 6.859522 1.429809 ## x 8.048351 122.682505 -77.021070 158.026145 71.127011 1.032196 xの分布は以下のようになる。かなり急であることが分かる。 traceplotもうまく収束していないことを示している。 この例はNealの漏斗と呼ばれる。対数事後確率\\(p(v,x) = p(x|v)*p(v)\\)を計算し、その上にMCMCサンプルも図示する。対数事後確率の高いところからうまくサンプリングできていないことが分かる。これは、zが小さいところではxの事後分布が0付近でかなり急になっているからである。 これを解決するために、non-centered parametarazationという最パラメータ化を行う。この方法は、あるパラメータの分布が別のパラメータに強く依存しないようにすることである。non-centered parametarazationでは、標準化と逆のことを行っている。すなわち、下の式では\\(z\\)が\\(x\\)を標準化したものになるようにしている。 \\[ \\begin{aligned} v &amp;\\sim Normal(0,3)\\\\ z &amp;\\sim Normal(0,1)\\\\ x &amp;= z\\:exp(v) \\end{aligned} \\] 最パラメータ後はvとzについて事後分布を得ることになるが、このとき対数事後分布に傾きが急な場所ができないため、うまくサンプリングができる。 #m13.7nc &lt;- ulam( #alist( #v ~ normal(0,3), #z ~ normal(0,1), #gq&gt; real[1]:x&lt;&lt;- z*exp(v) #), #data = list(N=1), chains=4 #) m13.7nc &lt;- readRDS(file=&quot;output/Chapter13/m13.7nc&quot;) precis(m13.7nc) ## mean sd 5.5% 94.5% n_eff Rhat4 ## v 0.04191068 3.1933725 -5.076669 5.278982 1329.498 1.0002242 ## z 0.02140103 0.9948099 -1.582802 1.623487 1269.240 0.9991946 ## x -27.78654239 2164.0742844 -28.304278 42.382686 1479.803 1.0000306 13.4.2 Non-centered chimpanzees もともとnon-centered parametarazationを行っていたモデルの有効サンプルサイズに問題はなさそう。 ところで、常に再パラメータ化を行った方がいいということではない。クラスター内でのばらつきが小さい場合や、クラスター内で各ユニットのデータ数が少ない場合には再パラメータ化が有効である。 13.5 Multilevel posterior predictions 13.5.1 Posterior prediction for same clusters chimpの個体2の事後予測を描写する。 nd &lt;- d2 %&gt;% distinct(treatment) %&gt;% mutate(actor=2, block=1) labels &lt;- c(&quot;R/N&quot;, &quot;L/N&quot;, &quot;R/P&quot;, &quot;L,P&quot;) fit13.4 &lt;- fitted(b13.4, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(treatment = factor(treatment, labels = labels)) 個体2の推定値は以下の通り（表13.8）。 fit13.4 %&gt;% kable(booktabs = TRUE, caption = &quot;個体2の推定値&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;,&quot;hold_position&quot;)) 表13.8: 個体2の推定値 Estimate Est.Error Q2.5 Q97.5 treatment actor block 0.9786448 0.0203763 0.9243916 0.9993467 R/N 2 1 0.9871279 0.0128489 0.9521603 0.9995752 L/N 2 1 0.9705514 0.0274387 0.8965248 0.9990162 R/P 2 1 0.9856228 0.0140419 0.9469928 0.9995442 L,P 2 1 描写すると以下のようになる。 続いて個体5について、fitted()を用いずに事後分布を描く。 post &lt;- posterior_samples(b13.4) post %&gt;% transmute(actor_5 = `r_actor__a[5,Intercept]`) %&gt;% ggplot(aes(x=actor_5))+ geom_density(size=0, fill=&quot;orange1&quot;, color = &quot;transparent&quot;)+ theme(aspect.ratio=0.8)+ ggtitle(&quot;Chimp #5&#39;s density&quot;) 個体5の推定値は以下の通り（表13.9）。 post %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(b_a_Intercept + value + `r_actor__a[1,Intercept]` + `r_block__a[1,Intercept]`)) %&gt;% mutate(treatment = factor(str_remove(name, &quot;b_b_treatment&quot;), labels = labels)) %&gt;% dplyr::select(name:treatment) %&gt;% group_by(treatment) %&gt;% mean_qi(fitted) -&gt; fit13.4_act5 表13.9: 個体5の推定値 treatment fitted .lower .upper .width .point .interval R/N 0.35 0.20 0.51 0.95 mean qi L/N 0.47 0.29 0.63 0.95 mean qi R/P 0.28 0.15 0.42 0.95 mean qi L,P 0.44 0.27 0.61 0.95 mean qi 図示すると以下の通り。 13.5.2 Posterior prediction for new clusters 現在の推定結果から新たなclusterについて予測をしたい。まず、平均的な個体（つまり、ランダム効果を0にしたとき）の推定値と80%信用区間は以下の通り（表13.10）。 fit13.4_new_ave &lt;- post %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(b_a_Intercept + value)) %&gt;% mutate(treatment = factor(str_remove(name, &quot;b_b_treatment&quot;), labels = labels)) %&gt;% dplyr::select(name:treatment) %&gt;% group_by(treatment) %&gt;% mean_qi(fitted, .width = .8) fit13.4_new_ave %&gt;% kable(booktabs = TRUE, digits=2, caption = &quot;平均的な個体&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;,&quot;hold_position&quot;)) 表13.10: 平均的な個体 treatment fitted .lower .upper .width .point .interval R/N 0.60 0.40 0.80 0.8 mean qi L/N 0.71 0.52 0.87 0.8 mean qi R/P 0.53 0.32 0.73 0.8 mean qi L,P 0.69 0.49 0.86 0.8 mean qi 続いて、シミュレーションを行ってランダム効果を考慮に入れた（個体間のばらつきを考慮に入れた）推定値と80%信用区間を図示すると、以下のようになる（表@ref(tab:res-b13.4-qi-sim)）。 fit13.4_new_sim &lt;- post %&gt;% mutate(a_sim = rnorm(n(),mean=b_a_Intercept, sd = sd_actor__a_Intercept)) %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(a_sim + value)) %&gt;% mutate(treatment = factor(str_remove(name, &quot;b_b_treatment&quot;), labels = labels)) %&gt;% dplyr::select(name:treatment) %&gt;% group_by(treatment) %&gt;% mean_qi(fitted, .width = .8) (#tab:res-b13.4-qi-sim)ランダム効果を考慮した個体 treatment fitted .lower .upper .width .point .interval R/N 0.57 0.10 0.96 0.8 mean qi L/N 0.64 0.15 0.98 0.8 mean qi R/P 0.52 0.07 0.94 0.8 mean qi L,P 0.63 0.14 0.97 0.8 mean qi 最後に、新たに100個体についてデータを取得した際の推定値をシミュレートする。 nd &lt;- distinct(d2, treatment) %&gt;% tidyr::expand(actor = str_c(&quot;new&quot;,1:100), treatment) %&gt;% mutate(row = 1:n()) set.seed(13) fit13.4_new_100 &lt;- fitted(b13.4, newdata=nd, allow_new_levels =TRUE, sample_new_levels = &quot;gaussian&quot;, summary =F, nsamples=100) %&gt;% data.frame() p7 &lt;- fit13.4_new_100 %&gt;% set_names(pull(nd,row)) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter,names_to=&quot;row&quot;) %&gt;% mutate(row=as.double(row)) %&gt;% left_join(nd, by=&quot;row&quot;) %&gt;% mutate(actor_n = str_remove(actor, &quot;new&quot;) %&gt;% as.double()) %&gt;% filter(actor_n == iter) %&gt;% mutate(treatment = factor(treatment, labels = labels)) %&gt;% ggplot(aes(x = treatment, y = value, group = actor)) + geom_line(alpha = 1/2, color = &quot;navy&quot;) + ggtitle(&quot;100 simulated actors&quot;) + theme(plot.title = element_text(size = 14, hjust = .5), aspect.ratio = 0.8) p5|p6|p7 13.6 Post stratification 新たなクラスターに関する予測をする他の方法としては、post stratificationがある。例えば、それぞれのカテゴリー\\(i\\)について推定値\\(p_{i}\\)が推定されたとき、それぞれのカテゴリーのデータ数に基づいて算出した以下の式を用いる。 \\[ \\frac{\\sum_{i} N_{i}p_{i}}{\\sum_{i} N_{i}} \\] 13.6.1 Meet the data 以下では、Monica Alexanderのブログの例を用いる。 load(&quot;data/mrp_data_ch13.rds&quot;) d %&gt;% head(10) %&gt;% kable(booktabs = TRUE) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;,&quot;hold_position&quot;)) kept_name state_name age_group decade_married educ_group 0 ohio 50 1979 &gt;BA 0 virginia 35 1999 &gt;BA 1 new york 35 2009 &gt;BA 0 rhode island 55 1999 &gt;BA 0 illinois 35 2009 &gt;BA 0 north carolina 25 2009 &gt;BA 1 iowa 35 1999 &gt;BA 1 texas 35 2009 &gt;BA 0 south dakota 35 1999 &gt;BA 0 texas 35 2009 &gt;BA データdは、女性が結婚後に姓を変更したかについてのデータ。変数の説明は以下の通り。 - kept_name 回答、1 = “yes”, 0 = “no”。 - age_group 25歳から75歳まで、25 = [25,30), 30 = [30,40)…。 - decade_married 1979年から2019年まで、1979 = [1979,1989), 1989 =[1989,1999), …。 - educ_group &lt;BA = no bachelor’s degree, BA = bachelor’s degree, &gt;BA = above bachelor’s degree。 - state_name 州の名前。 データcell_countsはUS censusによるデータで、nは各カテゴリーに該当する人数を表している。一部のカテゴリーについては十分なサンプル数があるが、それ以外についてはサンプル数が小さいものもあり、ばらつきが大きい。よって、新たなかてごりーについて予測を行う場合には、 個のばらつきを考慮する必要がある。 cell_counts %&gt;% ggplot(aes(x = n)) + geom_histogram(binwidth = 2000, fill = &quot;blue&quot;) + scale_x_continuous(breaks = 0:3 * 100000, labels = c(0, &quot;100K&quot;, &quot;200K&quot;, &quot;300K&quot;))+ theme(aspect.ratio=1) 13.6.2 Settle the MR part of MRP. まずは、データを用いてモデリングを行う。全体の平均の事前分布としては、\\(Normal(-1,1)\\)を用いる。これは、結婚後には姓を変えると答える人の方が多いと考えられるため。 tibble(n = rnorm(1e6, -1, 1)) %&gt;% mutate(p = inv_logit_scaled(n)) %&gt;% ggplot(aes(x = p)) + geom_histogram(fill = &quot;blue&quot;, binwidth = .02) + scale_y_continuous(breaks = NULL)+ theme(aspect.ratio=0.7) それでは、モデリングを行う。 b13.7 &lt;- brm(data =d, family = bernoulli, formula = kept_name ~ 1 + (1|age_group) + (1|decade_married) + (1|educ_group) + (1|state_name), prior = c(prior(normal(-1, 1), class = Intercept), prior(exponential(1), class = sd)), seed = 13, control = list(adapt_delta =.98), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13.7&quot;) 結果は以下の通り（表@ref{tab:res-b13-7}） 表13.11: b13.7の結果 par Estimate Est.Error Q2.5 Q97.5 b_Intercept -0.72 0.62 -1.97 0.50 sd_age_group__Intercept 1.16 0.29 0.73 1.86 sd_decade_married__Intercept 1.01 0.40 0.50 2.05 sd_educ_group__Intercept 0.92 0.48 0.35 2.22 sd_state_name__Intercept 0.25 0.06 0.14 0.38 推定値の90%信用区間は以下の通り。 13.6.3 Post-stratify to put the P in MRP. ここでは、age_groupとstateのみに焦点を当てる。以下では、3つの推定方法による結果を比較する。 13.6.3.1 Estimate by age group まずは、生データの姓を変更した女性の割合を年齢層ごとに図示する。 続いて、階層モデルの推定結果を図示する。 最後に、post-stratificationによる推定結果を図示する。 まずは、cell_countsデータの各年齢カテゴリーの全体に占める割合を算出し、モデルの結果をもとにそれぞれのカテゴリーにおける事後予測分布を算出する。 age_prop &lt;- cell_counts %&gt;% group_by(age_group) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() p &lt;- add_predicted_draws(b13.7, newdata = age_prop %&gt;% filter(between(age_group,20,80), decade_married &gt;1969), allow_new_levels = TRUE) この結果を用い、以下の数式に基づき各カテゴリのデータ数に応じて重みづけを行う。 \\[ \\frac{\\sum_{i} N_{i}p_{i}}{\\sum_{i} N_{i}} \\] p &lt;- p %&gt;% group_by(age_group, .draw) %&gt;% summarise(kept_name_predict = sum(.prediction*prop)) %&gt;% group_by(age_group) %&gt;% mean_qi(kept_name_predict) 全ての結果を図示すると以下の通り。 13.6.3.2 Estimate by US state. 続いて、州ごとの予測結果を図示する。 続いて、階層レベルモデルの結果を図示する。 最後に、post-stratificationによる推定結果を図示する。 state_prop &lt;- cell_counts %&gt;% group_by(state_name) %&gt;% mutate(prop=n/sum(n)) add_predicted_draws(b13.7, newdata = state_prop %&gt;% filter(between(age_group,20,80), decade_married &gt; 1969), allow_new_levels = TRUE) %&gt;% group_by(state_name, .draw) %&gt;% summarise(kept_name_predict = sum(.prediction*prop)) %&gt;% group_by(state_name) %&gt;% mean_qi(kept_name_predict) -&gt; pp 全部を図示すると以下の通り。 \\(\\sigma_{state}\\)の推定値は小さかったので、縮合によってモデルによる推定値のばらつきが小さくなっていることが分かる。 13.7 Practice 13.7.1 13M1 Revisit the Reed frog survival data, data(reedfrogs), and add the predation and size treatment variables to the varying intercepts model. Consider models with either main effect alone, both main effects, as well as a model including both and their interaction. Instead of focusing on inferences about these two predictor variables, focus on the inferred variation across tanks. Explain why it changes as it does across models. reedfrogsデータで、pred（捕食者がいたか）とsize（オタマジャクシの大きさ(big or small)）も説明変数に加える。どちらか一方を加えたモデル、両方を加えたモデル、交互作用も含めたモデルの計4つのモデルを比較する。 data(reedfrogs) d &lt;- reedfrogs head(d) ## density pred size surv propsurv ## 1 10 no big 9 0.9 ## 2 10 no big 10 1.0 ## 3 10 no big 7 0.7 ## 4 10 no big 10 1.0 ## 5 10 no small 9 0.9 ## 6 10 no small 9 0.9 d %&gt;% mutate(tank = 1:nrow(d)) -&gt;d head(d) ## density pred size surv propsurv tank ## 1 10 no big 9 0.9 1 ## 2 10 no big 10 1.0 2 ## 3 10 no big 7 0.7 3 ## 4 10 no big 10 1.0 4 ## 5 10 no small 9 0.9 5 ## 6 10 no small 9 0.9 6 d %&gt;% mutate(pred = ifelse(pred == &quot;pred&quot;,1,0), size = ifelse(size == &quot;big&quot;, 1,0)) -&gt;dd b13M1a &lt;- brm(data =dd, family = binomial, formula = bf(surv|trials(density) ~ a + b, a ~ 1 + (1|tank), b ~ 0 + pred, nl = TRUE), prior = c(prior(normal(0,1.5), class = b, coef = Intercept, nlpar =a), prior(normal(0,1.5), class = b, nlpar =b), prior(exponential(1), class = sd, nlpar =a)), backend = &quot;cmdstanr&quot;, seed = 15, file = &quot;output/Chapter13/b13M1a&quot; ) b13M1b &lt;- brm(data =dd, family = binomial, formula = bf(surv|trials(density) ~ a + b, a ~ 1 + (1|tank), b ~ 0 + size, nl = TRUE), prior = c(prior(normal(0,1.5), class = b, coef = Intercept, nlpar =a), prior(normal(0,1.5), class = b, nlpar =b), prior(exponential(1), class = sd, nlpar =a)), backend = &quot;cmdstanr&quot;, seed = 15, file = &quot;output/Chapter13/b13M1b&quot; ) b13M1c &lt;- brm(data =dd, family = binomial, formula = bf(surv|trials(density) ~ a + b, a ~ 1 + (1|tank), b ~ 0 + pred + size, nl = TRUE), prior = c(prior(normal(0,1.5), class = b, coef = Intercept, nlpar =a), prior(normal(0,1.5), class = b, nlpar =b), prior(exponential(1), class = sd, nlpar =a)), backend = &quot;cmdstanr&quot;, seed = 15, file = &quot;output/Chapter13/b13M1c&quot; ) b13M1d &lt;- brm(data =dd, family = binomial, formula = bf(surv|trials(density) ~ a + b, a ~ 1 + (1|tank), b ~ 0 + pred*size, nl = TRUE), prior = c(prior(normal(0,1.5), class = b, coef = Intercept, nlpar =a), prior(normal(0,1.5), class = b, nlpar =b), prior(exponential(1), class = sd, nlpar =a)), backend = &quot;cmdstanr&quot;, seed = 15, file = &quot;output/Chapter13/b13M1d&quot; ) posterior_summary(b13M1a) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par, &quot;b_|sd_&quot;)) %&gt;% set_names(c(&quot;par&quot;,&quot;Est_a&quot;,&quot;SE_a&quot;,&quot;Q2.5_a&quot;,&quot;Q97.5_a&quot;))-&gt; res_13M1a posterior_summary(b13M1b) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par, &quot;b_|sd_&quot;)) %&gt;% set_names(c(&quot;par&quot;,&quot;Est_b&quot;,&quot;SE_b&quot;,&quot;Q2.5_b&quot;,&quot;Q97.5_b&quot;))-&gt; res_13M1b posterior_summary(b13M1c) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par, &quot;b_|sd_&quot;))%&gt;% set_names(c(&quot;par&quot;,&quot;Est_c&quot;,&quot;SE_c&quot;,&quot;Q2.5_c&quot;,&quot;Q97.5_c&quot;)) -&gt; res_13M1c posterior_summary(b13M1d) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par, &quot;b_|sd_&quot;))%&gt;% set_names(c(&quot;par&quot;,&quot;Est_d&quot;,&quot;SE_d&quot;,&quot;Q2.5_d&quot;,&quot;Q97.5_d&quot;)) -&gt; res_13M1d res_13M1a %&gt;% full_join(res_13M1b,by=&quot;par&quot;) %&gt;% full_join(res_13M1c) %&gt;% full_join(res_13M1d) %&gt;% mutate(across(where(is.numeric),~round(.x,2)))-&gt; res_13M1 res_13M1[is.na(res_13M1)] &lt;- &quot;-&quot; 結果は以下の通り（表13.12）。ランダム効果の標準偏差の推定値は、predが入っているモデルではほとんど変わらないが、sizeだけのモデルでは大きくなっている。これは、predがタンクごとの違いの大部分を説明しているからだと考えられる。 表13.12: モデルの比較 pred only size only pred and size including interaction par Estimate SE Q2.5 Q97.5 Estimate SE Q2.5 Q97.5 Estimate SE Q2.5 Q97.5 Estimate SE Q2.5 Q97.5 b_a_Intercept 2.57 0.25 2.09 3.08 1.52 0.34 0.84 2.2 2.8 0.27 2.26 3.34 2.48 0.29 1.92 3.08 b_b_pred -2.5 0.31 -3.12 -1.88 -2.51 0.3 -3.13 -1.92 -1.99 0.37 -2.70 -1.28 sd_tank__a_Intercept 0.82 0.15 0.57 1.14 1.62 0.22 1.24 2.1 0.78 0.15 0.53 1.1 0.74 0.14 0.49 1.05 b_b_size -0.35 0.49 -1.31 0.63 -0.46 0.29 -1.01 0.15 0.22 0.42 -0.58 1.07 b_b_pred:size -1.13 0.52 -2.16 -0.13 13.7.2 13M2 Compare the models you fit just above, using WAIC. Can you reconcile the differences in WAIC with the posterior distributions of the models? WAICを比較する（表@ref{tab:comp-waic}）。ほとんど変わらないが、やはりpredのみのモデルが最も予測がよい。 b13M1a &lt;- add_criterion(b13M1a, &quot;waic&quot;) b13M1b &lt;- add_criterion(b13M1b, &quot;waic&quot;) b13M1c &lt;- add_criterion(b13M1c, &quot;waic&quot;) b13M1d &lt;- add_criterion(b13M1d, &quot;waic&quot;) loo_compare(b13M1a,b13M1b,b13M1c,b13M1d, criterion=&quot;waic&quot;) %&gt;% print(simplify = F) %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;WAICの比較。&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13M1a 0.0 0.0 -99.3 4.6 19.0 2.0 198.6 9.1 ## b13M1c -0.7 1.0 -100.0 4.4 19.1 1.9 199.9 8.8 ## b13M1b -0.9 2.9 -100.2 3.6 21.0 0.9 200.4 7.3 ## b13M1d -1.0 1.7 -100.2 4.8 19.3 2.2 200.5 9.6 表13.13: WAICの比較。 elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic b13M1a 0.00 0.00 -99.28 4.55 19.03 1.96 198.55 9.11 b13M1c -0.69 0.96 -99.96 4.41 19.12 1.89 199.92 8.83 b13M1b -0.94 2.89 -100.22 3.63 21.02 0.86 200.43 7.27 b13M1d -0.97 1.68 -100.24 4.82 19.34 2.15 200.49 9.65 13.7.3 13M3 Re-estimate the basic Reed frog varying intercept model, but now using a Cauchy distribution in place of the Gaussian distribution for the varying intercepts. That is, fit this model: $$ \\[\\begin{aligned} s_i &amp;\\sim Binomial(n_i,p_i)\\\\ logit(p_i) &amp;= \\alpha_{tank}\\\\ \\alpha_{tank} &amp;\\sim Cauchy(\\alpha,\\sigma)\\\\ \\alpha &amp;\\sim Normal(0,1)\\\\ \\sigma &amp;\\sim Exponential(1) \\end{aligned}\\] $$ Compare the posterior means of the intercepts, , to the posterior means produced in the chapter, using the customary Gaussian prior. Can you explain the pattern of differences? ランダム効果の標準偏差にコーシー分布を用いてモデリングを行う。 brmsだと実装できないので、ulam関数でモデリングする。 #b13M3 &lt;- # m_TankCauchy &lt;- ulam( #alist( # surv ~ dbinom(density, p), #logit(p) &lt;- a[tank], #a[tank] ~ dcauchy(a_bar, sigma), #a_bar ~ dnorm(0, 1.5), #sigma ~ dexp(1) #), #data = d, chains = 4, cores = 4, log_lik = TRUE, #iter = 2e3, control = list(adapt_delta = 0.99) #) b13M3 &lt;- readRDS(&quot;output/Chapter13/b13M3.rds&quot;) 各tankのランダム切片の推定値を示すと以下のようになる(表13.1)。白抜きの点は実測値、オレンジの点はコーシー分布を事前分布としたときの推定値、青い点は正規分布を事前分布としたときの推定値を示す。また、点線は実測値の平均値である。 正規分布のほうがコーシー分布よりも点線に近い推定値をとっており、縮合がより強く生じていることがわかる。これは、コーシー分布のほうが正規分布よりも裾の広い分布だからである。 posterior_summary(b13.2) %&gt;% data.frame() %&gt;% mutate(across(everything(), logistic)) %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par,&quot;r_t&quot;)) %&gt;% mutate(tank = 1:n(), type = &quot;normal&quot;) %&gt;% dplyr::select(-Est.Error, -par) -&gt; post_b13.2 extract.samples(b13M3) %&gt;% data.frame() %&gt;% mutate(across(everything(), logistic)) %&gt;% dplyr::select(-a_bar, -sigma) -&gt; a data.frame(tank = 1:48) %&gt;% mutate(Estimate = map_dbl(a,mean)) %&gt;% mutate(Q2.5 = map_dbl(a, ~quantile(., probs = 0.025))) %&gt;% mutate(Q97.5 = map_dbl(a, ~quantile(., probs = 0.975))) %&gt;% mutate(type = &quot;cauchy&quot;)-&gt; post_b13M3 bind_rows(post_b13.2, post_b13M3) -&gt; post post %&gt;% ggplot(aes(x=tank, y=Estimate))+ geom_point(aes(color = type), position = &quot;dodge&quot;)+ geom_point(data = d, aes(y = propsurv), shape = 1)+ geom_hline(data = d, yintercept = mean(d$propsurv), linetype = &quot;dashed&quot;)+ theme(aspect.ratio = 1)+ scale_color_manual(values = c(&quot;orange&quot;,&quot;blue3&quot;)) 図13.1: 練習問題13M3の図。ランダム効果の事前分布に正規分布とコーシー分布を使用したときの違い 13.7.4 13M4 今度はstudentのt分布を事前分布として用いて先ほどの結果と比較せよ。 #b13M4 &lt;- # m_TankCauchy &lt;- ulam( #alist( # surv ~ dbinom(density, p), # logit(p) &lt;- a[tank], # a[tank] ~ dstudent(2, a_bar, sigma), # a_bar ~ dnorm(0, 1.5), # sigma ~ dexp(1) #), #data = d, chains = 4, cores = 4, log_lik = TRUE, #iter = 2e3, control = list(adapt_delta = 0.99) #) b13M4 &lt;- readRDS(&quot;output/Chapter13/b13M4.rds&quot;) 各tankのランダム切片の推定値を示すと以下のようになる(表13.1)。白抜きの点は実測値、オレンジの点はコーシー分布を事前分布としたときの推定値、青い点はt分布を事前分布としたときの推定値を示す。また、点線は実測値の平均値である。 結果、ほとんど同じ推定値をとることがわかる。 extract.samples(b13M4) %&gt;% data.frame() %&gt;% mutate(across(everything(), logistic)) %&gt;% dplyr::select(-a_bar, -sigma) -&gt; b data.frame(tank = 1:48) %&gt;% mutate(Estimate = map_dbl(a,mean)) %&gt;% mutate(Q2.5 = map_dbl(b, ~quantile(., probs = 0.025))) %&gt;% mutate(Q97.5 = map_dbl(b, ~quantile(., probs = 0.975))) %&gt;% mutate(type = &quot;student&quot;)-&gt; post_b13M4 bind_rows(post_b13M4, post_b13M3) -&gt; post post %&gt;% ggplot(aes(x=tank, y=Estimate))+ geom_point(aes(color = type), alpha = 0.5,size = 2, position = position_dodge(width = 1))+ geom_point(data = d, aes(y = propsurv), shape = 1)+ geom_hline(data = d, yintercept = mean(d$propsurv), linetype = &quot;dashed&quot;)+ theme(aspect.ratio = 1)+ scale_color_manual(values = c(&quot;orange&quot;,&quot;blue3&quot;)) 13.7.5 13M5 Modify the cross-classified chimpanzees model m13.4 so that the adaptive prior for blocks contains a parameter for its mean: \\[ \\gamma_i \\sim Normal(\\bar{\\gamma}, \\sigma_{\\gamma})\\\\ \\bar{\\gamma} \\sim Normal(0,1.5) \\] Compare this model to m13.4. What has including done? これもbrmsパッケージでは推定ができなそうなので、ulam関数を用いる。 data(chimpanzees) d2 &lt;- chimpanzees d2$treatment &lt;- 1 + d2$prosoc_left + 2 * d2$condition dat_list &lt;- list( pulled_left = d2$pulled_left, actor = d2$actor, block_id = d2$block, treatment = as.integer(d2$treatment) ) #b13M5 &lt;- ulam( # alist( # pulled_left ~ dbinom(1, p), # logit(p) &lt;- a[actor] + g[block_id] + b[treatment], # b[treatment] ~ dnorm(0, 0.5), # ## adaptive priors # a[actor] ~ dnorm(a_bar, sigma_a), # g[block_id] ~ dnorm(g_bar, sigma_g), # ## hyper-priors # a_bar ~ dnorm(0, 1.5), # g_bar ~ dnorm(0, 1.5), # sigma_a ~ dexp(1), # sigma_g ~ dexp(1) #), #data = dat_list, chains = 4, cores = 4, log_lik = TRUE # ) b13M5 &lt;- readRDS(&quot;output/Chapter13/b13M5.rds&quot;) それでは、b13.4と結果を比較する。 新しいモデル(b13M5)の有効サンプル数(n_eff)とGelman-Rubin統計量(Rhat)を見ると、そのサンプリングは非常に非効率的であることがわかる。元のモデル(b13.4)と比較すると、すべてのパラメータで有効サンプル数が大幅に悪化している。 これは、b13M5はオーバー・パラメータ化されているからである。actorとblockのランダム切片の平均は(ここでは、a_bar、g_bar)現在のモデルではどちらもモデルの切片の中に含まれている。そのため、これらは識別不能であり、別々に推定することはできない。 library(easystats) model_parameters(b13.4) ## Parameter | Median | 95% CI | pd | Rhat | ESS ## ---------------------------------------------------------------- ## a_Intercept | 0.59 | [-0.85, 2.01] | 79.95% | 1.006 | 945.00 ## b_treatment1 | -0.12 | [-0.71, 0.46] | 66.25% | 1.002 | 2476.00 ## b_treatment2 | 0.40 | [-0.17, 0.98] | 91.20% | 1.001 | 2494.00 ## b_treatment3 | -0.47 | [-1.07, 0.13] | 93.70% | 1.002 | 2300.00 ## b_treatment4 | 0.29 | [-0.30, 0.86] | 83.83% | 1.000 | 2596.00 precis(b13M5, 2, pars = c(&quot;a_bar&quot;, &quot;b&quot;, &quot;g_bar&quot;)) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a_bar 0.4049342 1.1218173 -1.33440410 2.30157449 257.0455 1.010909 ## b[1] -0.1430008 0.3095102 -0.61967802 0.36858667 573.5765 1.004046 ## b[2] 0.3889532 0.2985868 -0.08154383 0.88374908 528.3317 1.003222 ## b[3] -0.4791117 0.3108014 -0.98463977 0.01555968 492.5316 1.001266 ## b[4] 0.2649693 0.3059801 -0.20340321 0.77382354 665.3800 1.002914 ## g_bar 0.2384293 1.1329296 -1.56088869 2.03449592 118.2272 1.028422 13.7.6 13M6 Sometimes the prior and the data are in conflict, because they concentrate around different regions of parameter space. What happens in these cases depends a lot upon the shape of the tails of distributions. Likewise, the tails of distributions strongly influence can outliers are shrunk or not towards mean. I want you to consider four different models to fit to one observation at \\(y=0\\). The models differ only in the distributions assigned to the likelihood and prior. Here are the four models: $$ \\[\\begin{aligned} Model \\;NN: y &amp;\\sim Normal(\\mu,1)\\\\ \\mu &amp;\\sim Normal(10,1)\\\\ \\\\ Model \\;NT: y &amp;\\sim Normal(\\mu,1)\\\\ \\mu &amp;\\sim Student(2,10,1)\\\\ \\\\ Model \\;TN: y &amp;\\sim Student(2,\\mu,1)\\\\ \\mu &amp;\\sim Normal(10,1)\\\\ \\\\ Model \\;TT: y &amp;\\sim Student(2,\\mu,1)\\\\ \\mu &amp;\\sim Student(2,10,1)\\\\ \\\\ \\end{aligned}\\] $$ \\(y=0\\)の1つのデータに対する4つのモデルを比較する。 d3 &lt;- data.frame(y=0) b13M6_a &lt;- brm(data = d3, family = &quot;gaussian&quot;, formula = y ~ 1, prior = c(prior(normal(10,1), class = Intercept), prior(constant(1), class = sigma)), backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13M6_a&quot;) b13M6_b &lt;- brm(data = d3, family = student, formula = y ~ 1, prior = c(prior(&quot;constant(2)&quot;, class = &quot;nu&quot;), prior(&quot;constant(1)&quot;, class = &quot;sigma&quot;), prior(&quot;normal(10,1)&quot;,class= &quot;Intercept&quot;)), seed =13, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13M6_b&quot;) b13M6_c &lt;- brm(data = d3, family = gaussian, formula = y ~ 1, prior = c(prior(constant(1), class = sigma), prior(student_t(2,10,1),class=Intercept)), seed =13, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13M6_c&quot;) b13M6_d &lt;- brm(data = d3, family = student, formula = y ~ 1, prior = c(prior(constant(2), class = nu), prior(constant(1), class = sigma), prior(student_t(2,10,1),class=Intercept)), seed =13, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13M6_d&quot;) 3つ目のモデルのみ正しく推定できた（応答変数に正規分布、事前分布にt分布）。事前分布が真の値から離れているとうまく推定できない。一方で、応答変数の分布が広すぎてもうまく推定できない？ Estimate Est.Error Q2.5 Q97.5 model NN 5.01 0.70 3.63 6.41 model TN 9.68 0.99 7.81 11.63 model NT 0.31 1.00 -1.61 2.27 model TT 4.44 4.51 -1.66 11.40 13.7.7 13H1 In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on three of them for this practice problem: district: ID number of administrative district each woman resided in use.contraception: An indicator (0/1) of whether the woman was using contraception urban: An indicator (0/1) of whether the woman lived in a city, as opposed to living in a rural area The first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you’ll have parameters for which there is no data to inform them. Worse, the model probably won’t run. Look at the unique values of the district variable: data(bangladesh) d4 &lt;- bangladesh sort(unique(d4$district)) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## [51] 51 52 53 55 56 57 58 59 60 61 District 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it: d4$district_id &lt;- as.integer(as.factor(d4$district)) sort(unique(d4$district_id)) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## [51] 51 52 53 54 55 56 57 58 59 60 ow there are 60 values, contiguous integers 1 to 60. Now, focus on predicting use.contraception, clustered by district_id. Do not include urban just yet. Fit both (1) a traditional fixed-effects model that uses dummy variables for district and (2) a multilevel model with varying intercepts for district. Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In particular, can you explain the most extreme cases of disagreement, both why they happen where they do and why the models reach different inferences? バングラデシュの産子数に関するデータ。 - district: 女性の住んでいた地区。 - use.contraception: 避妊を行ったか否か。 data(&quot;bangladesh&quot;) d4 &lt;- bangladesh glimpse(d4) ## Rows: 1,934 ## Columns: 6 ## $ woman &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1… ## $ district &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ use.contraception &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1… ## $ living.children &lt;int&gt; 4, 1, 3, 4, 1, 1, 4, 4, 2, 4, 1, 1, 2, 4, 4, 4, 1, 4… ## $ age.centered &lt;dbl&gt; 18.4400, -5.5599, 1.4400, 8.4400, -13.5590, -11.5600… ## $ urban &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… 避妊の有無が、地区によってどのくらいばらついていたのかを推定。 固定効果とランダム効果に地区名を入れたモデルをそれぞれまわす。 d4 %&gt;% mutate(district = factor(district)) -&gt; d4 b13H1a &lt;- brm( data = d4, family = bernoulli, formula = use.contraception ~ 0 + district, prior = prior(normal(0,3), class = b), backend = &quot;cmdstanr&quot;, seed = 13, file = &quot;output/Chapter13/b13H1a&quot; ) b13H1b &lt;- brm( data = d4, family = bernoulli, formula = use.contraception ~ 1 + (1|district), prior = c(prior(normal(0,3), class = Intercept), prior(exponential(1), class =sd)), backend = &quot;cmdstanr&quot;, seed = 13, file = &quot;output/Chapter13/b13H1b&quot; ) ランダム効果モデルでは縮合が起きて、元データの平均（点線）に近づいていることが分かる。また、データ数が少ない地区（十字架が小さい地区）ほどより縮合が起きている。 13.7.8 13H2 Return to the Trolley data, data(Trolley), from Chapter 12. Define and fit a varying intercepts model for these data. Cluster intercepts on individual participants, as indicated by the unique values in the id variable. Include action, intention, and contact as ordinary terms. Compare the varying intercepts model and a model that ignores individuals, using both WAIC and posterior predictions. What is the impact of individual variation in these data? Chapter12でも用いたトロッコ問題のデータ(Cushman et al. 2006)を用いる。被験者のIDをランダム効果に入れる。 data(Trolley) d5 &lt;- Trolley inits &lt;- list(`Intercept[1]` = -2, `Intercept[2]` = -1, `Intercept[3]` = 0, `Intercept[4]` = 1, `Intercept[5]` = 2, `Intercept[6]` = 2.5) inits_list &lt;- list(inits, inits, inits, inits) b13H2 &lt;- brm(data = d5, family = cumulative, response ~ 1 + action + contact + intention + intention:action + intention:contact + (1|id), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 6500, warmup = 4000, cores = 4, chains = 4, seed = 12, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13H2&quot;) b12.5 &lt;- readRDS(&quot;output/Chapter12/b12.5.rds&quot;) 結果は以下の通り（表13.14）。ランダム効果を含む方が推定値が0から離れている。これは、ランダム効果がデータのばらつきの多くを説明しているため、説明変数の効果がより明確になったから？ 表13.14: モデルの比較2 including random fixed only par Estimate SE Q2.5 Q97.5 Estimate SE Q2.5 Q97.5 b_Intercept[1] -3.73 0.12 -3.96 -3.49 -2.64 0.05 -2.74 -2.54 b_Intercept[2] -2.78 0.12 -3.01 -2.55 -1.94 0.05 -2.04 -1.85 b_Intercept[3] -1.97 0.12 -2.20 -1.75 -1.35 0.05 -1.44 -1.26 b_Intercept[4] -0.47 0.11 -0.70 -0.25 -0.31 0.04 -0.4 -0.23 b_Intercept[5] 0.58 0.11 0.35 0.80 0.36 0.04 0.27 0.45 b_Intercept[6] 1.99 0.12 1.76 2.22 1.26 0.05 1.17 1.36 b_action -0.65 0.06 -0.77 -0.55 -0.48 0.05 -0.58 -0.37 b_contact -0.46 0.07 -0.60 -0.32 -0.34 0.07 -0.48 -0.21 b_intention -0.39 0.06 -0.51 -0.27 -0.29 0.06 -0.41 -0.18 b_action:intention -0.55 0.08 -0.71 -0.39 -0.43 0.08 -0.59 -0.27 b_contact:intention -1.66 0.10 -1.86 -1.46 -1.23 0.1 -1.42 -1.04 sd_id__Intercept 1.92 0.08 1.76 2.08 予測分布を描くと以下の通り。 最後に、WAICを比較すると以下の通り。圧倒的にランダム効果を含んだ方がよい。 b12.5 &lt;- add_criterion(b12.5, &quot;waic&quot;) b13H2 &lt;- add_criterion(b13H2, &quot;waic&quot;) loo_compare(b12.5, b13H2, criterion=&quot;waic&quot;) %&gt;% print(simplify = FALSE) %&gt;% kable(booktabs = TRUE, digits=2) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;,&quot;hold_position&quot;)) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic ## b13H2 0.0 0.0 -15528.8 89.7 356.2 4.7 31057.5 ## b12.5 -2935.8 86.8 -18464.5 40.4 10.9 0.1 36929.0 ## se_waic ## b13H2 179.5 ## b12.5 80.7 elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic b13H2 0.00 0.0 -15528.75 89.73 356.24 4.68 31057.50 179.46 b12.5 -2935.75 86.8 -18464.50 40.37 10.85 0.06 36929.01 80.75 13.7.9 13H3 The Trolley data are also clustered by story, which indicates a unique narrative for each vignette. Define and fit a cross-classified varying intercepts model with both id and story. Use the same ordinary terms as in the previous problem. Compare this model to the previous models. What do you infer about the impact of different stories on responses? 先ほどのモデルに、storyもランダム効果に加える。 b13H3 &lt;- brm(data = d5, family = cumulative, response ~ 1 + action + contact + intention + intention:action + intention:contact + (1|id) +(1|story), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 6500, warmup = 5000, cores = 4, chains = 4, seed = 12, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter13/b13H3&quot;) b13H3 %&gt;% posterior_summary() %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par,&quot;b_|sd&quot;)) %&gt;% set_names(c(&quot;par&quot;,&quot;Est_r2&quot;,&quot;SE_r2&quot;,&quot;Q2.5_r2&quot;,&quot;Q97.5_r2&quot;)) -&gt; r_random2 idのみをランダム効果に入れたモデルとの比較は以下の通り（表13.15）。少し推定値に違いが出ている。 r_random2 %&gt;% full_join(r_random) %&gt;% mutate(across(where(is.numeric), ~ round(.x,2)))-&gt; r_full2 r_full2[is.na(r_full2)] &lt;- &quot;-&quot; r_full2 %&gt;% kable(booktabs = TRUE, caption = &quot;モデルの比較3&quot;, digits =2, col.names = c(&quot;par&quot;, rep(c(&quot;Estimate&quot;,&quot;SE&quot;,&quot;Q2.5&quot;,&quot;Q97.5&quot;),2)), align = &quot;c&quot;) %&gt;% add_header_above(c(&quot;&quot;, &quot;id + story&quot;=4, &quot;id only&quot;=4)) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;,&quot;hold_position&quot;, &quot;repeat_header&quot;,&quot;scale_down&quot;)) 表13.15: モデルの比較3 id + story id only par Estimate SE Q2.5 Q97.5 Estimate SE Q2.5 Q97.5 b_Intercept[1] -4.04 0.20 -4.43 -3.65 -3.73 0.12 -3.96 -3.49 b_Intercept[2] -3.07 0.20 -3.45 -2.68 -2.78 0.12 -3.01 -2.55 b_Intercept[3] -2.24 0.20 -2.62 -1.86 -1.97 0.12 -2.2 -1.75 b_Intercept[4] -0.69 0.20 -1.06 -0.31 -0.47 0.11 -0.7 -0.25 b_Intercept[5] 0.40 0.20 0.02 0.77 0.58 0.11 0.35 0.8 b_Intercept[6] 1.84 0.20 1.46 2.22 1.99 0.12 1.76 2.22 b_action -0.90 0.07 -1.04 -0.77 -0.65 0.06 -0.77 -0.55 b_contact -1.09 0.10 -1.28 -0.90 -0.46 0.07 -0.6 -0.32 b_intention -0.47 0.07 -0.60 -0.33 -0.39 0.06 -0.51 -0.27 b_action:intention -0.52 0.08 -0.69 -0.36 -0.55 0.08 -0.71 -0.39 b_contact:intention -1.28 0.11 -1.49 -1.06 -1.66 0.1 -1.86 -1.46 sd_id__Intercept 1.98 0.08 1.81 2.15 1.92 0.08 1.76 2.08 sd_story__Intercept 0.55 0.14 0.35 0.89 WAIを比較すると以下の通り。storyもランダム効果に含んだモデルの方が少しだけ良い。 b13H3 &lt;- add_criterion(b13H3, &quot;waic&quot;) loo_compare(b13H3, b13H2, criterion=&quot;waic&quot;) %&gt;% print(simplify = FALSE) %&gt;% kable(booktabs = TRUE, digits=2) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;,&quot;hold_position&quot;)) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic ## b13H3 0.0 0.0 -15283.5 90.2 366.7 4.8 30567.0 ## b13H2 -245.2 21.3 -15528.8 89.7 356.2 4.7 31057.5 ## se_waic ## b13H3 180.5 ## b13H2 179.5 elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic b13H3 0.00 0.00 -15283.51 90.24 366.75 4.80 30567.03 180.47 b13H2 -245.24 21.33 -15528.75 89.73 356.24 4.68 31057.50 179.46 13.7.10 13H4 Revisit the Reed frog survival data, data(reedfrogs), and add the predation and size treatmenr variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interactions. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (th \\(\\sigma\\) across tanks). Explain why it changes as it does across models with different predictors included. 再びreed frogデータで、predとsizeもランダム効果に入れてみる。 b13H4 &lt;- brm(data =dd, family = binomial, formula = surv|trials(density) ~ 1 + (1|tank) + (1|size)+ (1|pred), prior = c(prior(normal(0,1.5), class = Intercept), prior(exponential(1), class = sd)), control = list(adapt_delta =.999), iter = 6000, warmup=5000, seed = 15, file = &quot;output/Chapter13/b13H4&quot;, backend = &quot;cmdstanr&quot;, ) これまでのモデルと比較する（表13.16）。そこまで大きな違いはない。 b13H4 &lt;- add_criterion(b13H4, &quot;waic&quot;) loo_compare(b13M1a,b13M1b,b13M1c,b13M1d, b13H4,criterion=&quot;waic&quot;) %&gt;% print(simplify = F) %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;WAICの比較2。&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13M1a 0.0 0.0 -99.3 4.6 19.0 2.0 198.6 9.1 ## b13M1c -0.7 1.0 -100.0 4.4 19.1 1.9 199.9 8.8 ## b13H4 -0.8 0.9 -100.1 4.5 19.4 2.0 200.1 9.1 ## b13M1b -0.9 2.9 -100.2 3.6 21.0 0.9 200.4 7.3 ## b13M1d -1.0 1.7 -100.2 4.8 19.3 2.2 200.5 9.6 表13.16: WAICの比較2。 elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic b13M1a 0.00 0.00 -99.28 4.55 19.03 1.96 198.55 9.11 b13M1c -0.69 0.96 -99.96 4.41 19.12 1.89 199.92 8.83 b13H4 -0.78 0.85 -100.05 4.53 19.40 1.97 200.11 9.07 b13M1b -0.94 2.89 -100.22 3.63 21.02 0.86 200.43 7.27 b13M1d -0.97 1.68 -100.24 4.82 19.34 2.15 200.49 9.65 交互作用も含むモデルと事後分布を比較すると以下の通り。すべてではないが、全部ランダム効果に入れたモデルの方がより縮合が起きている？ ランダム効果tankの標準偏差の推定値はほとんど変わらない。 posterior_summary(b13H4) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par,&quot;sd&quot;)) %&gt;% kable(booktabs =TRUE, digits =2) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;, &quot;striped&quot;)) par Estimate Est.Error Q2.5 Q97.5 sd_pred__Intercept 1.74 0.82 0.70 3.77 sd_size__Intercept 0.64 0.62 0.03 2.28 sd_tank__Intercept 0.79 0.15 0.52 1.11 posterior_summary(b13M1d) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par,&quot;sd&quot;)) %&gt;% kable(booktabs =TRUE, digits =2) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;, &quot;striped&quot;)) par Estimate Est.Error Q2.5 Q97.5 sd_tank__a_Intercept 0.74 0.14 0.49 1.05 References "],["adventures-in-covariance.html", "14 Adventures in Covariance 14.1 Varying slopes by construction 14.2 Advanced varing slopes 14.3 Instruments and causal designs 14.4 Social relations as correlated varying effects 14.5 Continuous categories and the Gaussian process 14.6 Practice", " 14 Adventures in Covariance 14.1 Varying slopes by construction ランダム切片とランダム傾きの両方を推定するためには、多変量正規分布を用いる必要がある。 14.1.1 Simulate and population ロボットがカフェを訪れた際の待ち時間に関するデータをシミュレートする。パラメータは以下のとおりとする。すなわち、ランダム切片とランダム傾きは平均と標準偏差がそれぞれ3.5と-1, 1と0.7の多変量正規分布から得られるとする。なお、分散共分散行列は通常以下のようにあらわす。なお、\\(\\sigma_{\\alpha}\\)と\\(\\sigma_{\\beta}\\)はそれぞれランダム切片とランダム傾きのばらつき（標準偏差）を表し、\\(\\rho\\)はそれらの相関を表す。 \\[ \\sum = \\begin{bmatrix} \\sigma_{\\alpha}^2 &amp; \\sigma_{\\alpha}\\sigma_{\\beta}\\rho\\\\ \\sigma_{\\alpha}\\sigma_{\\beta}\\rho &amp; \\sigma_{\\beta} \\end{bmatrix} \\] a &lt;- 3.5 # averange morning wait time b &lt;- (-1) # average difference afternoon wait time sigma_a &lt;- 1 # std dev in intercepts sigma_b &lt;- 0.5 # std dev in slopes rho &lt;- (-0.7) # correlation between interecpts and slopes mu &lt;- c(a,b) cov_ab &lt;- sigma_a*sigma_b*rho #共分散 sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol=2) # 分散共分散行列 なお、分散共分散行列は以下のようにしても作成できる。 sigmas &lt;- c(sigma_a, sigma_b) rho &lt;- matrix(c(1, rho, rho, 1), nrow =2) sigma &lt;- diag(sigmas) %*% rho %*% diag(sigmas) それでは、それぞれのお店（N=20）の切片と傾きをシミュレートする。 n_cafes &lt;- 20 set.seed(5) vary_effects &lt;- MASS::mvrnorm(n_cafes, mu, sigma) %&gt;% data.frame() %&gt;% set_names(&quot;a_cafe&quot;, &quot;b_cafe&quot;) head(vary_effects) ## a_cafe b_cafe ## 1 4.223962 -1.6093565 ## 2 2.010498 -0.7517704 ## 3 4.565811 -1.9482646 ## 4 3.343635 -1.1926539 ## 5 1.700971 -0.5855618 ## 6 4.134373 -1.1444539 14.1.2 Simulate observation それでは、それぞれのお店を10回訪れたときのデータをシミュレートする。 n_visits &lt;- 10 sigma_y &lt;- 0.5 set.seed(22) d &lt;- vary_effects %&gt;% mutate(cafe = 1:n()) %&gt;% tidyr::expand(nesting(cafe, a_cafe, b_cafe), visits = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n()/2)) %&gt;% mutate(mu = a_cafe + b_cafe*afternoon) %&gt;% mutate(wait = rnorm(n=n(), mean=mu, sd=sigma_y)) d %&gt;% slice_sample(n=10) ## # A tibble: 10 × 7 ## cafe a_cafe b_cafe visits afternoon mu wait ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2.01 -0.752 4 1 1.26 1.22 ## 2 14 3.51 -1.44 10 1 2.07 2.98 ## 3 10 3.47 -0.680 5 0 3.47 3.67 ## 4 20 3.77 -1.06 10 1 2.72 3.63 ## 5 4 3.34 -1.19 1 0 3.34 3.61 ## 6 13 4.30 -2.11 4 1 2.19 1.21 ## 7 5 1.70 -0.586 1 0 1.70 1.59 ## 8 10 3.47 -0.680 3 0 3.47 3.69 ## 9 1 4.22 -1.61 4 1 2.61 2.76 ## 10 17 4.22 -0.919 5 0 4.22 4.08 実際のデータをカフェ2とカフェ8について描写してみる。 14.1.3 The varying slope model それでは、モデリングを行う。モデル式は以下の通り。 \\[ \\begin{aligned} W_{i} &amp;\\sim Normal(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp;= \\alpha_{cafe[i]} + \\beta_{cafe[i]}A_{i}\\\\ \\begin{bmatrix} \\alpha_{cafe} \\\\ \\beta_{cafe} \\end{bmatrix} &amp;\\sim MVNormal( \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S})\\\\ \\mathbf{S} &amp;= \\begin{bmatrix} \\sigma_{\\alpha} &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta} \\end{bmatrix} \\mathbf{R} \\begin{bmatrix} \\sigma_{\\alpha} &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta} \\end{bmatrix}\\\\ \\mathbf{R} &amp;= \\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}\\\\ \\alpha &amp;\\sim Normal(5,2)\\\\ \\beta &amp;\\sim Normal(-1,0.5)\\\\ \\sigma &amp;\\sim Exponential(1)\\\\ \\sigma_{\\alpha} &amp;\\sim Exponential(1)\\\\ \\sigma_{\\beta} &amp;\\sim Exponential(1)\\\\ mathbf{R} &amp;\\sim LKJcorr(2)\\\\ \\end{aligned} \\] なお、\\(LKJcorr\\)分布は以下のようなパラメータ\\(\\eta\\)を持つ分布である。\\(\\eta=1\\)のときは一様分布で、\\(\\eta\\)が大きくなるほど分布が狭くなっていく。 それでは、モデリングを行う。 b14.1 &lt;- brm(data =d, family = gaussian, wait ~ 1 + afternoon + (1+afternoon|cafe), prior = c(prior(normal(5,2), class = Intercept), prior(normal(-1,0.5), class = b), prior(exponential(1), class = sigma), prior(exponential(1), class =sd), prior(lkj(2), class = cor)), backend = &quot;cmdstanr&quot;, seed = 155, file = &quot;output/Chapter14/b14.1&quot;) 切片と傾きの相関の事前分布と事後分布は以下の通り。 各カフェの切片と傾きの推定値と元データを比較したものが以下の図である。モデルによる推定値では全体の平均へと縮合が起きていることが分かる。 続いて、午前と午後の待ち時間の元データとモデルによる推定値は以下の通り。同様に縮合が生じていることが分かる。 14.2 Advanced varing slopes チンパンジーの実験データ(Silk et al. 2005)を用いて、2つ以上ランダム傾きを用いるモデルを考える。 data(chimpanzees) d2 &lt;- chimpanzees d2 &lt;- d2 %&gt;% mutate(actor = factor(actor), block = factor(block), treatment = factor(1+prosoc_left + 2*condition), labels = factor(treatment, levels =1:4, labels = c(&quot;r/n&quot;,&quot;l/n&quot;,&quot;r/p&quot;,&quot;l/p&quot;))) モデル式は以下のようになる。以下のモデルでは、各個体、各ブロックによって条件（treatment）の効果が変わることを想定している。 \\[ \\begin{aligned} L_{i} &amp;\\sim Binomial(1,p_{i})\\\\ logit(p_{i}) &amp;= \\gamma_{TID[i]} + \\alpha_{ACTOR[i],TID[i]}+ \\beta_{BLOCK[i],TID[i]}\\\\ \\gamma_{j} &amp;\\sim Normal(0,1)\\\\ \\begin{bmatrix} \\alpha_{j,1}\\\\ \\alpha_{j,2}\\\\ \\alpha_{j,3}\\\\ \\alpha_{j,4} \\end{bmatrix} &amp; \\sim MVNormal( \\begin{bmatrix} 0\\\\0\\\\0\\\\0 \\end{bmatrix} , \\sum_{ACTOR})\\\\ \\begin{bmatrix} \\beta_{j,1}\\\\ \\beta_{j,2}\\\\ \\beta_{j,3}\\\\ \\beta_{j,4} \\end{bmatrix} &amp; \\sim MVNormal( \\begin{bmatrix} 0\\\\0\\\\0\\\\0 \\end{bmatrix} , \\sum_{BLOCK})\\\\ \\sum_{ACTOR} &amp;= \\mathbf{S}_{\\alpha}\\mathbf{R}_{\\alpha}\\mathbf{S}_{\\alpha}\\\\ \\sum_{BLOCK} &amp;= \\mathbf{S}_{\\beta}\\mathbf{R}_{\\beta}\\mathbf{S}_{\\beta}\\\\ \\mathbf{S}_{\\alpha} &amp;= \\begin{bmatrix} \\sigma_{\\alpha,[1]}&amp;0&amp;0&amp;0\\\\ 0&amp;\\sigma_{\\alpha,[2]}&amp;0&amp;0\\\\ 0&amp;0&amp;\\sigma_{\\alpha,[3]}&amp;0\\\\ 0&amp;0&amp;0&amp;\\sigma_{\\alpha,[4]} \\end{bmatrix}\\\\ \\mathbf{S}_{\\beta} &amp;= \\begin{bmatrix} \\sigma_{\\beta,[1]}&amp;0&amp;0&amp;0\\\\ 0&amp;\\sigma_{\\beta,[2]}&amp;0&amp;0\\\\ 0&amp;0&amp;\\sigma_{\\beta,[3]}&amp;0\\\\ 0&amp;0&amp;0&amp;\\sigma_{\\beta,[4]} \\end{bmatrix}\\\\ \\mathbf{R}_{\\alpha} &amp;= \\begin{bmatrix} 1&amp;\\rho_{\\alpha,[1,2]} &amp; \\rho_{\\alpha,[1,3]} &amp; \\rho_{\\alpha,[1,4]}\\\\ \\rho_{\\alpha,[2,1]} &amp; 1 &amp;\\rho_{\\alpha,[2,3]} &amp; \\rho_{\\alpha,[2,4]}\\\\ \\rho_{\\alpha,[3,1]} &amp; \\rho_{\\alpha,[3,2]} &amp; 1 &amp; \\rho_{\\alpha,[3,4]}\\\\ \\rho_{\\alpha,[4,1]} &amp; \\rho_{\\alpha,[4,2]} &amp; \\rho_{\\alpha,[4,3]} &amp; 1\\\\ \\end{bmatrix}\\\\ \\mathbf{R}_{\\beta} &amp;= \\begin{bmatrix} 1&amp;\\rho_{\\beta,[1,2]} &amp; \\rho_{\\beta,[1,3]} &amp; \\rho_{\\beta,[1,4]}\\\\ \\rho_{\\beta,[2,1]} &amp; 1 &amp;\\rho_{\\beta,[2,3]} &amp; \\rho_{\\beta,[2,4]}\\\\ \\rho_{\\beta,[3,1]} &amp; \\rho_{\\beta,[3,2]} &amp; 1 &amp; \\rho_{\\beta,[3,4]}\\\\ \\rho_{\\beta,[4,1]} &amp; \\rho_{\\beta,[4,2]} &amp; \\rho_{\\beta,[4,3]} &amp; 1\\\\ \\end{bmatrix}\\\\ \\sigma_{\\alpha,[1]},...\\sigma_{\\alpha,[4]} &amp;\\sim Exponential(1)\\\\ \\sigma_{\\beta,[1]},...\\sigma_{\\beta,[4]} &amp;\\sim Exponential(1)\\\\ \\mathbf{R}_{\\alpha} &amp;\\sim LKJ(2)\\\\ \\mathbf{R}_{\\beta} &amp;\\sim LKJ(2) \\end{aligned} \\] それでは、brmsでモデルを回す。 b14.3 &lt;- brm(data =d2, family = bernoulli, pulled_left ~ 0 + treatment + (0+treatment|actor) + (0+treatment|block), prior = c(prior(normal(0,1), class = b), prior(exponential(1), class = sd), prior(lkj(2), class = cor)), backend = &quot;cmdstanr&quot;, seed = 123, file = &quot;output/Chapter14/b14.3&quot;) ランダム切片の標準偏差は以下の通り（表14.1）。 posterior_summary(b14.3) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par,&quot;sd&quot;)) %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;b14.3の結果。&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表14.1: b14.3の結果。 par Estimate Est.Error Q2.5 Q97.5 sd_actor__treatment1 1.40 0.49 0.70 2.62 sd_actor__treatment2 0.92 0.42 0.33 1.94 sd_actor__treatment3 1.86 0.58 1.02 3.27 sd_actor__treatment4 1.60 0.63 0.74 3.15 sd_block__treatment1 0.42 0.33 0.01 1.25 sd_block__treatment2 0.42 0.34 0.02 1.27 sd_block__treatment3 0.30 0.27 0.01 0.99 sd_block__treatment4 0.48 0.38 0.02 1.37 結果を図示すると以下の通り。青い点は実データを表す。以前のモデルよりも、個体ごとのばらつきをうまく表現することができている。また、縮合が生じていることも分かる（特にactor2）。また、標準偏差が小さかった条件（treatment1と2）でより縮合が生じていることも分かる。 14.3 Instruments and causal designs 14.3.1 Instrumental variables 教育が賃金に与える影響を検証したいとする。しかし、単に賃金を教育の程度で回帰したとしても、その因果関係を知ることはできない。例えば、両方に対して影響を与えている共通の要因が考えられるからである（下図）。 このようなとき、以下の基準を満たすinstrumental variableを用いることでこのような状況を克服することができることがある。 Uとは独立（\\(U\\bot Q\\)） Eと独立でない（\\(E\\not\\bot Q\\)） QはEを通してのみWに影響を与える。 そのようなQをDAGで表すと以下のようになる。例えば、この例では生まれたのがいずれの四半期にあたるのかをQとして用いることができるかもしれない(Angrist and Keueger 1991)。これは、早く生まれた人ほど学校教育を受けない傾向があることによる。このとき、QとEを同時に説明変数に含めるとEで条件づけるとQとUが独立ではなくなり（EはQとUの合流点であるため）、結果的にQとWも独立ではなくなる。その結果、さらにEの係数が交絡するので、Qは\\(\\color{blue}{\\text{BIAS AMPLIFIER}}\\)と呼べる。 それでは、上のDAGに基づいてデータをシミュレートする。ここでは、教育による収入への影響はないものとする。 set.seed(73) n &lt;- 500 dat_sim &lt;- tibble(u_sim = rnorm(n,mean=0,sd=1), q_sim = sample(1:4, size=n, replace=TRUE)) %&gt;% mutate(e_sim = rnorm(n, mean=u_sim + q_sim, sd=1), w_sim = rnorm(n, mean=u_sim + 0*e_sim, sd =1)) %&gt;% mutate(w = standardize(w_sim), e = standardize(e_sim), q = standardize(q_sim)) head(dat_sim) ## # A tibble: 6 × 7 ## u_sim q_sim e_sim w_sim w e q ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; ## 1 -0.145 1 1.51 0.216 0.17252701 -0.57472658 -1.3562176 ## 2 0.291 1 0.664 0.846 0.58357373 -1.08822480 -1.3562176 ## 3 0.0938 3 2.44 -0.664 -0.40238235 -0.01850495 0.4282792 ## 4 -0.127 3 4.09 -0.725 -0.44241750 0.97829629 0.4282792 ## 5 -0.847 4 2.62 -1.24 -0.78028083 0.09392642 1.3205276 ## 6 0.141 4 3.54 -0.0700 -0.01457435 0.65088153 1.3205276 それでは、モデリングを行う。まずは、Qを含めないモデル。 b14.4 &lt;- brm(data = dat_sim, family = gaussian, w ~ 1 + e, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed =14, file = &quot;output/Chapter14/b14.4&quot; ) 結果は以下の通り（表14.2）。教育の影響はないにもかかわらず、交絡によって係数が正になっていることが分かる。 posterior_summary(b14.4) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(par != &quot;lp__&quot;) %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;b14.4の結果&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表14.2: b14.4の結果 par Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.00 0.04 -0.08 0.08 b_e 0.40 0.04 0.32 0.48 sigma 0.92 0.03 0.86 0.98 lprior -0.80 0.08 -0.96 -0.66 続いて、Qを含めたモデルを考える。 b14.5 &lt;- brm(data = dat_sim, family = gaussian, w ~ 1 + e + q, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed =14, file = &quot;output/Chapter14/b14.5&quot; ) 結果は以下の通り（表14.3）。DAGの通りQとEの間に関連が生じており、その結果Eの係数もさらに0から離れていることが分かる。 表14.3: b14.5の結果。 par Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.00 0.04 -0.08 0.07 b_e 0.64 0.05 0.55 0.73 b_q -0.41 0.05 -0.50 -0.31 sigma 0.86 0.03 0.80 0.91 lprior -1.78 0.18 -2.15 -1.46 Qを用いて正しい因果関係を推定するためには、以下のような多変量モデルを作成すればよい。このモデルではWとEの残差に相関がある可能性があることを示しており（これは、Uが交絡要因となっているため）、これが5章で扱ったモデルとは異なる。なお、Uを欠損値として代入する方法もあるが、それについては次の章で学ぶ。 \\[ \\begin{aligned} \\begin{bmatrix} W_{i}\\\\E_{i} \\end{bmatrix} &amp;\\sim MVNorm( \\begin{bmatrix} \\mu_{W,i}\\\\ \\mu_{E,i} \\end{bmatrix} , \\mathbf{\\sum})\\\\ \\mu_{W,i} &amp;= \\alpha_{W} + \\beta_{EW}E_{i}\\\\ \\mu_{E,i} &amp;= \\alpha_{E} + \\beta_{QW}Q_{i}\\\\ \\mathbf{\\sigma} &amp;= \\begin{bmatrix} \\sigma_{W}&amp;0\\\\0&amp;\\sigma_{E} \\end{bmatrix} \\mathbf{R} \\begin{bmatrix} \\sigma_{W}&amp;0\\\\0&amp;\\sigma_{E} \\end{bmatrix}\\\\ \\mathbf{R} &amp;= \\begin{bmatrix} 1&amp;\\rho\\\\ \\rho&amp;1 \\end{bmatrix}\\\\ \\alpha_{W}, \\alpha_{E} &amp;\\sim Normal(0,0.2)\\\\ \\beta_{EW}. \\beta_{QE} &amp;\\sim Normal(0,0.5)\\\\ \\sigma_{W}, \\sigma_{W} &amp;\\sim Exponential(1)\\\\ \\rho &amp;\\sim LKJcorr(2) \\end{aligned} \\] brmsでモデリングする。 e_model &lt;- bf(e ~ 1+q) w_model &lt;- bf(w ~ 1+e) b14.6 &lt;- brm(data = dat_sim, family = gaussian, e_model + w_model + set_rescor(TRUE), prior = c(prior(normal(0, 0.2), class = Intercept,resp = e), prior(normal(0, 0.5), class = b, resp = e), prior(exponential(1), class = sigma, resp = e), prior(normal(0, 0.2), class = Intercept,resp = w), prior(normal(0, 0.5), class = b, resp = w), prior(exponential(1), class = sigma, resp = w), prior(lkj(2), class = rescor)), backend = &quot;cmdstanr&quot;, seed = 14, file = &quot;output/Chapter14/b14.6&quot;) 結果は以下の通り（表14.4）。うまく推定が行えていることが分かる。誤差間の相関（rescor）が大きいことは、 EとWが共通の要因から影響を受けていることを強く示唆している。 表14.4: b14.6の結果 par Estimate Est.Error Q2.5 Q97.5 b_e_Intercept 0.00 0.04 -0.07 0.07 b_w_Intercept 0.00 0.05 -0.09 0.09 b_e_q 0.59 0.04 0.52 0.66 b_w_e -0.05 0.08 -0.20 0.10 sigma_e 0.81 0.02 0.76 0.86 sigma_w 1.02 0.05 0.94 1.12 rescor__e__w 0.54 0.05 0.43 0.64 lprior -2.29 0.14 -2.59 -2.04 それでは、今度は教育による効果があるというデータをシミュレートする。 set.seed(73) n &lt;- 500 dat_sim2 &lt;- tibble(u_sim = rnorm(n,mean=0,sd=1), q_sim = sample(1:4, size=n, replace=TRUE)) %&gt;% mutate(e_sim = rnorm(n, mean=u_sim + q_sim, sd=1), w_sim = rnorm(n, mean=-u_sim + 0.2*e_sim, sd =1)) %&gt;% mutate(w = standardize(w_sim), e = standardize(e_sim), q = standardize(q_sim)) head(dat_sim2) ## # A tibble: 6 × 7 ## u_sim q_sim e_sim w_sim w e q ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; &lt;dw_trnsf&gt; ## 1 -0.145 1 1.51 0.809 0.24777771 -0.57472658 -1.3562176 ## 2 0.291 1 0.664 0.396 -0.05626233 -1.08822480 -1.3562176 ## 3 0.0938 3 2.44 -0.364 -0.61500793 -0.01850495 0.4282792 ## 4 -0.127 3 4.09 0.347 -0.09215742 0.97829629 0.4282792 ## 5 -0.847 4 2.62 0.976 0.37002007 0.09392642 1.3205276 ## 6 0.141 4 3.54 0.357 -0.08516221 0.65088153 1.3205276 それでは、同様に3つのモデリングを行う。 b14.4x &lt;- update(b14.4, newdata = dat_sim2, seed =14, file = &quot;output/Chapter14/b14.4x&quot;) b14.5x &lt;- update(b14.5, newdata = dat_sim2, seed =14, file = &quot;output/Chapter14/b14.5x&quot;) b14.6x &lt;- update(b14.6, newdata = dat_sim2, seed =14, file = &quot;output/Chapter14/b14.6x&quot;) 推定値を図示すると以下のようになる。やはり3つめのモデルでうまく推定ができたことが分かる。また、EとQの残差の相関は負になっている。これは、UがWには正に影響し、Eには負に影響していたからである。 dagittyパッケージにはDAG内でinstrumental variableがあるか判定してくれる関数が存在する。 library(dagitty) dagIV &lt;- dagitty(&quot;dag{Q -&gt; E &lt;- U -&gt; W &lt;- E}&quot;) instrumentalVariables(dagIV, exposure = &quot;E&quot;, outcome = &quot;W&quot;) ## Q 現実的にはある変数がinstrumental variableかを判断することは難しい。背景となる科学的な知識に基づいて判断することが重要である。 14.3.2 Other designs 14.3.2.1 Front door criterion 以下のようなDAGがあるとする。もしXとYの因果関係を調べたいとする。もし、Zが確実にYに影響を与えているとすれば、フロントドア基準が用いて因果関係を推定することができる。 例として、大学で形成された社会的紐帯と上院における投票行動の関連を分析した研究がある(Cohen and Malloy 2014)。上院では隣の座席になった人同士で同じような投票行動をとる可能性が高いと考えられるが、席順はランダムに割り振られる。よって、席順と出身大学は同じ要因から影響を受けていないといえる。よって、もし座席が隣同士でかつ同じ出身大学の人が座席が隣でかつ同じ出身大学でない人よりも同じような投票行動をするとき、出身大学が同じであることが投票行動に影響しているということができる。 14.3.2.2 Regression discontinuity 回帰分断デザインについては、他の本参照。 14.4 Social relations as correlated varying effects ニカラグアの家庭間での贈り物の交換に関するデータを扱う(Koster and Leckie 2014)。 data(&quot;KosterLeckie&quot;) d3 &lt;- kl_dyads head(kl_dyads) %&gt;% kable(booktabs = TRUE) %&gt;% kable_styling(latex_options = c(&quot;hold_position&quot;,&quot;striped&quot;)) hidA hidB did giftsAB giftsBA offset drel1 drel2 drel3 drel4 dlndist dass d0125 1 2 1 0 4 0.000 0 0 1 0 -2.790 0.000 0 1 3 2 6 31 -0.003 0 1 0 0 -2.817 0.044 0 1 4 3 2 5 -0.019 0 1 0 0 -1.886 0.025 0 1 5 4 4 2 0.000 0 1 0 0 -1.892 0.011 0 1 6 5 8 2 -0.003 1 0 0 0 -3.499 0.022 0 1 7 6 2 1 0.000 0 0 0 0 -1.853 0.071 0 下図は、家庭AとBの間の贈り物の数をプロットしたもの。相関は0.24と高くないが、それだけで家庭間の交換でバランスが取れていないと判断するべきではない。なぜなら、他家庭に送るプレゼントの量は各家庭の経済状況などに依存するため、これらの影響を切り離す必要があるからである。 そこで、以下のモデルを考える。なお、\\(g_{A}\\)は家庭Aの贈り物を送る傾向を表すランダム効果、\\(r_B\\)は家庭Bが贈り物を受ける傾向を示すランダム効果、\\(d_{AB}\\)は各ダイアッド間での贈り物の量のばらつきを表すランダム効果である。 \\[ \\begin{aligned} y_{A\\to B} &amp;\\sim Poisson(\\lambda_{AB})\\\\ log\\lambda_{AB} &amp;= \\alpha + g_{A} + r_{B} + d_{AB}\\\\ \\end{aligned} \\] 家庭Bに対する贈り物についても同様。 \\[ \\begin{aligned} y_{B\\to A} &amp;\\sim Poisson(\\lambda_{BA})\\\\ log\\lambda_{BA} &amp;= \\alpha + g_{B} + r_{A} + d_{BA}\\\\ \\end{aligned} \\] また、\\(g\\)と\\(r\\)、\\(d_{AB}\\)と\\(d_{BA}\\)の間には相関があると想定できるので、以下のようにモデリングする。 \\[ \\begin{aligned} \\begin{pmatrix} g_{i}\\\\r_{i} \\end{pmatrix} &amp;\\sim MVNormal \\begin{pmatrix} \\begin{pmatrix} 0\\\\0 \\end{pmatrix}, \\begin{pmatrix} \\sigma^2_{g} &amp; \\sigma_{g}\\sigma_{r}\\rho_{gr}\\\\ \\sigma_{g}\\sigma_{r}\\rho_{gr} &amp; \\sigma^2_{r} \\end{pmatrix} \\end{pmatrix}\\\\ \\begin{pmatrix} d_{ij}\\\\d_{ji} \\end{pmatrix} &amp;\\sim MVNormal \\begin{pmatrix} \\begin{pmatrix} 0\\\\0 \\end{pmatrix}, \\begin{pmatrix} \\sigma^2_{d} &amp; \\sigma^2_{d}\\rho_{d}\\\\ \\sigma^2_{d}\\rho_{d} &amp; \\sigma^2_{d} \\end{pmatrix} \\end{pmatrix} \\end{aligned} \\] それでは、モデリングを行う。brmsパッケージではこのモデルを記述できないため、ここではrethinkingパッケージを用いる。 # kl_data &lt;- # list( # N = nrow(kl_dyads), # N_households = max(kl_dyads$hidB), # did = kl_dyads$did, # hidA = kl_dyads$hidA, # hidB = kl_dyads$hidB, # giftsAB = kl_dyads$giftsAB, # giftsBA = kl_dyads$giftsBA # ) # # m14.7 &lt;- # ulam( # alist( # giftsAB ~ poisson(lambdaAB), # giftsBA ~ poisson(lambdaBA), # # log(lambdaAB) &lt;- a + gr[hidA, 1] + gr[hidB, 2] + d[did, 1] , # log(lambdaBA) &lt;- a + gr[hidB, 1] + gr[hidA, 2] + d[did, 2] , # # a ~ normal(0, 1), # # vector[2]:gr[N_households]~multi_normal(0,Rho_gr,sigma_gr), # Rho_gr ~ lkj_corr(4), # sigma_gr ~ exponential(1), # # transpars&gt; matrix[N,2]:d &lt;- # compose_noncentered(rep_vector(sigma_d, 2), L_Rho_d, z), # matrix[2,N]:z ~ normal(0, 1), # cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky(8), # sigma_d ~ exponential(1), # gq&gt; matrix[2, 2]:Rho_d &lt;&lt;- Chol_to_Corr(L_Rho_d) # ), # data = kl_data, # chains = 4, cores = 4, iter = 2000 # ) m14.7 &lt;- readRDS(&quot;output/Chapter14/m14.7.rds&quot;) \\(\\rho_{gr}\\)と\\(\\sigma^2_{g}\\)、\\(\\sigma^2_{r}\\)の推定値は以下の通り（表14.5）。結果、多くの贈り物をする人は、あまり贈り物をもらわない傾向にあることが分かった。また、受け取る量よりも受け取る量のばらつきの方が大きい。 表14.5: m14.7の結果 par mean sd X5.5. X94.5. n_eff Rhat4 Rho_gr[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho_gr[1,2] -0.41 0.20 -0.71 -0.05 1127.75 1 Rho_gr[2,1] -0.41 0.20 -0.71 -0.05 1127.75 1 Rho_gr[2,2] 1.00 0.00 1.00 1.00 3920.71 1 sigma_gr[1] 0.84 0.14 0.64 1.08 2320.01 1 sigma_gr[2] 0.42 0.09 0.29 0.58 1181.81 1 各家庭の事後平均を示したのが下図である。楕円は、50%信用区間を表す。 \\(\\sigma^2_{d}\\)と\\(\\rho_{d}\\)の推定値は以下の通り（表14.6）。 precis(m14.7, depth = 3, pars = c(&quot;Rho_d&quot;, &quot;sigma_d&quot;)) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% kable(booktabs =T, digits=2, caption = &quot;m14.7の結果2 &quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;,&quot;hold_position&quot;)) 表14.6: m14.7の結果2 par mean sd X5.5. X94.5. n_eff Rhat4 Rho_d[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho_d[1,2] 0.88 0.03 0.82 0.93 1012.29 1 Rho_d[2,1] 0.88 0.03 0.82 0.93 1012.29 1 Rho_d[2,2] 1.00 0.00 1.00 1.00 NaN NaN sigma_d 1.10 0.06 1.02 1.19 1283.94 1 下図は、事後平均をプロットしたものである。\\(d_{ij}\\)と\\(d_{ji}\\)の間には強い相関があることが分かる。すなわち、家庭間の贈与量はバランスが取れている（家庭AがBにあげる量が少なければ、BがAにあげる量も少ない）。また、ダイアッド間でばらつきが大きいことも分かる。この結果は、互恵性が成立していることを示している。 14.5 Continuous categories and the Gaussian process ここまでは、ランダム効果は順序のない、不連続なカテゴリーに対して適応されていた。もし、連続的なカテゴリー（年齢、身長、収入など）をランダム効果に組み込む場合にはどうすればよいだろうか？ このような場合に用いることができるのがガウス過程回帰（Gaussian process regression）である。 14.5.1 Example: Spatial autocorrelation in Oceanic tools 11章でオセアニア諸国の道具数についてモデリングを行ったとき、他国との交渉頻度を低いか高いかの2カテゴリーに分類した。しかし、このモデルでは、交渉相手国がどの国なのか（小さい国なのか大きい国なのか）が考慮されていない。また、道具が各国間で交換されているのであれば、各国の道具数は独立ではないという問題点もある。また、各国の地理的距離（近い国ほど同じような素材がある）も考慮する必要がある。 以上から、各国間の地理的距離を考慮したモデルを考える必要がある。ガウス過程モデルではモデルに各国の距離マトリックスを組み込むことでこれを実現する。 data(&quot;islandsDistMatrix&quot;) d_mat &lt;- islandsDistMatrix colnames(d_mat) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) 各国間の距離は以下の通り。 モデル式は以下の通り。距離マトリックスをランダム切片に加える。 \\[ \\begin{aligned} T_{i} &amp;\\sim Poisson(\\lambda_{i})\\\\ \\lambda_{i} &amp;= exp(k_{society[i]})\\alpha P^{\\beta}_{i}/ \\gamma\\\\ \\begin{pmatrix} k_{1}\\\\k_{2}\\\\k_{3}\\\\...\\\\k_{10} \\end{pmatrix} &amp;\\sim Normal \\begin{pmatrix} \\begin{pmatrix} 0\\\\0\\\\0\\\\...\\\\0 \\end{pmatrix}, \\mathbf{K} \\end{pmatrix}\\\\ \\mathbf{K_{ij}} &amp;= \\eta^2 exp(\\rho^2D^2_{ij}) + \\delta_{ij}\\sigma^2\\\\ \\end{aligned} \\] なお、\\(K_{ij}\\)は国\\(i\\)と国\\(j\\)の共分散を表す。\\(D_{ij}\\)は国間の距離なので、\\(K_{ij}\\)は国家間の距離の二乗が大きくなるほど共分散は小さくなる。\\(\\rho\\)はこの減少の速度を表す。距離の二乗を用いたのは、それが実際の共分散の減少をよりよく表していると考えられるためである。下図は、\\(D_{ij}\\)に応じて\\(K_ij\\)がどのように変化するかを示したものである。 \\(\\eta^2\\)は最大の共分散を、\\(\\delta_{ij}\\sigma^2\\)は\\(i =j\\)のときの共分散の追加分を示す。\\(\\delta_{ij}\\)は\\(i=j\\)のときに1、それ以外のときに0をとる関数で、\\(\\sigma\\)は国内のばらつきを示す。今回は各国につき1つしかデータがないので関係ない。\\(\\eta^2\\)と\\(\\rho^2\\)の事前分布を以下のとおりとする。 \\[ \\begin{aligned} \\eta^2 &amp;\\sim Exponential(2)\\\\ \\rho^2 &amp;\\sim Exponential(0.5) \\end{aligned} \\] brmsパッケージでは、gp()を用いることでガウス過程を扱うことができる。この関数内では、各国の緯度と経度を用いて\\(D_{ij}\\)を算出する。scale = TRUEは\\(D_{ij}\\)の最大値を1とする。今回はFALSEにする。データ上の緯度と経度は十進角（decimal degree）で記されているのでkmに直すために0.11132をかける。 data(&quot;Kline2&quot;) d4 &lt;- Kline2 %&gt;% mutate(lat_adj = lat*0.11132, lon2_adj = lon2*0.11132) d4 %&gt;% dplyr::select(culture, lat, lon2, lat_adj:lon2_adj) %&gt;% kable(booktabs = TRUE, digits =2) %&gt;% kable_styling(latex_options = &quot;striped&quot;) culture lat lon2 lat_adj lon2_adj Malekula -16.3 -12.5 -1.81 -1.39 Tikopia -12.3 -11.2 -1.37 -1.25 Santa Cruz -10.7 -14.0 -1.19 -1.56 Yap 9.5 -41.9 1.06 -4.66 Lau Fiji -17.7 -1.9 -1.97 -0.21 Trobriand -8.7 -29.1 -0.97 -3.24 Chuuk 7.4 -28.4 0.82 -3.16 Manus -2.1 -33.1 -0.23 -3.68 Tonga -21.2 4.8 -2.36 0.53 Hawaii 19.9 24.4 2.22 2.72 それでは、モデリングを行う。 b14.8 &lt;- brm(data =d4, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(a)*population^b/g, a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE), b + g ~ 1, nl = TRUE), prior = c(prior(normal(0, 1), nlpar = a), prior(exponential(1), nlpar = b, lb = 0), prior(exponential(1), nlpar = g, lb = 0), prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_adjlon2_adj, nlpar = a), prior(exponential(2), class = sdgp, coef = gplat_adjlon2_adj, nlpar = a)), iter = 3000, warmup = 2000, chains = 4, cores = 4, seed = 14, sample_prior = TRUE, backend = &quot;cmdstanr&quot;, file = &quot;output/Chapter14/b14.8&quot;) 結果は以下の通り（表14.7）。地理的な情報を考慮しても、人口が道具数に影響を与えていることが示唆される。 表14.7: b14.8の結果 Estimate Est.Error Q2.5 Q97.5 b_a_Intercept 0.36 0.85 -1.32 1.97 b_b_Intercept 0.26 0.08 0.10 0.43 b_g_Intercept 0.68 0.66 0.06 2.45 sdgp_a_gplat_adjlon2_adj 0.42 0.22 0.15 0.97 lscale_a_gplat_adjlon2_adj 1.49 0.75 0.52 3.32 zgp_a_gplat_adjlon2_adj[1] -0.50 0.75 -2.04 0.94 zgp_a_gplat_adjlon2_adj[2] 0.40 0.86 -1.30 2.07 zgp_a_gplat_adjlon2_adj[3] -0.61 0.75 -2.01 0.99 zgp_a_gplat_adjlon2_adj[4] 0.98 0.67 -0.32 2.34 zgp_a_gplat_adjlon2_adj[5] 0.25 0.70 -1.12 1.68 zgp_a_gplat_adjlon2_adj[6] -1.08 0.75 -2.52 0.38 zgp_a_gplat_adjlon2_adj[7] 0.19 0.72 -1.31 1.55 zgp_a_gplat_adjlon2_adj[8] -0.24 0.88 -1.95 1.50 zgp_a_gplat_adjlon2_adj[9] 0.47 0.89 -1.49 2.08 zgp_a_gplat_adjlon2_adj[10] -0.42 0.79 -1.99 1.11 prior_b_a -0.03 1.01 -2.00 2.03 prior_sdgp_a_gplat_adjlon2_adj 0.49 0.50 0.01 1.88 prior_lscale_a__1_gplat_adjlon2_adj 1.60 1.85 0.43 5.10 prior_b_b 0.98 0.97 0.02 3.51 prior_b_g 1.03 1.01 0.03 3.81 lprior -3.50 1.49 -7.28 -1.56 lp__ -51.39 3.28 -58.72 -46.10 b_a_Interceptの推定値が教科書と異なるのは、パラメータ化の仕方が少し異なるためである。指数関数をとってやると教科書の値とほとんど同じになる（信用区間は少し違うが）。 fixef(b14.8, probs = c(.055,.945))[&quot;a_Intercept&quot;,c(1,3,4)] %&gt;% exp() ## Estimate Q5.5 Q94.5 ## 1.4385190 0.3661252 5.4049424 brmsパッケージでは、ガウス過程は以下のようにパラメータ化を行う。\\(k(x_{i},x_{j})\\)は教科書の\\(\\mathbf{K_{ij}}\\)であり、\\(||x_{i}-x_{i}||^2\\)は\\(D_{ij}^2\\)である。さらに、\\(sdgp^2\\)は\\(\\eta^2\\)であり、\\(\\rho^2 = 1/(2lscale^2)\\)である。 \\[ k(x_{i},x_{j}) = sdgp^2 exp(-||x_{i}-x_{i}||^2/(2lscale^2)) \\] brmsでは\\(sdgp^2\\)ではなく\\(sdgp\\)が、\\(lscale^2\\)ではなく \\(lscale\\)が推定される。これは、以下の変換によって教科書と同じパラメータの推定値が得られる。\\(\\eta\\)はほとんど同じ値だが、\\(\\rho\\)が大きく異なる。これは、教科書とは異なる事前分布を用いたため？ post &lt;- posterior_samples(b14.8) %&gt;% mutate(etasq = sdgp_a_gplat_adjlon2_adj^2, rhosq = 1 / (2 * lscale_a_gplat_adjlon2_adj^2)) post %&gt;% pivot_longer(etasq:rhosq, values_to = &quot;value&quot;) %&gt;% dplyr::select(name, value) %&gt;% group_by(name) %&gt;% mean_hdi(value, .width=.89) %&gt;% mutate(across(where(is.numeric), ~round(.,digits=2))) ## # A tibble: 2 × 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 etasq 0.22 0 0.43 0.89 mean hdi ## 2 rhosq 0.47 0.02 1 0.89 mean hdi 共分散\\(\\mathbf{K_{ij}}\\)の事前分布と事後分布は以下のようになる。ばらつきが大きいことが分かる。 共分散行列の事後推定値は以下のとおりである。 k &lt;- matrix(0, nrow =10, ncol =10) for(i in 1:10){ for(j in 1:10){ k[i,j] &lt;- median(post$etasq)*exp(-median(post$rhosq)* islandsDistMatrix[i,j]^2) diag(k) &lt;- median(post$etasq) + 0.01 } } rownames(k) &lt;- rownames(d_mat) colnames(k) &lt;- colnames(d_mat) 相関行列は以下の通り。行列の左上の小さな社会が強い相関を持っていることが分かる。一方、ハワイ（Ha）は他の地域から離れているのでゼロである。 rho &lt;- round(cov2cor(k),2) colnames(rho) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) rownames(rho) &lt;- colnames(rho) モデルの推定結果を図示したものが以下である。地理的に近い地域ほど事後平均が強く相関しており（A）、道具数も類似していることが読み取れる（B）。 14.5.2 Example: Phylogenetic distance 空間的な距離だけでなく、時間的な距離に対しても同様の分析を施すことができる。例えば、系統上の距離を考慮した分析を施すことで複数の種を含むデータをうまく扱うことができる。系統関係は因果関係に2つの影響を与えうる。 最近分岐した種は類似した形質を多く持つ（まだ短時間しか異なる淘汰圧にさらされていないため）。 系統的な距離は、種間の共変動を生み出す観測できない要因の代わり（proxy）として用いることができる。例えば、鳥類には空を飛行することに起因する類似の要因が影響を与えている可能性が高い。 集団サイズ(G)と脳容量(B)の関係を考える。社会脳仮説に従えば、集団サイズが大きいほど脳容量も大きいと予測される。この因果関係をDAGで表すと以下のようになる。添字は進化史上の異なる時点を表す。すなわち、\\(G_{1}\\)と時点1における集団サイズを、\\(G_{2}\\)は時点2における集団サイズを表す。また、Uは未観測の要因を表す。 しかし、現実的には\\(U\\)をコントロールできない。また、\\(G_{1}\\)や\\(B_{1}\\)も知ることができない。 このように観測できない過去の要因の影響を知るために、系統関係を用いることができる。DAGで表すと以下のようになる。Mは体重を示し、集団サイズと脳容量の両方に影響していると考えられる。未観測変数Uはいずれにも影響を与えていると考えられる。また系統的距離PはUに影響していると考えられる。バックドア基準を満たすためには、MとUで条件付けする必要がある。Mは説明変数に加えることで可能である。またUもPを用いてphylogenetic regressionを行うことで統制可能である。 それでは、実際にモデリングを行っていく。霊長類の系統樹を以下に示す。以下の分析では、この系統樹を用いて未観測要因Uをモデリングするとともに、分類群ごとの種数をコントロールする（例えば、キツネザルは他のサルに比べてデータが多いなど）。 data(&quot;Primates301&quot;) data(&quot;Primates301_nex&quot;) 系統関係を考慮したモデリングを行うに、まずはそれを考慮しない以下のモデルを考える。\\(\\mathbf{S}\\)は分散共分散行列を表し、\\(\\mathbf{I}\\)は単位行列である。すなわち、このモデルでは共分散が0であることを想定しているので、通常の回帰分析と同じである。 \\[ \\begin{aligned} \\mathbf{B} &amp;\\sim MVNormal(\\mu, \\mathbf{S})\\\\ \\mu_{i} &amp;= \\alpha + \\beta_{G} G_{i} + \\beta_{M} M_{i}\\\\ \\mathbf{S} &amp;= \\sigma^2\\mathbf{I} \\end{aligned} \\] それでは、モデリングを行う。 d5 &lt;- Primates301 %&gt;% mutate(name = as.character(name)) %&gt;% drop_na(group_size, body, brain) %&gt;% mutate(m = log(body) %&gt;% standardize(), b = log(brain) %&gt;% standardize(), g = log(group_size) %&gt;% standardize()) b14.9 &lt;- brm(data = d5, family = gaussian, b ~ 1 + g + m, prior = c(prior(student_t(3,0,2.5),class = Intercept), prior(student_t(3,0,2.5), class = b), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed =14, file = &quot;output/Chapter14/b14.9&quot; ) 結果は以下の通り(表14.8)。集団サイズと体重が脳容量に影響を与えているように見える。しかし、このモデルではバイアスを取り除くことができていないため、因果推論を行うことができない。 表14.8: b14.9の結果。 par Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.00 0.02 -0.04 0.04 b_g 0.12 0.02 0.08 0.17 b_m 0.90 0.02 0.85 0.94 sigma 0.22 0.01 0.19 0.24 lprior -6.05 0.01 -6.08 -6.03 それでは、以下では2種類のphylogenetic regressionを行う。どちらのモデルでも、先ほどのモデルの分散共分散行列\\(\\mathbf{S}\\)を系統的距離に基づいて変えるだけである。 1つ目のモデルは、ブラウン運動（正規分布のランダムウォーク）に基づいて系統樹を解釈する。この方法では、種間の共分散が系統的距離に比例して小さくなることを仮定する。実際は、系統樹の異なる場所では進化速度が異なるはずだが、それについては考慮しない。Rではapeパッケージを用いることで系統的距離を算出できる。 library(ape) spp_obs &lt;- d5$name d6 &lt;- Primates301_nex tree_trimmed &lt;- keep.tip(d6, spp_obs) Rbm &lt;- corBrownian(phy = tree_trimmed) V &lt;- vcv(Rbm) Dmat &lt;- cophenetic(tree_trimmed) 以下の図から、共分散は距離の逆数であることが分かる。 それでは、モデリングを行う。brmsパッケージでは、fcor()で既知の共分散行列をモデルに組み込む。 ## 相関行列に変換 R &lt;- V[spp_obs, spp_obs]/max(V) ## モデリング b14.10 &lt;- brm(data = d5, data2 = list(R=R), family = gaussian, b ~ 1 + m + g + fcor(R), prior = c(prior(student_t(3,0,2.5),class = Intercept), prior(student_t(3,0,2.5), class = b), prior(exponential(1), class = sigma)), backend = &quot;cmdstanr&quot;, seed =14, file = &quot;output/Chapter14/b14.10&quot;) 結果は以下の通り（表14.9）。集団サイズの影響がほとんど0になっていることが分かる。この結果から、近縁種ほど脳容量が類似しており、そのことによって集団サイズとの間に疑似相関が生じていたことが示唆される。 表14.9: b14.10の結果。 par Estimate Est.Error Q2.5 Q97.5 b_Intercept -0.20 0.17 -0.52 0.13 b_m 0.70 0.04 0.63 0.77 b_g -0.01 0.02 -0.05 0.03 sigma 0.40 0.02 0.36 0.45 lprior -6.21 0.03 -6.26 -6.17 ブラウン運動モデルは、ガウス過程の特殊例である。ブラウン運動モデルを改良するため、Ornstein-Uhlenbeck Process（OU process）が用いられることがある。このモデルでは、系統的距離と共分散の間に以下のような非線形な関係を想定する。 \\[ K_{i,j} = \\eta^2 exp(-\\rho^2 D_{ij}) \\] 前節のオセアニアの道具モデルと違う点は、距離が二乗されていないことである。OU processもガウス過程の一種である。現在brmsではOU過程をサポートしていないので、rethinkingパッケージでモデリングを行う。 dat_list &lt;- list( N_spp = nrow(d5), M = standardize(log(d5$body)), B = standardize(log(d5$brain)), G = standardize(log(d5$group_size)), Imat = diag(nrow(d5)), V = V[spp_obs, spp_obs], R = V[spp_obs, spp_obs] / max(V[spp_obs, spp_obs]), Dmat = Dmat[spp_obs, spp_obs] / max(Dmat) ) m14.11 &lt;- ulam( alist( B ~ multi_normal(mu, SIGMA), mu &lt;- a + bM * M + bG * G, matrix[N_spp,N_spp]: SIGMA &lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01), a ~ normal(0, 1), c(bM,bG) ~ normal(0, 0.5), etasq ~ half_normal(1, 0.25), rhosq ~ half_normal(3, 0.25) ), data = dat_list, chains = 4, cores = 4) 結果は以下の通り（表14.10）。集団サイズは小さいながらも、脳容量に影響を与えていることが分かる。 precis(m14.11) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% kable(digits = 2, booktabs = TRUE, caption = &quot;b14.11の結果。&quot;) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) 表14.10: b14.11の結果。 par mean sd X5.5. X94.5. n_eff Rhat4 a -0.06 0.08 -0.19 0.06 1809.88 1 bG 0.05 0.02 0.01 0.09 1839.62 1 bM 0.83 0.03 0.79 0.88 1473.26 1 etasq 0.04 0.01 0.03 0.05 2057.37 1 rhosq 2.80 0.23 2.42 3.18 2084.41 1 \\(K_{ij}\\)の事前分布と事後分布を描いたのが下図である。推定結果から、いずれの系統的距離においても種間のばらつきが小さいことが分かる。 14.6 Practice 14.6.1 14M1 Repeat the café robot simulation from the beginning of the chapter. This time, set rho to zero, so that there is no correlation between intercepts and slopes. How does the posterior distribution of the correlation reflect this change in the underlying simulation? 本章冒頭のロボットの例で、ランダム切片とランダム傾きの間の相関をゼロにしてみる。 a &lt;- 3.5 # averange morning wait time b &lt;- (-1) # average difference afternoon wait time sigma_a &lt;- 1 # std dev in intercepts sigma_b &lt;- 0.5 # std dev in slopes rho &lt;- 0 # correlation between interecpts and slopes mu &lt;- c(a,b) cov_ab &lt;- sigma_a*sigma_b*rho #共分散 sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol=2) # 分散共分散行列 n_cafes &lt;- 20 set.seed(5) vary_effects &lt;- MASS::mvrnorm(n_cafes, mu, sigma) %&gt;% data.frame() %&gt;% set_names(&quot;a_cafe&quot;, &quot;b_cafe&quot;) n_visits &lt;- 10 sigma_y &lt;- 0.5 set.seed(22) dat &lt;- vary_effects %&gt;% mutate(cafe = 1:n()) %&gt;% tidyr::expand(nesting(cafe, a_cafe, b_cafe), visits = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n()/2)) %&gt;% mutate(mu = a_cafe + b_cafe*afternoon) %&gt;% mutate(wait = rnorm(n=n(), mean=mu, sd=sigma_y)) それでは、モデリングを行う。 b14M1 &lt;- brm(data =dat, family = gaussian, wait ~ 1 + afternoon + (1+afternoon|cafe), prior = c(prior(normal(5,2), class = Intercept), prior(normal(-1,0.5), class = b), prior(exponential(1), class = sigma), prior(exponential(1), class =sd), prior(lkj(2), class = cor)), backend = &quot;cmdstanr&quot;, seed = 155, file = &quot;output/Chapter14/b14M1&quot;) 推定値もほとんどゼロになる。 posterior_summary(b14M1) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;par&quot;) %&gt;% filter(str_detect(par,&quot;cor&quot;)) %&gt;% kable(digits = 2, booktabs = TRUE) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;hold_position&quot;)) par Estimate Est.Error Q2.5 Q97.5 cor_cafe__Intercept__afternoon -0.04 0.22 -0.45 0.38 14.6.2 14M2 Fit this multilevel model to the simulated café data: \\[ \\begin{aligned} W_{i} &amp;\\sim Normal(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp;= \\alpha_{cafe[i]} + \\beta_{cafe[i]}A_{i}\\\\ \\alpha_{cafe} &amp;\\sim Normal(\\alpha, \\sigma_{\\alpha}) \\\\ \\beta_{cafe} &amp;\\sim Normal(\\beta, \\sigma_{\\beta})\\\\ \\alpha &amp;\\sim Normal(0,10)\\\\ \\beta &amp;\\sim Normal(0,10)\\\\ \\sigma &amp;\\sim HalfCauchy(0,1)\\\\ \\sigma_{\\alpha} &amp;\\sim HalfCauchy(0,1)\\\\ \\sigma_{\\beta} &amp;\\sim HalfCauchy(0,1)\\\\ \\end{aligned} \\] Use WAIC to compare this model to the model from the chapter, the one that uses a multi-variate Gaussian prior. Explain the result. 本章冒頭のシミュレーションデータで、ランダム切片とランダム傾きの間に相関を仮定しない（それぞれが独立に得られる）モデルを考える。brmsでは、(1+afternoon||cafe)のように書くことで、ランダム切片とランダム傾きの相関を考慮しないモデルを作れる。 b14M2 &lt;- brm(data =d, family = gaussian, wait ~ 1 + afternoon + (1+afternoon||cafe), prior = c(prior(normal(0,10), class = Intercept), prior(normal(0,10), class = b), prior(exponential(1), class = sigma), prior(exponential(1), class =sd)), backend = &quot;cmdstanr&quot;, seed = 155, file = &quot;output/Chapter14/b14M2&quot;) b14.1の結果と比較してみる。推定結果に大きな違いはなさそう。 表14.11: b14.1の結果 par Estimate Est.Error Q2.5 Q97.5 b_Intercept 3.65 0.22 3.23 4.07 b_afternoon -1.13 0.14 -1.41 -0.86 sd_cafe__Intercept 0.96 0.17 0.69 1.36 sd_cafe__afternoon 0.60 0.13 0.38 0.88 cor_cafe__Intercept__afternoon -0.50 0.18 -0.80 -0.08 sigma 0.47 0.03 0.42 0.53 lprior -5.07 0.44 -6.09 -4.38 表14.12: b14M2の結果。 par Estimate Est.Error Q2.5 Q97.5 b_Intercept 3.64 0.21 3.22 4.06 b_afternoon -1.13 0.15 -1.43 -0.85 sd_cafe__Intercept 0.94 0.16 0.68 1.31 sd_cafe__afternoon 0.57 0.13 0.36 0.86 sigma 0.48 0.03 0.43 0.53 lprior -8.48 0.22 -8.97 -8.11 図示してみると、結果は類似している。しかし、切片と傾きの相関はb14.1では-0.62、b14M2では-0.52で、相関を仮定したモデルの方が高いことが分かる。 References "],["実行環境.html", "実行環境", " 実行環境 sessionInfo() ## R version 4.2.2 (2022-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 22621) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Japanese_Japan.utf8 LC_CTYPE=Japanese_Japan.utf8 ## [3] LC_MONETARY=Japanese_Japan.utf8 LC_NUMERIC=C ## [5] LC_TIME=Japanese_Japan.utf8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ggtree_3.6.2 ape_5.6-2 see_0.7.4 ## [4] report_0.5.5 parameters_0.20.0 performance_0.10.1 ## [7] modelbased_0.8.5 insight_0.18.8 effectsize_0.8.2 ## [10] datawizard_0.6.5 correlation_0.8.3 bayestestR_0.13.0 ## [13] easystats_0.6.0 statebins_1.4.0 posterior_1.3.1 ## [16] gtools_3.9.4 MASS_7.3-58.1 lemon_0.4.6 ## [19] gghighlight_0.4.0 glue_1.6.2 modelr_0.1.10 ## [22] fontregisterer_0.3 systemfonts_1.0.4 extrafont_0.18 ## [25] gt_0.8.0 loo_2.5.1 ggthemes_4.2.4 ## [28] bayesplot_1.10.0 dagitty_0.3-1 ggdag_0.2.7 ## [31] patchwork_1.1.2 ggrepel_0.9.2 tidybayes_3.0.2 ## [34] ggmcmc_1.5.1.1 brms_2.18.0 Rcpp_1.0.9 ## [37] rethinking_2.13.2 forcats_0.5.2 stringr_1.5.0 ## [40] dplyr_1.0.10 purrr_1.0.0 readr_2.1.3 ## [43] tidyr_1.2.1 tibble_3.1.8 tidyverse_1.3.2 ## [46] knitr_1.41 kableExtra_1.3.4 bookdown_0.31 ## [49] rstan_2.26.13 StanHeaders_2.26.13 GGally_2.1.2 ## [52] ggplot2_3.4.0 DT_0.27 ## ## loaded via a namespace (and not attached): ## [1] estimability_1.4.1 coda_0.19-4 dygraphs_1.1.1.6 ## [4] data.table_1.14.6 inline_0.3.19 generics_0.1.3 ## [7] callr_3.7.3 tzdb_0.3.0 webshot_0.5.4 ## [10] xml2_1.3.3 lubridate_1.9.0 httpuv_1.6.7 ## [13] assertthat_0.2.1 isoband_0.2.7 viridis_0.6.2 ## [16] gargle_1.2.1 xfun_0.36 hms_1.1.2 ## [19] ggdist_3.2.0 jquerylib_0.1.4 evaluate_0.20 ## [22] promises_1.2.0.1 fansi_1.0.3 dbplyr_2.2.1 ## [25] readxl_1.4.1 igraph_1.3.5 DBI_1.1.3 ## [28] htmlwidgets_1.6.1 reshape_0.8.9 tensorA_0.36.2 ## [31] googledrive_2.0.0 stats4_4.2.2 ellipsis_0.3.2 ## [34] crosstalk_1.2.0 backports_1.4.1 V8_4.2.2 ## [37] markdown_1.4 RcppParallel_5.1.6 vctrs_0.5.1 ## [40] cmdstanr_0.5.3 abind_1.4-5 cachem_1.0.6 ## [43] RcppEigen_0.3.3.9.3 withr_2.5.0 ggforce_0.4.1 ## [46] checkmate_2.1.0 emmeans_1.8.3 treeio_1.22.0 ## [49] xts_0.12.2 prettyunits_1.1.1 svglite_2.1.1 ## [52] lazyeval_0.2.2 crayon_1.5.2 pkgconfig_2.0.3 ## [55] labeling_0.4.2 tweenr_2.0.2 nlme_3.1-160 ## [58] rlang_1.0.6 lifecycle_1.0.3 miniUI_0.1.1.1 ## [61] colourpicker_1.2.0 extrafontdb_1.0 cellranger_1.1.0 ## [64] distributional_0.3.1 polyclip_1.10-4 matrixStats_0.63.0 ## [67] flextable_0.8.3 Matrix_1.5-1 aplot_0.1.9 ## [70] boot_1.3-28 zoo_1.8-11 reprex_2.0.2 ## [73] base64enc_0.1-3 processx_3.8.0 googlesheets4_1.0.1 ## [76] viridisLite_0.4.1 shape_1.4.6 shinystan_2.6.0 ## [79] gridGraphics_0.5-1 scales_1.2.1 magrittr_2.0.3 ## [82] plyr_1.8.8 hexbin_1.28.2 threejs_0.3.3 ## [85] compiler_4.2.2 rstantools_2.2.0 RColorBrewer_1.1-3 ## [88] santoku_0.9.0 cli_3.6.0 ps_1.7.2 ## [91] Brobdingnag_1.2-9 tidyselect_1.2.0 stringi_1.7.8 ## [94] highr_0.10 yaml_2.3.6 askpass_1.1 ## [97] svUnit_1.0.6 bridgesampling_1.1-2 grid_4.2.2 ## [100] sass_0.4.5 tools_4.2.2 timechange_0.1.1 ## [103] rstudioapi_0.14 uuid_1.1-0 BH_1.81.0-1 ## [106] gridExtra_2.3 farver_2.1.1 ggraph_2.1.0 ## [109] digest_0.6.31 shiny_1.7.4 broom_1.0.2 ## [112] later_1.3.0 httr_1.4.4 gdtools_0.2.4 ## [115] colorspace_2.0-3 rvest_1.0.3 fs_1.5.2 ## [118] yulab.utils_0.0.6 tidytree_0.4.2 graphlayouts_0.8.4 ## [121] shinythemes_1.2.0 ggplotify_0.1.0 xtable_1.8-4 ## [124] jsonlite_1.8.4 tidygraph_1.2.2 ggfun_0.0.9 ## [127] R6_2.5.1 pillar_1.8.1 htmltools_0.5.4 ## [130] mime_0.12 fastmap_1.1.0 codetools_0.2-18 ## [133] pkgbuild_1.4.0 mvtnorm_1.1-3 utf8_1.2.2 ## [136] lattice_0.20-45 bslib_0.4.2 arrayhelpers_1.1-0 ## [139] curl_4.3.3 HDInterval_0.2.4 officer_0.5.1 ## [142] zip_2.2.2 shinyjs_2.1.0 openssl_2.0.5 ## [145] Rttf2pt1_1.3.8 rmarkdown_2.20 munsell_0.5.0 ## [148] haven_2.5.1 reshape2_1.4.4 gtable_0.3.1 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
